{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = list(paths.list_images('data/Caltech101/001'))\n",
    "# image_paths = list(paths.list_images('data/cars_side-view'))\n",
    "image_paths = list(paths.list_images('data/Caltech101/016'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:00<00:00, 2668.46it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for img_path in tqdm(image_paths):\n",
    "    label = img_path.split(os.path.sep)[-2]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    data.append(img)\n",
    "    labels.append(label)\n",
    "    if len(labels) > 5000:\n",
    "        break\n",
    "    \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lb = LabelEncoder()\n",
    "labels = lb.fit_transform(labels)\n",
    "print(f\"Total Number of Classes: {len(lb.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 123})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train examples: (110, 197, 300, 3)\n",
      "x_test examples: (13, 197, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# divide the data into train and test set\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(data, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "print(f\"x_train examples: {x_train.shape}\\nx_test examples: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {'size': 64, 'channels': 3, 'classes': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((dataset_config['size'], dataset_config['size'])),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((dataset_config['size'],dataset_config['size'])),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BS = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BS, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BS, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "# custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels= None, transforms = None):\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.images[index][:]\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        \n",
    "        return (data, self.labels[index])\n",
    "        \n",
    "train_data = CustomDataset(x_train, y_train, train_transforms)\n",
    "test_data = CustomDataset(x_test, y_test, val_transform)       \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=BS, shuffle=True, num_workers=4, drop_last=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Main --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def gradients(y, x):\n",
    "    return autograd.grad(\n",
    "                outputs=y, inputs=x, retain_graph=True,\n",
    "                create_graph=True, grad_outputs=torch.ones_like(y), only_inputs=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWReduction(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x -> [B, C, H, W]\n",
    "        return x.mean(dim=(-1, -2))\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape: list):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape([batch_size] + self.shape)\n",
    "\n",
    "class VAE_Cifar10(nn.Module):\n",
    "    def __init__(self, label = 'cifar10', image_size = dataset_config['size'],\n",
    "                 channel_num = dataset_config['channels'],\n",
    "                 z_size=128):\n",
    "        # configurations\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.image_size = image_size\n",
    "        self.channel_num = channel_num\n",
    "        self.z_size = z_size\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.capacity_conv(channel_num, 16), # 16 x 64 x 64\n",
    "            nn.InstanceNorm2d(16),\n",
    "            self.downsampling_conv(16, 32), # 32 x 32 x 32\n",
    "            self.capacity_conv(32, 64), # 64 x 32 x 32\n",
    "            self.downsampling_conv(64, 128), # 128 x 16 x 16\n",
    "            self.capacity_conv(128, 256), # 256 x 16 x 16\n",
    "            self.downsampling_conv(256, 512), # 512 x 8 x 8\n",
    "            HWReduction(),\n",
    "        )\n",
    "\n",
    "        # H, W will be reduced\n",
    "\n",
    "\n",
    "        # q\n",
    "        self.q_mean = self._linear(512, z_size, relu=False)\n",
    "        self.q_logvar = self._linear(512, z_size, relu=False)\n",
    "\n",
    "        # projection\n",
    "        self.project = nn.Sequential(\n",
    "            self._linear(z_size, 1024),\n",
    "            self._linear(1024, 8 * 8 * 128),\n",
    "            Reshape([128, 8, 8])\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upsampling_conv(128, 64), # 32 x 16 x 16\n",
    "            self.capacity_conv(64, 64), # 64 x 16 x 16\n",
    "            self.upsampling_conv(64, 32), # 32 x 32 x 32\n",
    "            self.capacity_conv(32, 32), # 32 x 32 x 32\n",
    "            self.upsampling_conv(32, 16), # 16 x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                16, channel_num,\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # sample latent code z from q given x.\n",
    "        mean, logvar = self.q_mean(encoded), self.q_logvar(encoded) - 10.0\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        z_projected = self.project(z)\n",
    "\n",
    "        # reconstruct x from z\n",
    "        x_reconstructed = self.decoder(z_projected)\n",
    "        return x_reconstructed, mean, logvar\n",
    "    \n",
    "    # ======\n",
    "    # Layers\n",
    "    # ======\n",
    "\n",
    "    def downsampling_conv(self, channel_size, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channel_size, kernel_num,\n",
    "                kernel_size=4, stride=2, padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(kernel_num),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "    def capacity_conv(self, channel_num, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channel_num, kernel_num,\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "            ),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "\n",
    "    def upsampling_conv(self, channel_num, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                channel_num, kernel_num,\n",
    "                kernel_size=4, stride=2, padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(kernel_num),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "\n",
    "    def _linear(self, in_size, out_size, relu=True):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.ReLU(),\n",
    "        ) if relu else nn.Linear(in_size, out_size)\n",
    "\n",
    "model = VAE_Cifar10().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = (recon_x - x.view(-1,  dataset_config['channels'],dataset_config['size'],dataset_config['size'])) ** 2\n",
    "    MSE = MSE.sum(dim=(-1,-2,-3))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
    "\n",
    "    return MSE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_eta):\n",
    "    model.train()\n",
    "    log_eta, just_updated = log_eta\n",
    "    train_recons_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        recons_loss, kld_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = (recons_loss + float(np.exp(log_eta)) * kld_loss).mean(dim=0)\n",
    "        loss.backward()\n",
    "        train_recons_loss += recons_loss.mean(dim=0).item()\n",
    "        train_kld_loss += kld_loss.mean(dim=0).item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tRecons Loss: {:.6f}; KLD Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                recons_loss.mean(dim=0).item(), recons_loss.mean(dim=0).item()))\n",
    "    \n",
    "    print('====> Epoch: {} Average recons loss: {:.4f}, kld loss: {:.4f}'.format(\n",
    "          epoch, train_recons_loss / len(train_loader.dataset), train_kld_loss / len(train_loader.dataset)))\n",
    "    if train_recons_loss / len(train_loader.dataset) < 6.0 and (epoch - just_updated) >= 10 and (train_recons_loss / len(train_loader.dataset)) < train_kld_loss / len(train_loader.dataset):\n",
    "        log_eta += 0.25\n",
    "        if log_eta >= 0.0:\n",
    "            log_eta = 0.0\n",
    "        just_updated = epoch\n",
    "        print('====> Epoch: {} Eta is now {:.4f} due to sufficient recons results'.format(epoch, float(np.exp(log_eta))))\n",
    "    return (log_eta, just_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_recons_loss = 0\n",
    "    test_kld_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            recons_loss, kld_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            test_recons_loss += recons_loss.mean(dim=0).item()\n",
    "            test_kld_loss += kld_loss.mean(dim=0).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch[:n].view(n,  dataset_config['channels'], dataset_config['size'], dataset_config['size'])[:n]])\n",
    "                if not os.path.exists(\"results/\"):\n",
    "                    os.mkdir(\"results\")\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "        for i, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch[:n].view(n,  dataset_config['channels'], dataset_config['size'], dataset_config['size'])[:n]])\n",
    "                if not os.path.exists(\"overfit_results/\"):\n",
    "                    os.mkdir(\"overfit_results\")\n",
    "                save_image(comparison.cpu(),\n",
    "                         'overfit_results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_recons_loss /= len(test_loader.dataset)\n",
    "    test_kld_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set recons loss: {:.4f}, kld_loss: {:.4f}'.format(test_recons_loss, test_kld_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/110 (0%)]\tRecons Loss: 602.973755; KLD Loss: 602.973755\n",
      "Train Epoch: 1 [64/110 (50%)]\tRecons Loss: 589.637573; KLD Loss: 589.637573\n",
      "====> Epoch: 1 Average recons loss: 21.3870, kld loss: 41.7312\n",
      "====> Test set recons loss: 41.4171, kld_loss: 87.0031\n",
      "Train Epoch: 2 [0/110 (0%)]\tRecons Loss: 501.622375; KLD Loss: 501.622375\n",
      "Train Epoch: 2 [64/110 (50%)]\tRecons Loss: 625.666687; KLD Loss: 625.666687\n",
      "====> Epoch: 2 Average recons loss: 19.1828, kld loss: 40.7559\n",
      "====> Test set recons loss: 39.4873, kld_loss: 84.8462\n",
      "Train Epoch: 3 [0/110 (0%)]\tRecons Loss: 539.166626; KLD Loss: 539.166626\n",
      "Train Epoch: 3 [64/110 (50%)]\tRecons Loss: 493.024567; KLD Loss: 493.024567\n",
      "====> Epoch: 3 Average recons loss: 18.7149, kld loss: 39.7263\n",
      "====> Test set recons loss: 38.0889, kld_loss: 82.6257\n",
      "Train Epoch: 4 [0/110 (0%)]\tRecons Loss: 543.895508; KLD Loss: 543.895508\n",
      "Train Epoch: 4 [64/110 (50%)]\tRecons Loss: 495.436340; KLD Loss: 495.436340\n",
      "====> Epoch: 4 Average recons loss: 18.0295, kld loss: 38.6757\n",
      "====> Test set recons loss: 37.3324, kld_loss: 80.4146\n",
      "Train Epoch: 5 [0/110 (0%)]\tRecons Loss: 492.419617; KLD Loss: 492.419617\n",
      "Train Epoch: 5 [64/110 (50%)]\tRecons Loss: 540.404053; KLD Loss: 540.404053\n",
      "====> Epoch: 5 Average recons loss: 17.0175, kld loss: 37.6271\n",
      "====> Test set recons loss: 36.3341, kld_loss: 78.1668\n",
      "Train Epoch: 6 [0/110 (0%)]\tRecons Loss: 473.115784; KLD Loss: 473.115784\n",
      "Train Epoch: 6 [64/110 (50%)]\tRecons Loss: 454.304352; KLD Loss: 454.304352\n",
      "====> Epoch: 6 Average recons loss: 16.1395, kld loss: 36.6148\n",
      "====> Test set recons loss: 34.2342, kld_loss: 76.1597\n",
      "Train Epoch: 7 [0/110 (0%)]\tRecons Loss: 448.704956; KLD Loss: 448.704956\n",
      "Train Epoch: 7 [64/110 (50%)]\tRecons Loss: 399.168152; KLD Loss: 399.168152\n",
      "====> Epoch: 7 Average recons loss: 15.9862, kld loss: 35.6100\n",
      "====> Test set recons loss: 33.9964, kld_loss: 73.8916\n",
      "Train Epoch: 8 [0/110 (0%)]\tRecons Loss: 407.157715; KLD Loss: 407.157715\n",
      "Train Epoch: 8 [64/110 (50%)]\tRecons Loss: 428.036011; KLD Loss: 428.036011\n",
      "====> Epoch: 8 Average recons loss: 15.4244, kld loss: 34.6070\n",
      "====> Test set recons loss: 33.5881, kld_loss: 71.8914\n",
      "Train Epoch: 9 [0/110 (0%)]\tRecons Loss: 428.486511; KLD Loss: 428.486511\n",
      "Train Epoch: 9 [64/110 (50%)]\tRecons Loss: 386.061707; KLD Loss: 386.061707\n",
      "====> Epoch: 9 Average recons loss: 14.3608, kld loss: 33.6441\n",
      "====> Test set recons loss: 32.2091, kld_loss: 69.8514\n",
      "Train Epoch: 10 [0/110 (0%)]\tRecons Loss: 400.801514; KLD Loss: 400.801514\n",
      "Train Epoch: 10 [64/110 (50%)]\tRecons Loss: 335.973389; KLD Loss: 335.973389\n",
      "====> Epoch: 10 Average recons loss: 13.4630, kld loss: 32.7020\n",
      "====> Test set recons loss: 32.1331, kld_loss: 67.9382\n",
      "Train Epoch: 11 [0/110 (0%)]\tRecons Loss: 379.401001; KLD Loss: 379.401001\n",
      "Train Epoch: 11 [64/110 (50%)]\tRecons Loss: 307.790588; KLD Loss: 307.790588\n",
      "====> Epoch: 11 Average recons loss: 12.9862, kld loss: 31.8598\n",
      "====> Test set recons loss: 32.0411, kld_loss: 66.0627\n",
      "Train Epoch: 12 [0/110 (0%)]\tRecons Loss: 379.195129; KLD Loss: 379.195129\n",
      "Train Epoch: 12 [64/110 (50%)]\tRecons Loss: 326.007843; KLD Loss: 326.007843\n",
      "====> Epoch: 12 Average recons loss: 12.4963, kld loss: 30.9961\n",
      "====> Test set recons loss: 31.5317, kld_loss: 64.4342\n",
      "Train Epoch: 13 [0/110 (0%)]\tRecons Loss: 360.093292; KLD Loss: 360.093292\n",
      "Train Epoch: 13 [64/110 (50%)]\tRecons Loss: 347.432953; KLD Loss: 347.432953\n",
      "====> Epoch: 13 Average recons loss: 12.5405, kld loss: 30.3149\n",
      "====> Test set recons loss: 29.8972, kld_loss: 63.1837\n",
      "Train Epoch: 14 [0/110 (0%)]\tRecons Loss: 338.490417; KLD Loss: 338.490417\n",
      "Train Epoch: 14 [64/110 (50%)]\tRecons Loss: 330.411072; KLD Loss: 330.411072\n",
      "====> Epoch: 14 Average recons loss: 11.5891, kld loss: 29.7660\n",
      "====> Test set recons loss: 29.9543, kld_loss: 62.1528\n",
      "Train Epoch: 15 [0/110 (0%)]\tRecons Loss: 335.209900; KLD Loss: 335.209900\n",
      "Train Epoch: 15 [64/110 (50%)]\tRecons Loss: 291.587952; KLD Loss: 291.587952\n",
      "====> Epoch: 15 Average recons loss: 11.3835, kld loss: 29.2909\n",
      "====> Test set recons loss: 27.0812, kld_loss: 61.1413\n",
      "Train Epoch: 16 [0/110 (0%)]\tRecons Loss: 302.437469; KLD Loss: 302.437469\n",
      "Train Epoch: 16 [64/110 (50%)]\tRecons Loss: 295.410339; KLD Loss: 295.410339\n",
      "====> Epoch: 16 Average recons loss: 11.1407, kld loss: 28.7851\n",
      "====> Test set recons loss: 28.2380, kld_loss: 60.1552\n",
      "Train Epoch: 17 [0/110 (0%)]\tRecons Loss: 273.790741; KLD Loss: 273.790741\n",
      "Train Epoch: 17 [64/110 (50%)]\tRecons Loss: 301.518829; KLD Loss: 301.518829\n",
      "====> Epoch: 17 Average recons loss: 10.3509, kld loss: 28.4099\n",
      "====> Test set recons loss: 28.5078, kld_loss: 59.6775\n",
      "Train Epoch: 18 [0/110 (0%)]\tRecons Loss: 275.291626; KLD Loss: 275.291626\n",
      "Train Epoch: 18 [64/110 (50%)]\tRecons Loss: 257.746338; KLD Loss: 257.746338\n",
      "====> Epoch: 18 Average recons loss: 10.0612, kld loss: 28.2089\n",
      "====> Test set recons loss: 29.4667, kld_loss: 59.3639\n",
      "Train Epoch: 19 [0/110 (0%)]\tRecons Loss: 282.209290; KLD Loss: 282.209290\n",
      "Train Epoch: 19 [64/110 (50%)]\tRecons Loss: 260.044891; KLD Loss: 260.044891\n",
      "====> Epoch: 19 Average recons loss: 9.8514, kld loss: 28.1418\n",
      "====> Test set recons loss: 27.9308, kld_loss: 59.1542\n",
      "Train Epoch: 20 [0/110 (0%)]\tRecons Loss: 258.551727; KLD Loss: 258.551727\n",
      "Train Epoch: 20 [64/110 (50%)]\tRecons Loss: 236.224884; KLD Loss: 236.224884\n",
      "====> Epoch: 20 Average recons loss: 9.3170, kld loss: 28.0252\n",
      "====> Test set recons loss: 29.6261, kld_loss: 58.9331\n",
      "Train Epoch: 21 [0/110 (0%)]\tRecons Loss: 269.897644; KLD Loss: 269.897644\n",
      "Train Epoch: 21 [64/110 (50%)]\tRecons Loss: 230.865631; KLD Loss: 230.865631\n",
      "====> Epoch: 21 Average recons loss: 9.0882, kld loss: 28.0533\n",
      "====> Test set recons loss: 28.4725, kld_loss: 59.3013\n",
      "Train Epoch: 22 [0/110 (0%)]\tRecons Loss: 245.763306; KLD Loss: 245.763306\n",
      "Train Epoch: 22 [64/110 (50%)]\tRecons Loss: 224.729828; KLD Loss: 224.729828\n",
      "====> Epoch: 22 Average recons loss: 8.9362, kld loss: 28.1576\n",
      "====> Test set recons loss: 28.9559, kld_loss: 59.4432\n",
      "Train Epoch: 23 [0/110 (0%)]\tRecons Loss: 240.754730; KLD Loss: 240.754730\n",
      "Train Epoch: 23 [64/110 (50%)]\tRecons Loss: 238.220581; KLD Loss: 238.220581\n",
      "====> Epoch: 23 Average recons loss: 8.2863, kld loss: 28.2510\n",
      "====> Test set recons loss: 28.1052, kld_loss: 59.7658\n",
      "Train Epoch: 24 [0/110 (0%)]\tRecons Loss: 252.688858; KLD Loss: 252.688858\n",
      "Train Epoch: 24 [64/110 (50%)]\tRecons Loss: 201.245438; KLD Loss: 201.245438\n",
      "====> Epoch: 24 Average recons loss: 8.1776, kld loss: 28.4885\n",
      "====> Test set recons loss: 27.4931, kld_loss: 60.3431\n",
      "Train Epoch: 25 [0/110 (0%)]\tRecons Loss: 218.472015; KLD Loss: 218.472015\n",
      "Train Epoch: 25 [64/110 (50%)]\tRecons Loss: 209.745270; KLD Loss: 209.745270\n",
      "====> Epoch: 25 Average recons loss: 7.7867, kld loss: 28.6779\n",
      "====> Test set recons loss: 28.9518, kld_loss: 60.7186\n",
      "Train Epoch: 26 [0/110 (0%)]\tRecons Loss: 206.566544; KLD Loss: 206.566544\n",
      "Train Epoch: 26 [64/110 (50%)]\tRecons Loss: 213.270935; KLD Loss: 213.270935\n",
      "====> Epoch: 26 Average recons loss: 7.6284, kld loss: 28.9157\n",
      "====> Test set recons loss: 28.2854, kld_loss: 61.2503\n",
      "Train Epoch: 27 [0/110 (0%)]\tRecons Loss: 191.725388; KLD Loss: 191.725388\n",
      "Train Epoch: 27 [64/110 (50%)]\tRecons Loss: 206.966003; KLD Loss: 206.966003\n",
      "====> Epoch: 27 Average recons loss: 7.2736, kld loss: 29.1250\n",
      "====> Test set recons loss: 28.1249, kld_loss: 61.5891\n",
      "Train Epoch: 28 [0/110 (0%)]\tRecons Loss: 189.157898; KLD Loss: 189.157898\n",
      "Train Epoch: 28 [64/110 (50%)]\tRecons Loss: 190.388977; KLD Loss: 190.388977\n",
      "====> Epoch: 28 Average recons loss: 7.1130, kld loss: 29.2695\n",
      "====> Test set recons loss: 28.5044, kld_loss: 61.9177\n",
      "Train Epoch: 29 [0/110 (0%)]\tRecons Loss: 170.985794; KLD Loss: 170.985794\n",
      "Train Epoch: 29 [64/110 (50%)]\tRecons Loss: 199.010544; KLD Loss: 199.010544\n",
      "====> Epoch: 29 Average recons loss: 7.0179, kld loss: 29.5002\n",
      "====> Test set recons loss: 27.2700, kld_loss: 62.3266\n",
      "Train Epoch: 30 [0/110 (0%)]\tRecons Loss: 184.448364; KLD Loss: 184.448364\n",
      "Train Epoch: 30 [64/110 (50%)]\tRecons Loss: 177.898193; KLD Loss: 177.898193\n",
      "====> Epoch: 30 Average recons loss: 6.8079, kld loss: 29.5987\n",
      "====> Test set recons loss: 29.7139, kld_loss: 62.5927\n",
      "Train Epoch: 31 [0/110 (0%)]\tRecons Loss: 166.686310; KLD Loss: 166.686310\n",
      "Train Epoch: 31 [64/110 (50%)]\tRecons Loss: 195.210114; KLD Loss: 195.210114\n",
      "====> Epoch: 31 Average recons loss: 6.4482, kld loss: 29.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 27.3322, kld_loss: 62.9920\n",
      "Train Epoch: 32 [0/110 (0%)]\tRecons Loss: 186.011566; KLD Loss: 186.011566\n",
      "Train Epoch: 32 [64/110 (50%)]\tRecons Loss: 170.248108; KLD Loss: 170.248108\n",
      "====> Epoch: 32 Average recons loss: 6.2441, kld loss: 29.9525\n",
      "====> Test set recons loss: 29.6751, kld_loss: 63.2534\n",
      "Train Epoch: 33 [0/110 (0%)]\tRecons Loss: 168.512192; KLD Loss: 168.512192\n",
      "Train Epoch: 33 [64/110 (50%)]\tRecons Loss: 177.907471; KLD Loss: 177.907471\n",
      "====> Epoch: 33 Average recons loss: 6.0061, kld loss: 30.0889\n",
      "====> Test set recons loss: 28.0845, kld_loss: 63.5753\n",
      "Train Epoch: 34 [0/110 (0%)]\tRecons Loss: 175.476868; KLD Loss: 175.476868\n",
      "Train Epoch: 34 [64/110 (50%)]\tRecons Loss: 156.874329; KLD Loss: 156.874329\n",
      "====> Epoch: 34 Average recons loss: 5.9269, kld loss: 30.2944\n",
      "====> Epoch: 34 Eta is now 0.0639 due to sufficient recons results\n",
      "====> Test set recons loss: 28.1702, kld_loss: 64.0722\n",
      "Train Epoch: 35 [0/110 (0%)]\tRecons Loss: 153.029358; KLD Loss: 153.029358\n",
      "Train Epoch: 35 [64/110 (50%)]\tRecons Loss: 165.149933; KLD Loss: 165.149933\n",
      "====> Epoch: 35 Average recons loss: 5.6254, kld loss: 30.4129\n",
      "====> Test set recons loss: 29.8278, kld_loss: 64.2140\n",
      "Train Epoch: 36 [0/110 (0%)]\tRecons Loss: 161.227997; KLD Loss: 161.227997\n",
      "Train Epoch: 36 [64/110 (50%)]\tRecons Loss: 154.816528; KLD Loss: 154.816528\n",
      "====> Epoch: 36 Average recons loss: 5.7992, kld loss: 30.4543\n",
      "====> Test set recons loss: 29.2006, kld_loss: 64.1615\n",
      "Train Epoch: 37 [0/110 (0%)]\tRecons Loss: 142.056061; KLD Loss: 142.056061\n",
      "Train Epoch: 37 [64/110 (50%)]\tRecons Loss: 151.605042; KLD Loss: 151.605042\n",
      "====> Epoch: 37 Average recons loss: 5.5639, kld loss: 30.4488\n",
      "====> Test set recons loss: 29.8351, kld_loss: 64.1215\n",
      "Train Epoch: 38 [0/110 (0%)]\tRecons Loss: 144.025497; KLD Loss: 144.025497\n",
      "Train Epoch: 38 [64/110 (50%)]\tRecons Loss: 143.964417; KLD Loss: 143.964417\n",
      "====> Epoch: 38 Average recons loss: 5.3326, kld loss: 30.3966\n",
      "====> Test set recons loss: 29.8966, kld_loss: 64.0161\n",
      "Train Epoch: 39 [0/110 (0%)]\tRecons Loss: 128.045517; KLD Loss: 128.045517\n",
      "Train Epoch: 39 [64/110 (50%)]\tRecons Loss: 136.502182; KLD Loss: 136.502182\n",
      "====> Epoch: 39 Average recons loss: 5.0843, kld loss: 30.3879\n",
      "====> Test set recons loss: 29.8458, kld_loss: 63.8624\n",
      "Train Epoch: 40 [0/110 (0%)]\tRecons Loss: 131.451660; KLD Loss: 131.451660\n",
      "Train Epoch: 40 [64/110 (50%)]\tRecons Loss: 141.628754; KLD Loss: 141.628754\n",
      "====> Epoch: 40 Average recons loss: 5.0367, kld loss: 30.2791\n",
      "====> Test set recons loss: 30.9099, kld_loss: 63.6994\n",
      "Train Epoch: 41 [0/110 (0%)]\tRecons Loss: 123.668701; KLD Loss: 123.668701\n",
      "Train Epoch: 41 [64/110 (50%)]\tRecons Loss: 137.092560; KLD Loss: 137.092560\n",
      "====> Epoch: 41 Average recons loss: 5.0232, kld loss: 30.2472\n",
      "====> Test set recons loss: 30.1664, kld_loss: 63.7275\n",
      "Train Epoch: 42 [0/110 (0%)]\tRecons Loss: 127.622650; KLD Loss: 127.622650\n",
      "Train Epoch: 42 [64/110 (50%)]\tRecons Loss: 135.180893; KLD Loss: 135.180893\n",
      "====> Epoch: 42 Average recons loss: 4.7304, kld loss: 30.1950\n",
      "====> Test set recons loss: 30.0205, kld_loss: 63.4475\n",
      "Train Epoch: 43 [0/110 (0%)]\tRecons Loss: 128.334747; KLD Loss: 128.334747\n",
      "Train Epoch: 43 [64/110 (50%)]\tRecons Loss: 133.292236; KLD Loss: 133.292236\n",
      "====> Epoch: 43 Average recons loss: 4.6259, kld loss: 30.1683\n",
      "====> Test set recons loss: 28.8528, kld_loss: 63.4192\n",
      "Train Epoch: 44 [0/110 (0%)]\tRecons Loss: 123.820755; KLD Loss: 123.820755\n",
      "Train Epoch: 44 [64/110 (50%)]\tRecons Loss: 127.578468; KLD Loss: 127.578468\n",
      "====> Epoch: 44 Average recons loss: 4.5490, kld loss: 30.0801\n",
      "====> Epoch: 44 Eta is now 0.0821 due to sufficient recons results\n",
      "====> Test set recons loss: 30.0583, kld_loss: 63.1432\n",
      "Train Epoch: 45 [0/110 (0%)]\tRecons Loss: 123.155205; KLD Loss: 123.155205\n",
      "Train Epoch: 45 [64/110 (50%)]\tRecons Loss: 117.544785; KLD Loss: 117.544785\n",
      "====> Epoch: 45 Average recons loss: 4.4335, kld loss: 29.9508\n",
      "====> Test set recons loss: 29.7674, kld_loss: 62.9957\n",
      "Train Epoch: 46 [0/110 (0%)]\tRecons Loss: 117.358994; KLD Loss: 117.358994\n",
      "Train Epoch: 46 [64/110 (50%)]\tRecons Loss: 130.795837; KLD Loss: 130.795837\n",
      "====> Epoch: 46 Average recons loss: 4.3640, kld loss: 29.8962\n",
      "====> Test set recons loss: 29.3008, kld_loss: 62.6612\n",
      "Train Epoch: 47 [0/110 (0%)]\tRecons Loss: 118.373383; KLD Loss: 118.373383\n",
      "Train Epoch: 47 [64/110 (50%)]\tRecons Loss: 114.160156; KLD Loss: 114.160156\n",
      "====> Epoch: 47 Average recons loss: 4.2587, kld loss: 29.6687\n",
      "====> Test set recons loss: 29.7598, kld_loss: 62.0622\n",
      "Train Epoch: 48 [0/110 (0%)]\tRecons Loss: 110.231628; KLD Loss: 110.231628\n",
      "Train Epoch: 48 [64/110 (50%)]\tRecons Loss: 120.224899; KLD Loss: 120.224899\n",
      "====> Epoch: 48 Average recons loss: 4.1975, kld loss: 29.4529\n",
      "====> Test set recons loss: 29.8801, kld_loss: 61.7469\n",
      "Train Epoch: 49 [0/110 (0%)]\tRecons Loss: 105.453049; KLD Loss: 105.453049\n",
      "Train Epoch: 49 [64/110 (50%)]\tRecons Loss: 105.972565; KLD Loss: 105.972565\n",
      "====> Epoch: 49 Average recons loss: 4.0767, kld loss: 29.2782\n",
      "====> Test set recons loss: 29.8103, kld_loss: 61.4653\n",
      "Train Epoch: 50 [0/110 (0%)]\tRecons Loss: 109.283257; KLD Loss: 109.283257\n",
      "Train Epoch: 50 [64/110 (50%)]\tRecons Loss: 114.237984; KLD Loss: 114.237984\n",
      "====> Epoch: 50 Average recons loss: 3.9249, kld loss: 29.1146\n",
      "====> Test set recons loss: 29.8821, kld_loss: 60.8992\n",
      "Train Epoch: 51 [0/110 (0%)]\tRecons Loss: 109.263786; KLD Loss: 109.263786\n",
      "Train Epoch: 51 [64/110 (50%)]\tRecons Loss: 105.229462; KLD Loss: 105.229462\n",
      "====> Epoch: 51 Average recons loss: 3.8155, kld loss: 28.8666\n",
      "====> Test set recons loss: 29.8485, kld_loss: 60.2614\n",
      "Train Epoch: 52 [0/110 (0%)]\tRecons Loss: 93.014687; KLD Loss: 93.014687\n",
      "Train Epoch: 52 [64/110 (50%)]\tRecons Loss: 109.605354; KLD Loss: 109.605354\n",
      "====> Epoch: 52 Average recons loss: 3.8530, kld loss: 28.6227\n",
      "====> Test set recons loss: 30.7662, kld_loss: 59.8846\n",
      "Train Epoch: 53 [0/110 (0%)]\tRecons Loss: 94.601257; KLD Loss: 94.601257\n",
      "Train Epoch: 53 [64/110 (50%)]\tRecons Loss: 109.689453; KLD Loss: 109.689453\n",
      "====> Epoch: 53 Average recons loss: 3.6765, kld loss: 28.4473\n",
      "====> Test set recons loss: 29.8229, kld_loss: 59.6324\n",
      "Train Epoch: 54 [0/110 (0%)]\tRecons Loss: 93.372635; KLD Loss: 93.372635\n",
      "Train Epoch: 54 [64/110 (50%)]\tRecons Loss: 102.917099; KLD Loss: 102.917099\n",
      "====> Epoch: 54 Average recons loss: 3.7059, kld loss: 28.2980\n",
      "====> Epoch: 54 Eta is now 0.1054 due to sufficient recons results\n",
      "====> Test set recons loss: 30.5200, kld_loss: 59.1754\n",
      "Train Epoch: 55 [0/110 (0%)]\tRecons Loss: 97.841904; KLD Loss: 97.841904\n",
      "Train Epoch: 55 [64/110 (50%)]\tRecons Loss: 99.935783; KLD Loss: 99.935783\n",
      "====> Epoch: 55 Average recons loss: 3.6689, kld loss: 28.1240\n",
      "====> Test set recons loss: 30.1516, kld_loss: 58.7114\n",
      "Train Epoch: 56 [0/110 (0%)]\tRecons Loss: 101.576782; KLD Loss: 101.576782\n",
      "Train Epoch: 56 [64/110 (50%)]\tRecons Loss: 97.606567; KLD Loss: 97.606567\n",
      "====> Epoch: 56 Average recons loss: 3.6461, kld loss: 27.8491\n",
      "====> Test set recons loss: 30.4552, kld_loss: 58.1152\n",
      "Train Epoch: 57 [0/110 (0%)]\tRecons Loss: 97.516159; KLD Loss: 97.516159\n",
      "Train Epoch: 57 [64/110 (50%)]\tRecons Loss: 98.390755; KLD Loss: 98.390755\n",
      "====> Epoch: 57 Average recons loss: 3.4540, kld loss: 27.5412\n",
      "====> Test set recons loss: 30.4177, kld_loss: 57.4199\n",
      "Train Epoch: 58 [0/110 (0%)]\tRecons Loss: 95.876968; KLD Loss: 95.876968\n",
      "Train Epoch: 58 [64/110 (50%)]\tRecons Loss: 94.769547; KLD Loss: 94.769547\n",
      "====> Epoch: 58 Average recons loss: 3.4892, kld loss: 27.2061\n",
      "====> Test set recons loss: 31.2810, kld_loss: 56.6461\n",
      "Train Epoch: 59 [0/110 (0%)]\tRecons Loss: 100.202194; KLD Loss: 100.202194\n",
      "Train Epoch: 59 [64/110 (50%)]\tRecons Loss: 97.551498; KLD Loss: 97.551498\n",
      "====> Epoch: 59 Average recons loss: 3.4111, kld loss: 26.9109\n",
      "====> Test set recons loss: 29.9424, kld_loss: 55.9477\n",
      "Train Epoch: 60 [0/110 (0%)]\tRecons Loss: 94.490341; KLD Loss: 94.490341\n",
      "Train Epoch: 60 [64/110 (50%)]\tRecons Loss: 88.575836; KLD Loss: 88.575836\n",
      "====> Epoch: 60 Average recons loss: 3.4730, kld loss: 26.6057\n",
      "====> Test set recons loss: 29.3873, kld_loss: 55.4189\n",
      "Train Epoch: 61 [0/110 (0%)]\tRecons Loss: 83.906006; KLD Loss: 83.906006\n",
      "Train Epoch: 61 [64/110 (50%)]\tRecons Loss: 101.307312; KLD Loss: 101.307312\n",
      "====> Epoch: 61 Average recons loss: 3.3382, kld loss: 26.4117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 30.8820, kld_loss: 55.2483\n",
      "Train Epoch: 62 [0/110 (0%)]\tRecons Loss: 94.405807; KLD Loss: 94.405807\n",
      "Train Epoch: 62 [64/110 (50%)]\tRecons Loss: 85.979446; KLD Loss: 85.979446\n",
      "====> Epoch: 62 Average recons loss: 3.3892, kld loss: 26.2661\n",
      "====> Test set recons loss: 30.7893, kld_loss: 54.7038\n",
      "Train Epoch: 63 [0/110 (0%)]\tRecons Loss: 96.738770; KLD Loss: 96.738770\n",
      "Train Epoch: 63 [64/110 (50%)]\tRecons Loss: 90.551598; KLD Loss: 90.551598\n",
      "====> Epoch: 63 Average recons loss: 3.3724, kld loss: 26.0615\n",
      "====> Test set recons loss: 31.8315, kld_loss: 54.4563\n",
      "Train Epoch: 64 [0/110 (0%)]\tRecons Loss: 93.128807; KLD Loss: 93.128807\n",
      "Train Epoch: 64 [64/110 (50%)]\tRecons Loss: 88.884651; KLD Loss: 88.884651\n",
      "====> Epoch: 64 Average recons loss: 3.3228, kld loss: 25.9224\n",
      "====> Epoch: 64 Eta is now 0.1353 due to sufficient recons results\n",
      "====> Test set recons loss: 30.7801, kld_loss: 54.2560\n",
      "Train Epoch: 65 [0/110 (0%)]\tRecons Loss: 81.023628; KLD Loss: 81.023628\n",
      "Train Epoch: 65 [64/110 (50%)]\tRecons Loss: 86.918129; KLD Loss: 86.918129\n",
      "====> Epoch: 65 Average recons loss: 3.3032, kld loss: 25.8023\n",
      "====> Test set recons loss: 29.6334, kld_loss: 53.6826\n",
      "Train Epoch: 66 [0/110 (0%)]\tRecons Loss: 88.042992; KLD Loss: 88.042992\n",
      "Train Epoch: 66 [64/110 (50%)]\tRecons Loss: 87.716377; KLD Loss: 87.716377\n",
      "====> Epoch: 66 Average recons loss: 3.2674, kld loss: 25.5550\n",
      "====> Test set recons loss: 31.2618, kld_loss: 53.0437\n",
      "Train Epoch: 67 [0/110 (0%)]\tRecons Loss: 97.036743; KLD Loss: 97.036743\n",
      "Train Epoch: 67 [64/110 (50%)]\tRecons Loss: 82.747391; KLD Loss: 82.747391\n",
      "====> Epoch: 67 Average recons loss: 3.2454, kld loss: 25.2482\n",
      "====> Test set recons loss: 30.6545, kld_loss: 52.5295\n",
      "Train Epoch: 68 [0/110 (0%)]\tRecons Loss: 88.967941; KLD Loss: 88.967941\n",
      "Train Epoch: 68 [64/110 (50%)]\tRecons Loss: 81.216156; KLD Loss: 81.216156\n",
      "====> Epoch: 68 Average recons loss: 3.2332, kld loss: 24.9345\n",
      "====> Test set recons loss: 33.6660, kld_loss: 51.7080\n",
      "Train Epoch: 69 [0/110 (0%)]\tRecons Loss: 78.559761; KLD Loss: 78.559761\n",
      "Train Epoch: 69 [64/110 (50%)]\tRecons Loss: 92.134888; KLD Loss: 92.134888\n",
      "====> Epoch: 69 Average recons loss: 3.1588, kld loss: 24.6310\n",
      "====> Test set recons loss: 32.5333, kld_loss: 51.1304\n",
      "Train Epoch: 70 [0/110 (0%)]\tRecons Loss: 83.629097; KLD Loss: 83.629097\n",
      "Train Epoch: 70 [64/110 (50%)]\tRecons Loss: 88.531113; KLD Loss: 88.531113\n",
      "====> Epoch: 70 Average recons loss: 3.1054, kld loss: 24.3542\n",
      "====> Test set recons loss: 29.7065, kld_loss: 50.4300\n",
      "Train Epoch: 71 [0/110 (0%)]\tRecons Loss: 81.297897; KLD Loss: 81.297897\n",
      "Train Epoch: 71 [64/110 (50%)]\tRecons Loss: 87.874420; KLD Loss: 87.874420\n",
      "====> Epoch: 71 Average recons loss: 3.1095, kld loss: 24.1205\n",
      "====> Test set recons loss: 31.8481, kld_loss: 50.1335\n",
      "Train Epoch: 72 [0/110 (0%)]\tRecons Loss: 86.692940; KLD Loss: 86.692940\n",
      "Train Epoch: 72 [64/110 (50%)]\tRecons Loss: 87.463676; KLD Loss: 87.463676\n",
      "====> Epoch: 72 Average recons loss: 3.1511, kld loss: 23.8590\n",
      "====> Test set recons loss: 31.4787, kld_loss: 49.5886\n",
      "Train Epoch: 73 [0/110 (0%)]\tRecons Loss: 87.999466; KLD Loss: 87.999466\n",
      "Train Epoch: 73 [64/110 (50%)]\tRecons Loss: 81.059715; KLD Loss: 81.059715\n",
      "====> Epoch: 73 Average recons loss: 3.0767, kld loss: 23.7582\n",
      "====> Test set recons loss: 32.1968, kld_loss: 49.4648\n",
      "Train Epoch: 74 [0/110 (0%)]\tRecons Loss: 89.237427; KLD Loss: 89.237427\n",
      "Train Epoch: 74 [64/110 (50%)]\tRecons Loss: 82.743561; KLD Loss: 82.743561\n",
      "====> Epoch: 74 Average recons loss: 3.0366, kld loss: 23.5842\n",
      "====> Epoch: 74 Eta is now 0.1738 due to sufficient recons results\n",
      "====> Test set recons loss: 31.2178, kld_loss: 49.1139\n",
      "Train Epoch: 75 [0/110 (0%)]\tRecons Loss: 87.719864; KLD Loss: 87.719864\n",
      "Train Epoch: 75 [64/110 (50%)]\tRecons Loss: 80.512154; KLD Loss: 80.512154\n",
      "====> Epoch: 75 Average recons loss: 2.9640, kld loss: 23.5233\n",
      "====> Test set recons loss: 32.7121, kld_loss: 48.6200\n",
      "Train Epoch: 76 [0/110 (0%)]\tRecons Loss: 83.325012; KLD Loss: 83.325012\n",
      "Train Epoch: 76 [64/110 (50%)]\tRecons Loss: 80.372772; KLD Loss: 80.372772\n",
      "====> Epoch: 76 Average recons loss: 3.0559, kld loss: 23.1407\n",
      "====> Test set recons loss: 31.5359, kld_loss: 47.9137\n",
      "Train Epoch: 77 [0/110 (0%)]\tRecons Loss: 82.798996; KLD Loss: 82.798996\n",
      "Train Epoch: 77 [64/110 (50%)]\tRecons Loss: 87.549774; KLD Loss: 87.549774\n",
      "====> Epoch: 77 Average recons loss: 2.9511, kld loss: 22.8523\n",
      "====> Test set recons loss: 31.7001, kld_loss: 46.7282\n",
      "Train Epoch: 78 [0/110 (0%)]\tRecons Loss: 87.312027; KLD Loss: 87.312027\n",
      "Train Epoch: 78 [64/110 (50%)]\tRecons Loss: 83.710976; KLD Loss: 83.710976\n",
      "====> Epoch: 78 Average recons loss: 3.0794, kld loss: 22.3415\n",
      "====> Test set recons loss: 33.2334, kld_loss: 46.0864\n",
      "Train Epoch: 79 [0/110 (0%)]\tRecons Loss: 80.406845; KLD Loss: 80.406845\n",
      "Train Epoch: 79 [64/110 (50%)]\tRecons Loss: 84.729752; KLD Loss: 84.729752\n",
      "====> Epoch: 79 Average recons loss: 3.0608, kld loss: 22.0072\n",
      "====> Test set recons loss: 31.4245, kld_loss: 45.4564\n",
      "Train Epoch: 80 [0/110 (0%)]\tRecons Loss: 88.792084; KLD Loss: 88.792084\n",
      "Train Epoch: 80 [64/110 (50%)]\tRecons Loss: 83.803314; KLD Loss: 83.803314\n",
      "====> Epoch: 80 Average recons loss: 3.0631, kld loss: 21.7819\n",
      "====> Test set recons loss: 33.9160, kld_loss: 44.9762\n",
      "Train Epoch: 81 [0/110 (0%)]\tRecons Loss: 89.317062; KLD Loss: 89.317062\n",
      "Train Epoch: 81 [64/110 (50%)]\tRecons Loss: 81.834152; KLD Loss: 81.834152\n",
      "====> Epoch: 81 Average recons loss: 2.9756, kld loss: 21.5774\n",
      "====> Test set recons loss: 32.1776, kld_loss: 44.8473\n",
      "Train Epoch: 82 [0/110 (0%)]\tRecons Loss: 83.900299; KLD Loss: 83.900299\n",
      "Train Epoch: 82 [64/110 (50%)]\tRecons Loss: 85.047607; KLD Loss: 85.047607\n",
      "====> Epoch: 82 Average recons loss: 3.1263, kld loss: 21.3982\n",
      "====> Test set recons loss: 33.2170, kld_loss: 44.3425\n",
      "Train Epoch: 83 [0/110 (0%)]\tRecons Loss: 76.611908; KLD Loss: 76.611908\n",
      "Train Epoch: 83 [64/110 (50%)]\tRecons Loss: 86.182976; KLD Loss: 86.182976\n",
      "====> Epoch: 83 Average recons loss: 3.0351, kld loss: 21.3459\n",
      "====> Test set recons loss: 31.6008, kld_loss: 44.3215\n",
      "Train Epoch: 84 [0/110 (0%)]\tRecons Loss: 76.469559; KLD Loss: 76.469559\n",
      "Train Epoch: 84 [64/110 (50%)]\tRecons Loss: 90.810257; KLD Loss: 90.810257\n",
      "====> Epoch: 84 Average recons loss: 3.0031, kld loss: 21.3484\n",
      "====> Epoch: 84 Eta is now 0.2231 due to sufficient recons results\n",
      "====> Test set recons loss: 32.6682, kld_loss: 44.2847\n",
      "Train Epoch: 85 [0/110 (0%)]\tRecons Loss: 79.794922; KLD Loss: 79.794922\n",
      "Train Epoch: 85 [64/110 (50%)]\tRecons Loss: 79.756790; KLD Loss: 79.756790\n",
      "====> Epoch: 85 Average recons loss: 2.9051, kld loss: 21.3245\n",
      "====> Test set recons loss: 32.7128, kld_loss: 43.6637\n",
      "Train Epoch: 86 [0/110 (0%)]\tRecons Loss: 80.578491; KLD Loss: 80.578491\n",
      "Train Epoch: 86 [64/110 (50%)]\tRecons Loss: 79.798981; KLD Loss: 79.798981\n",
      "====> Epoch: 86 Average recons loss: 2.8930, kld loss: 20.9518\n",
      "====> Test set recons loss: 31.6639, kld_loss: 42.6703\n",
      "Train Epoch: 87 [0/110 (0%)]\tRecons Loss: 81.600922; KLD Loss: 81.600922\n",
      "Train Epoch: 87 [64/110 (50%)]\tRecons Loss: 79.537109; KLD Loss: 79.537109\n",
      "====> Epoch: 87 Average recons loss: 3.0000, kld loss: 20.5262\n",
      "====> Test set recons loss: 33.8634, kld_loss: 42.0435\n",
      "Train Epoch: 88 [0/110 (0%)]\tRecons Loss: 74.160461; KLD Loss: 74.160461\n",
      "Train Epoch: 88 [64/110 (50%)]\tRecons Loss: 85.142494; KLD Loss: 85.142494\n",
      "====> Epoch: 88 Average recons loss: 2.9638, kld loss: 20.1515\n",
      "====> Test set recons loss: 33.8627, kld_loss: 41.3878\n",
      "Train Epoch: 89 [0/110 (0%)]\tRecons Loss: 80.826736; KLD Loss: 80.826736\n",
      "Train Epoch: 89 [64/110 (50%)]\tRecons Loss: 78.958038; KLD Loss: 78.958038\n",
      "====> Epoch: 89 Average recons loss: 3.0178, kld loss: 19.8457\n",
      "====> Test set recons loss: 33.3554, kld_loss: 40.4149\n",
      "Train Epoch: 90 [0/110 (0%)]\tRecons Loss: 88.280060; KLD Loss: 88.280060\n",
      "Train Epoch: 90 [64/110 (50%)]\tRecons Loss: 79.910751; KLD Loss: 79.910751\n",
      "====> Epoch: 90 Average recons loss: 3.0154, kld loss: 19.5533\n",
      "====> Test set recons loss: 34.3305, kld_loss: 40.3456\n",
      "Train Epoch: 91 [0/110 (0%)]\tRecons Loss: 85.210632; KLD Loss: 85.210632\n",
      "Train Epoch: 91 [64/110 (50%)]\tRecons Loss: 89.889130; KLD Loss: 89.889130\n",
      "====> Epoch: 91 Average recons loss: 3.1305, kld loss: 19.4510\n",
      "====> Test set recons loss: 32.6452, kld_loss: 39.7399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 92 [0/110 (0%)]\tRecons Loss: 76.795212; KLD Loss: 76.795212\n",
      "Train Epoch: 92 [64/110 (50%)]\tRecons Loss: 89.420975; KLD Loss: 89.420975\n",
      "====> Epoch: 92 Average recons loss: 3.0202, kld loss: 19.2626\n",
      "====> Test set recons loss: 33.4273, kld_loss: 39.8185\n",
      "Train Epoch: 93 [0/110 (0%)]\tRecons Loss: 81.173935; KLD Loss: 81.173935\n",
      "Train Epoch: 93 [64/110 (50%)]\tRecons Loss: 80.891006; KLD Loss: 80.891006\n",
      "====> Epoch: 93 Average recons loss: 2.9537, kld loss: 19.2875\n",
      "====> Test set recons loss: 33.0197, kld_loss: 39.4449\n",
      "Train Epoch: 94 [0/110 (0%)]\tRecons Loss: 79.846756; KLD Loss: 79.846756\n",
      "Train Epoch: 94 [64/110 (50%)]\tRecons Loss: 85.998634; KLD Loss: 85.998634\n",
      "====> Epoch: 94 Average recons loss: 2.9361, kld loss: 19.1298\n",
      "====> Epoch: 94 Eta is now 0.2865 due to sufficient recons results\n",
      "====> Test set recons loss: 32.0820, kld_loss: 39.0583\n",
      "Train Epoch: 95 [0/110 (0%)]\tRecons Loss: 80.799835; KLD Loss: 80.799835\n",
      "Train Epoch: 95 [64/110 (50%)]\tRecons Loss: 86.487244; KLD Loss: 86.487244\n",
      "====> Epoch: 95 Average recons loss: 2.9726, kld loss: 18.7877\n",
      "====> Test set recons loss: 34.1469, kld_loss: 38.1966\n",
      "Train Epoch: 96 [0/110 (0%)]\tRecons Loss: 83.069565; KLD Loss: 83.069565\n",
      "Train Epoch: 96 [64/110 (50%)]\tRecons Loss: 82.507332; KLD Loss: 82.507332\n",
      "====> Epoch: 96 Average recons loss: 2.9491, kld loss: 18.3757\n",
      "====> Test set recons loss: 34.2506, kld_loss: 36.9494\n",
      "Train Epoch: 97 [0/110 (0%)]\tRecons Loss: 79.777847; KLD Loss: 79.777847\n",
      "Train Epoch: 97 [64/110 (50%)]\tRecons Loss: 87.664909; KLD Loss: 87.664909\n",
      "====> Epoch: 97 Average recons loss: 3.1557, kld loss: 17.8594\n",
      "====> Test set recons loss: 36.4254, kld_loss: 36.4540\n",
      "Train Epoch: 98 [0/110 (0%)]\tRecons Loss: 86.698883; KLD Loss: 86.698883\n",
      "Train Epoch: 98 [64/110 (50%)]\tRecons Loss: 83.801422; KLD Loss: 83.801422\n",
      "====> Epoch: 98 Average recons loss: 3.1143, kld loss: 17.6415\n",
      "====> Test set recons loss: 35.9125, kld_loss: 35.6663\n",
      "Train Epoch: 99 [0/110 (0%)]\tRecons Loss: 88.241669; KLD Loss: 88.241669\n",
      "Train Epoch: 99 [64/110 (50%)]\tRecons Loss: 77.131714; KLD Loss: 77.131714\n",
      "====> Epoch: 99 Average recons loss: 3.0651, kld loss: 17.4395\n",
      "====> Test set recons loss: 34.6496, kld_loss: 35.2562\n",
      "Train Epoch: 100 [0/110 (0%)]\tRecons Loss: 87.611221; KLD Loss: 87.611221\n",
      "Train Epoch: 100 [64/110 (50%)]\tRecons Loss: 91.320770; KLD Loss: 91.320770\n",
      "====> Epoch: 100 Average recons loss: 3.1788, kld loss: 17.0691\n",
      "====> Test set recons loss: 34.3366, kld_loss: 34.8754\n",
      "Train Epoch: 101 [0/110 (0%)]\tRecons Loss: 73.769608; KLD Loss: 73.769608\n",
      "Train Epoch: 101 [64/110 (50%)]\tRecons Loss: 82.670242; KLD Loss: 82.670242\n",
      "====> Epoch: 101 Average recons loss: 3.2319, kld loss: 16.8993\n",
      "====> Test set recons loss: 35.8773, kld_loss: 34.3337\n",
      "Train Epoch: 102 [0/110 (0%)]\tRecons Loss: 87.192184; KLD Loss: 87.192184\n",
      "Train Epoch: 102 [64/110 (50%)]\tRecons Loss: 89.638016; KLD Loss: 89.638016\n",
      "====> Epoch: 102 Average recons loss: 3.2355, kld loss: 16.8214\n",
      "====> Test set recons loss: 34.6320, kld_loss: 34.4398\n",
      "Train Epoch: 103 [0/110 (0%)]\tRecons Loss: 85.959358; KLD Loss: 85.959358\n",
      "Train Epoch: 103 [64/110 (50%)]\tRecons Loss: 84.463531; KLD Loss: 84.463531\n",
      "====> Epoch: 103 Average recons loss: 3.0466, kld loss: 16.8856\n",
      "====> Test set recons loss: 34.2447, kld_loss: 34.2291\n",
      "Train Epoch: 104 [0/110 (0%)]\tRecons Loss: 89.106522; KLD Loss: 89.106522\n",
      "Train Epoch: 104 [64/110 (50%)]\tRecons Loss: 83.786942; KLD Loss: 83.786942\n",
      "====> Epoch: 104 Average recons loss: 3.1396, kld loss: 16.8284\n",
      "====> Epoch: 104 Eta is now 0.3679 due to sufficient recons results\n",
      "====> Test set recons loss: 35.0196, kld_loss: 34.3372\n",
      "Train Epoch: 105 [0/110 (0%)]\tRecons Loss: 85.922470; KLD Loss: 85.922470\n",
      "Train Epoch: 105 [64/110 (50%)]\tRecons Loss: 83.251480; KLD Loss: 83.251480\n",
      "====> Epoch: 105 Average recons loss: 3.1391, kld loss: 16.6276\n",
      "====> Test set recons loss: 35.8420, kld_loss: 33.3200\n",
      "Train Epoch: 106 [0/110 (0%)]\tRecons Loss: 85.100494; KLD Loss: 85.100494\n",
      "Train Epoch: 106 [64/110 (50%)]\tRecons Loss: 89.759331; KLD Loss: 89.759331\n",
      "====> Epoch: 106 Average recons loss: 3.1328, kld loss: 16.2340\n",
      "====> Test set recons loss: 35.9726, kld_loss: 32.1879\n",
      "Train Epoch: 107 [0/110 (0%)]\tRecons Loss: 79.619621; KLD Loss: 79.619621\n",
      "Train Epoch: 107 [64/110 (50%)]\tRecons Loss: 94.208420; KLD Loss: 94.208420\n",
      "====> Epoch: 107 Average recons loss: 3.1502, kld loss: 15.8198\n",
      "====> Test set recons loss: 35.1783, kld_loss: 31.3203\n",
      "Train Epoch: 108 [0/110 (0%)]\tRecons Loss: 94.359558; KLD Loss: 94.359558\n",
      "Train Epoch: 108 [64/110 (50%)]\tRecons Loss: 92.979218; KLD Loss: 92.979218\n",
      "====> Epoch: 108 Average recons loss: 3.3470, kld loss: 15.3322\n",
      "====> Test set recons loss: 35.0464, kld_loss: 31.1522\n",
      "Train Epoch: 109 [0/110 (0%)]\tRecons Loss: 87.100479; KLD Loss: 87.100479\n",
      "Train Epoch: 109 [64/110 (50%)]\tRecons Loss: 94.744141; KLD Loss: 94.744141\n",
      "====> Epoch: 109 Average recons loss: 3.4095, kld loss: 15.1583\n",
      "====> Test set recons loss: 35.4951, kld_loss: 30.8891\n",
      "Train Epoch: 110 [0/110 (0%)]\tRecons Loss: 85.113663; KLD Loss: 85.113663\n",
      "Train Epoch: 110 [64/110 (50%)]\tRecons Loss: 96.064713; KLD Loss: 96.064713\n",
      "====> Epoch: 110 Average recons loss: 3.4211, kld loss: 15.1081\n",
      "====> Test set recons loss: 34.4462, kld_loss: 30.1432\n",
      "Train Epoch: 111 [0/110 (0%)]\tRecons Loss: 89.510765; KLD Loss: 89.510765\n",
      "Train Epoch: 111 [64/110 (50%)]\tRecons Loss: 93.691971; KLD Loss: 93.691971\n",
      "====> Epoch: 111 Average recons loss: 3.2550, kld loss: 14.9716\n",
      "====> Test set recons loss: 35.8797, kld_loss: 29.9483\n",
      "Train Epoch: 112 [0/110 (0%)]\tRecons Loss: 87.336807; KLD Loss: 87.336807\n",
      "Train Epoch: 112 [64/110 (50%)]\tRecons Loss: 88.262070; KLD Loss: 88.262070\n",
      "====> Epoch: 112 Average recons loss: 3.3447, kld loss: 14.7728\n",
      "====> Test set recons loss: 38.0258, kld_loss: 29.6291\n",
      "Train Epoch: 113 [0/110 (0%)]\tRecons Loss: 92.608147; KLD Loss: 92.608147\n",
      "Train Epoch: 113 [64/110 (50%)]\tRecons Loss: 87.743958; KLD Loss: 87.743958\n",
      "====> Epoch: 113 Average recons loss: 3.4093, kld loss: 14.6777\n",
      "====> Test set recons loss: 35.1400, kld_loss: 29.4441\n",
      "Train Epoch: 114 [0/110 (0%)]\tRecons Loss: 84.003662; KLD Loss: 84.003662\n",
      "Train Epoch: 114 [64/110 (50%)]\tRecons Loss: 92.744781; KLD Loss: 92.744781\n",
      "====> Epoch: 114 Average recons loss: 3.3275, kld loss: 14.5652\n",
      "====> Epoch: 114 Eta is now 0.4724 due to sufficient recons results\n",
      "====> Test set recons loss: 36.6196, kld_loss: 29.2281\n",
      "Train Epoch: 115 [0/110 (0%)]\tRecons Loss: 88.991943; KLD Loss: 88.991943\n",
      "Train Epoch: 115 [64/110 (50%)]\tRecons Loss: 95.208908; KLD Loss: 95.208908\n",
      "====> Epoch: 115 Average recons loss: 3.3490, kld loss: 14.3920\n",
      "====> Test set recons loss: 34.6801, kld_loss: 28.4133\n",
      "Train Epoch: 116 [0/110 (0%)]\tRecons Loss: 93.246498; KLD Loss: 93.246498\n",
      "Train Epoch: 116 [64/110 (50%)]\tRecons Loss: 100.745079; KLD Loss: 100.745079\n",
      "====> Epoch: 116 Average recons loss: 3.5611, kld loss: 14.0461\n",
      "====> Test set recons loss: 36.7970, kld_loss: 27.8994\n",
      "Train Epoch: 117 [0/110 (0%)]\tRecons Loss: 86.894852; KLD Loss: 86.894852\n",
      "Train Epoch: 117 [64/110 (50%)]\tRecons Loss: 112.874634; KLD Loss: 112.874634\n",
      "====> Epoch: 117 Average recons loss: 3.5920, kld loss: 13.7606\n",
      "====> Test set recons loss: 38.5917, kld_loss: 26.7581\n",
      "Train Epoch: 118 [0/110 (0%)]\tRecons Loss: 94.164566; KLD Loss: 94.164566\n",
      "Train Epoch: 118 [64/110 (50%)]\tRecons Loss: 101.583252; KLD Loss: 101.583252\n",
      "====> Epoch: 118 Average recons loss: 3.6303, kld loss: 13.4409\n",
      "====> Test set recons loss: 36.7068, kld_loss: 26.6777\n",
      "Train Epoch: 119 [0/110 (0%)]\tRecons Loss: 102.553543; KLD Loss: 102.553543\n",
      "Train Epoch: 119 [64/110 (50%)]\tRecons Loss: 98.402710; KLD Loss: 98.402710\n",
      "====> Epoch: 119 Average recons loss: 3.6861, kld loss: 13.2789\n",
      "====> Test set recons loss: 36.4380, kld_loss: 25.8513\n",
      "Train Epoch: 120 [0/110 (0%)]\tRecons Loss: 98.083435; KLD Loss: 98.083435\n",
      "Train Epoch: 120 [64/110 (50%)]\tRecons Loss: 103.256363; KLD Loss: 103.256363\n",
      "====> Epoch: 120 Average recons loss: 3.7380, kld loss: 13.0032\n",
      "====> Test set recons loss: 35.8280, kld_loss: 25.6966\n",
      "Train Epoch: 121 [0/110 (0%)]\tRecons Loss: 101.192657; KLD Loss: 101.192657\n",
      "Train Epoch: 121 [64/110 (50%)]\tRecons Loss: 99.552490; KLD Loss: 99.552490\n",
      "====> Epoch: 121 Average recons loss: 3.7348, kld loss: 12.8622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 35.9524, kld_loss: 25.5172\n",
      "Train Epoch: 122 [0/110 (0%)]\tRecons Loss: 100.518570; KLD Loss: 100.518570\n",
      "Train Epoch: 122 [64/110 (50%)]\tRecons Loss: 99.953278; KLD Loss: 99.953278\n",
      "====> Epoch: 122 Average recons loss: 3.7119, kld loss: 12.7791\n",
      "====> Test set recons loss: 38.1531, kld_loss: 25.0911\n",
      "Train Epoch: 123 [0/110 (0%)]\tRecons Loss: 103.563225; KLD Loss: 103.563225\n",
      "Train Epoch: 123 [64/110 (50%)]\tRecons Loss: 95.995728; KLD Loss: 95.995728\n",
      "====> Epoch: 123 Average recons loss: 3.7070, kld loss: 12.7635\n",
      "====> Test set recons loss: 39.3326, kld_loss: 24.8972\n",
      "Train Epoch: 124 [0/110 (0%)]\tRecons Loss: 99.435875; KLD Loss: 99.435875\n",
      "Train Epoch: 124 [64/110 (50%)]\tRecons Loss: 94.302933; KLD Loss: 94.302933\n",
      "====> Epoch: 124 Average recons loss: 3.5488, kld loss: 12.6803\n",
      "====> Epoch: 124 Eta is now 0.6065 due to sufficient recons results\n",
      "====> Test set recons loss: 35.7266, kld_loss: 24.7925\n",
      "Train Epoch: 125 [0/110 (0%)]\tRecons Loss: 108.938972; KLD Loss: 108.938972\n",
      "Train Epoch: 125 [64/110 (50%)]\tRecons Loss: 95.147774; KLD Loss: 95.147774\n",
      "====> Epoch: 125 Average recons loss: 3.8335, kld loss: 12.4146\n",
      "====> Test set recons loss: 36.8875, kld_loss: 23.8481\n",
      "Train Epoch: 126 [0/110 (0%)]\tRecons Loss: 99.230873; KLD Loss: 99.230873\n",
      "Train Epoch: 126 [64/110 (50%)]\tRecons Loss: 108.574310; KLD Loss: 108.574310\n",
      "====> Epoch: 126 Average recons loss: 3.7490, kld loss: 12.0507\n",
      "====> Test set recons loss: 38.3602, kld_loss: 22.8909\n",
      "Train Epoch: 127 [0/110 (0%)]\tRecons Loss: 107.792526; KLD Loss: 107.792526\n",
      "Train Epoch: 127 [64/110 (50%)]\tRecons Loss: 111.481728; KLD Loss: 111.481728\n",
      "====> Epoch: 127 Average recons loss: 3.8091, kld loss: 11.5729\n",
      "====> Test set recons loss: 35.3914, kld_loss: 22.2833\n",
      "Train Epoch: 128 [0/110 (0%)]\tRecons Loss: 103.074821; KLD Loss: 103.074821\n",
      "Train Epoch: 128 [64/110 (50%)]\tRecons Loss: 105.737534; KLD Loss: 105.737534\n",
      "====> Epoch: 128 Average recons loss: 4.1233, kld loss: 11.2881\n",
      "====> Test set recons loss: 38.2472, kld_loss: 21.8438\n",
      "Train Epoch: 129 [0/110 (0%)]\tRecons Loss: 104.299072; KLD Loss: 104.299072\n",
      "Train Epoch: 129 [64/110 (50%)]\tRecons Loss: 123.358551; KLD Loss: 123.358551\n",
      "====> Epoch: 129 Average recons loss: 4.1309, kld loss: 11.1385\n",
      "====> Test set recons loss: 39.2848, kld_loss: 21.2186\n",
      "Train Epoch: 130 [0/110 (0%)]\tRecons Loss: 107.385788; KLD Loss: 107.385788\n",
      "Train Epoch: 130 [64/110 (50%)]\tRecons Loss: 121.764740; KLD Loss: 121.764740\n",
      "====> Epoch: 130 Average recons loss: 3.9342, kld loss: 10.8630\n",
      "====> Test set recons loss: 38.6687, kld_loss: 20.1442\n",
      "Train Epoch: 131 [0/110 (0%)]\tRecons Loss: 109.202255; KLD Loss: 109.202255\n",
      "Train Epoch: 131 [64/110 (50%)]\tRecons Loss: 112.427498; KLD Loss: 112.427498\n",
      "====> Epoch: 131 Average recons loss: 4.1256, kld loss: 10.5044\n",
      "====> Test set recons loss: 37.3524, kld_loss: 20.3189\n",
      "Train Epoch: 132 [0/110 (0%)]\tRecons Loss: 106.020050; KLD Loss: 106.020050\n",
      "Train Epoch: 132 [64/110 (50%)]\tRecons Loss: 120.281662; KLD Loss: 120.281662\n",
      "====> Epoch: 132 Average recons loss: 4.0552, kld loss: 10.3083\n",
      "====> Test set recons loss: 37.8367, kld_loss: 19.5566\n",
      "Train Epoch: 133 [0/110 (0%)]\tRecons Loss: 106.208038; KLD Loss: 106.208038\n",
      "Train Epoch: 133 [64/110 (50%)]\tRecons Loss: 129.731873; KLD Loss: 129.731873\n",
      "====> Epoch: 133 Average recons loss: 4.4880, kld loss: 10.2222\n",
      "====> Test set recons loss: 36.3073, kld_loss: 20.3568\n",
      "Train Epoch: 134 [0/110 (0%)]\tRecons Loss: 115.482796; KLD Loss: 115.482796\n",
      "Train Epoch: 134 [64/110 (50%)]\tRecons Loss: 109.598770; KLD Loss: 109.598770\n",
      "====> Epoch: 134 Average recons loss: 4.3165, kld loss: 10.3725\n",
      "====> Epoch: 134 Eta is now 0.7788 due to sufficient recons results\n",
      "====> Test set recons loss: 39.2720, kld_loss: 19.6269\n",
      "Train Epoch: 135 [0/110 (0%)]\tRecons Loss: 105.565010; KLD Loss: 105.565010\n",
      "Train Epoch: 135 [64/110 (50%)]\tRecons Loss: 129.943024; KLD Loss: 129.943024\n",
      "====> Epoch: 135 Average recons loss: 4.1298, kld loss: 10.0761\n",
      "====> Test set recons loss: 38.8864, kld_loss: 18.0330\n",
      "Train Epoch: 136 [0/110 (0%)]\tRecons Loss: 119.053543; KLD Loss: 119.053543\n",
      "Train Epoch: 136 [64/110 (50%)]\tRecons Loss: 129.831879; KLD Loss: 129.831879\n",
      "====> Epoch: 136 Average recons loss: 4.3917, kld loss: 9.6052\n",
      "====> Test set recons loss: 43.1834, kld_loss: 17.7801\n",
      "Train Epoch: 137 [0/110 (0%)]\tRecons Loss: 132.653137; KLD Loss: 132.653137\n",
      "Train Epoch: 137 [64/110 (50%)]\tRecons Loss: 138.135605; KLD Loss: 138.135605\n",
      "====> Epoch: 137 Average recons loss: 4.7183, kld loss: 9.3584\n",
      "====> Test set recons loss: 40.5012, kld_loss: 17.3618\n",
      "Train Epoch: 138 [0/110 (0%)]\tRecons Loss: 133.062042; KLD Loss: 133.062042\n",
      "Train Epoch: 138 [64/110 (50%)]\tRecons Loss: 121.713043; KLD Loss: 121.713043\n",
      "====> Epoch: 138 Average recons loss: 4.5924, kld loss: 9.2774\n",
      "====> Test set recons loss: 41.6848, kld_loss: 17.2992\n",
      "Train Epoch: 139 [0/110 (0%)]\tRecons Loss: 122.568642; KLD Loss: 122.568642\n",
      "Train Epoch: 139 [64/110 (50%)]\tRecons Loss: 155.663040; KLD Loss: 155.663040\n",
      "====> Epoch: 139 Average recons loss: 4.6303, kld loss: 8.9869\n",
      "====> Test set recons loss: 40.2946, kld_loss: 16.9406\n",
      "Train Epoch: 140 [0/110 (0%)]\tRecons Loss: 123.972519; KLD Loss: 123.972519\n",
      "Train Epoch: 140 [64/110 (50%)]\tRecons Loss: 135.500793; KLD Loss: 135.500793\n",
      "====> Epoch: 140 Average recons loss: 4.9402, kld loss: 8.6941\n",
      "====> Test set recons loss: 36.7459, kld_loss: 16.4218\n",
      "Train Epoch: 141 [0/110 (0%)]\tRecons Loss: 137.849258; KLD Loss: 137.849258\n",
      "Train Epoch: 141 [64/110 (50%)]\tRecons Loss: 140.907700; KLD Loss: 140.907700\n",
      "====> Epoch: 141 Average recons loss: 5.2333, kld loss: 8.7309\n",
      "====> Test set recons loss: 40.6258, kld_loss: 16.3156\n",
      "Train Epoch: 142 [0/110 (0%)]\tRecons Loss: 120.070038; KLD Loss: 120.070038\n",
      "Train Epoch: 142 [64/110 (50%)]\tRecons Loss: 137.603851; KLD Loss: 137.603851\n",
      "====> Epoch: 142 Average recons loss: 5.2615, kld loss: 8.6935\n",
      "====> Test set recons loss: 40.1686, kld_loss: 17.1301\n",
      "Train Epoch: 143 [0/110 (0%)]\tRecons Loss: 108.250259; KLD Loss: 108.250259\n",
      "Train Epoch: 143 [64/110 (50%)]\tRecons Loss: 130.608368; KLD Loss: 130.608368\n",
      "====> Epoch: 143 Average recons loss: 4.7488, kld loss: 8.8943\n",
      "====> Test set recons loss: 40.3284, kld_loss: 16.4584\n",
      "Train Epoch: 144 [0/110 (0%)]\tRecons Loss: 131.939575; KLD Loss: 131.939575\n",
      "Train Epoch: 144 [64/110 (50%)]\tRecons Loss: 131.760468; KLD Loss: 131.760468\n",
      "====> Epoch: 144 Average recons loss: 4.7900, kld loss: 8.6317\n",
      "====> Epoch: 144 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 38.8159, kld_loss: 15.3515\n",
      "Train Epoch: 145 [0/110 (0%)]\tRecons Loss: 119.532654; KLD Loss: 119.532654\n",
      "Train Epoch: 145 [64/110 (50%)]\tRecons Loss: 147.046082; KLD Loss: 147.046082\n",
      "====> Epoch: 145 Average recons loss: 5.0147, kld loss: 8.3180\n",
      "====> Test set recons loss: 38.7671, kld_loss: 14.6368\n",
      "Train Epoch: 146 [0/110 (0%)]\tRecons Loss: 140.226120; KLD Loss: 140.226120\n",
      "Train Epoch: 146 [64/110 (50%)]\tRecons Loss: 152.411682; KLD Loss: 152.411682\n",
      "====> Epoch: 146 Average recons loss: 5.1034, kld loss: 7.8854\n",
      "====> Test set recons loss: 42.2797, kld_loss: 13.5990\n",
      "Train Epoch: 147 [0/110 (0%)]\tRecons Loss: 128.371094; KLD Loss: 128.371094\n",
      "Train Epoch: 147 [64/110 (50%)]\tRecons Loss: 171.469299; KLD Loss: 171.469299\n",
      "====> Epoch: 147 Average recons loss: 5.5182, kld loss: 7.5141\n",
      "====> Test set recons loss: 36.5716, kld_loss: 13.3869\n",
      "Train Epoch: 148 [0/110 (0%)]\tRecons Loss: 148.466888; KLD Loss: 148.466888\n",
      "Train Epoch: 148 [64/110 (50%)]\tRecons Loss: 145.873245; KLD Loss: 145.873245\n",
      "====> Epoch: 148 Average recons loss: 5.5903, kld loss: 7.2535\n",
      "====> Test set recons loss: 41.0592, kld_loss: 12.4461\n",
      "Train Epoch: 149 [0/110 (0%)]\tRecons Loss: 144.491821; KLD Loss: 144.491821\n",
      "Train Epoch: 149 [64/110 (50%)]\tRecons Loss: 168.010925; KLD Loss: 168.010925\n",
      "====> Epoch: 149 Average recons loss: 5.5836, kld loss: 6.9179\n",
      "====> Test set recons loss: 39.6727, kld_loss: 11.7107\n",
      "Train Epoch: 150 [0/110 (0%)]\tRecons Loss: 152.784302; KLD Loss: 152.784302\n",
      "Train Epoch: 150 [64/110 (50%)]\tRecons Loss: 136.401642; KLD Loss: 136.401642\n",
      "====> Epoch: 150 Average recons loss: 5.7618, kld loss: 6.5971\n",
      "====> Test set recons loss: 37.8901, kld_loss: 11.1405\n",
      "Train Epoch: 151 [0/110 (0%)]\tRecons Loss: 168.682465; KLD Loss: 168.682465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 151 [64/110 (50%)]\tRecons Loss: 167.141052; KLD Loss: 167.141052\n",
      "====> Epoch: 151 Average recons loss: 6.0472, kld loss: 6.4058\n",
      "====> Test set recons loss: 40.0798, kld_loss: 11.2308\n",
      "Train Epoch: 152 [0/110 (0%)]\tRecons Loss: 153.011917; KLD Loss: 153.011917\n",
      "Train Epoch: 152 [64/110 (50%)]\tRecons Loss: 177.452499; KLD Loss: 177.452499\n",
      "====> Epoch: 152 Average recons loss: 6.2853, kld loss: 6.2894\n",
      "====> Test set recons loss: 39.6479, kld_loss: 11.1621\n",
      "Train Epoch: 153 [0/110 (0%)]\tRecons Loss: 173.128845; KLD Loss: 173.128845\n",
      "Train Epoch: 153 [64/110 (50%)]\tRecons Loss: 163.770950; KLD Loss: 163.770950\n",
      "====> Epoch: 153 Average recons loss: 5.9316, kld loss: 6.3025\n",
      "====> Test set recons loss: 40.0136, kld_loss: 10.5009\n",
      "Train Epoch: 154 [0/110 (0%)]\tRecons Loss: 159.664383; KLD Loss: 159.664383\n",
      "Train Epoch: 154 [64/110 (50%)]\tRecons Loss: 177.662140; KLD Loss: 177.662140\n",
      "====> Epoch: 154 Average recons loss: 5.9754, kld loss: 5.9924\n",
      "====> Epoch: 154 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 37.7767, kld_loss: 10.9044\n",
      "Train Epoch: 155 [0/110 (0%)]\tRecons Loss: 168.051971; KLD Loss: 168.051971\n",
      "Train Epoch: 155 [64/110 (50%)]\tRecons Loss: 177.116943; KLD Loss: 177.116943\n",
      "====> Epoch: 155 Average recons loss: 5.9735, kld loss: 5.9522\n",
      "====> Test set recons loss: 39.8249, kld_loss: 9.1550\n",
      "Train Epoch: 156 [0/110 (0%)]\tRecons Loss: 165.133179; KLD Loss: 165.133179\n",
      "Train Epoch: 156 [64/110 (50%)]\tRecons Loss: 161.419739; KLD Loss: 161.419739\n",
      "====> Epoch: 156 Average recons loss: 6.3868, kld loss: 5.8810\n",
      "====> Test set recons loss: 39.4549, kld_loss: 10.1456\n",
      "Train Epoch: 157 [0/110 (0%)]\tRecons Loss: 157.259155; KLD Loss: 157.259155\n",
      "Train Epoch: 157 [64/110 (50%)]\tRecons Loss: 160.102264; KLD Loss: 160.102264\n",
      "====> Epoch: 157 Average recons loss: 5.9355, kld loss: 5.6749\n",
      "====> Test set recons loss: 42.4503, kld_loss: 10.0729\n",
      "Train Epoch: 158 [0/110 (0%)]\tRecons Loss: 166.089935; KLD Loss: 166.089935\n",
      "Train Epoch: 158 [64/110 (50%)]\tRecons Loss: 176.289871; KLD Loss: 176.289871\n",
      "====> Epoch: 158 Average recons loss: 6.4425, kld loss: 5.6266\n",
      "====> Test set recons loss: 39.0823, kld_loss: 9.8659\n",
      "Train Epoch: 159 [0/110 (0%)]\tRecons Loss: 150.410065; KLD Loss: 150.410065\n",
      "Train Epoch: 159 [64/110 (50%)]\tRecons Loss: 189.551926; KLD Loss: 189.551926\n",
      "====> Epoch: 159 Average recons loss: 6.3073, kld loss: 5.7707\n",
      "====> Test set recons loss: 43.0708, kld_loss: 10.1673\n",
      "Train Epoch: 160 [0/110 (0%)]\tRecons Loss: 167.651688; KLD Loss: 167.651688\n",
      "Train Epoch: 160 [64/110 (50%)]\tRecons Loss: 158.623749; KLD Loss: 158.623749\n",
      "====> Epoch: 160 Average recons loss: 5.8341, kld loss: 5.7267\n",
      "====> Test set recons loss: 41.8619, kld_loss: 8.5996\n",
      "Train Epoch: 161 [0/110 (0%)]\tRecons Loss: 168.902267; KLD Loss: 168.902267\n",
      "Train Epoch: 161 [64/110 (50%)]\tRecons Loss: 137.047394; KLD Loss: 137.047394\n",
      "====> Epoch: 161 Average recons loss: 5.9596, kld loss: 5.7293\n",
      "====> Test set recons loss: 39.8082, kld_loss: 9.7636\n",
      "Train Epoch: 162 [0/110 (0%)]\tRecons Loss: 162.380798; KLD Loss: 162.380798\n",
      "Train Epoch: 162 [64/110 (50%)]\tRecons Loss: 155.311111; KLD Loss: 155.311111\n",
      "====> Epoch: 162 Average recons loss: 5.9738, kld loss: 5.6545\n",
      "====> Test set recons loss: 37.6714, kld_loss: 9.7537\n",
      "Train Epoch: 163 [0/110 (0%)]\tRecons Loss: 171.755768; KLD Loss: 171.755768\n",
      "Train Epoch: 163 [64/110 (50%)]\tRecons Loss: 161.311935; KLD Loss: 161.311935\n",
      "====> Epoch: 163 Average recons loss: 5.8011, kld loss: 5.6755\n",
      "====> Test set recons loss: 40.5038, kld_loss: 8.6392\n",
      "Train Epoch: 164 [0/110 (0%)]\tRecons Loss: 157.372192; KLD Loss: 157.372192\n",
      "Train Epoch: 164 [64/110 (50%)]\tRecons Loss: 174.025101; KLD Loss: 174.025101\n",
      "====> Epoch: 164 Average recons loss: 5.7009, kld loss: 5.5493\n",
      "====> Epoch: 164 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.8644, kld_loss: 8.9968\n",
      "Train Epoch: 165 [0/110 (0%)]\tRecons Loss: 143.053894; KLD Loss: 143.053894\n",
      "Train Epoch: 165 [64/110 (50%)]\tRecons Loss: 150.286255; KLD Loss: 150.286255\n",
      "====> Epoch: 165 Average recons loss: 5.5644, kld loss: 5.4017\n",
      "====> Test set recons loss: 38.8287, kld_loss: 8.4688\n",
      "Train Epoch: 166 [0/110 (0%)]\tRecons Loss: 140.644424; KLD Loss: 140.644424\n",
      "Train Epoch: 166 [64/110 (50%)]\tRecons Loss: 162.209488; KLD Loss: 162.209488\n",
      "====> Epoch: 166 Average recons loss: 5.5246, kld loss: 5.1898\n",
      "====> Test set recons loss: 40.3700, kld_loss: 8.1146\n",
      "Train Epoch: 167 [0/110 (0%)]\tRecons Loss: 144.633972; KLD Loss: 144.633972\n",
      "Train Epoch: 167 [64/110 (50%)]\tRecons Loss: 153.779099; KLD Loss: 153.779099\n",
      "====> Epoch: 167 Average recons loss: 5.5253, kld loss: 5.2296\n",
      "====> Test set recons loss: 37.4697, kld_loss: 8.5810\n",
      "Train Epoch: 168 [0/110 (0%)]\tRecons Loss: 147.849579; KLD Loss: 147.849579\n",
      "Train Epoch: 168 [64/110 (50%)]\tRecons Loss: 152.598404; KLD Loss: 152.598404\n",
      "====> Epoch: 168 Average recons loss: 5.2906, kld loss: 5.2040\n",
      "====> Test set recons loss: 37.7746, kld_loss: 7.5999\n",
      "Train Epoch: 169 [0/110 (0%)]\tRecons Loss: 154.249237; KLD Loss: 154.249237\n",
      "Train Epoch: 169 [64/110 (50%)]\tRecons Loss: 181.273163; KLD Loss: 181.273163\n",
      "====> Epoch: 169 Average recons loss: 5.9513, kld loss: 4.9657\n",
      "====> Test set recons loss: 41.4410, kld_loss: 8.5718\n",
      "Train Epoch: 170 [0/110 (0%)]\tRecons Loss: 164.645416; KLD Loss: 164.645416\n",
      "Train Epoch: 170 [64/110 (50%)]\tRecons Loss: 148.412872; KLD Loss: 148.412872\n",
      "====> Epoch: 170 Average recons loss: 5.3005, kld loss: 5.0686\n",
      "====> Test set recons loss: 41.8710, kld_loss: 6.7326\n",
      "Train Epoch: 171 [0/110 (0%)]\tRecons Loss: 165.853851; KLD Loss: 165.853851\n",
      "Train Epoch: 171 [64/110 (50%)]\tRecons Loss: 156.800232; KLD Loss: 156.800232\n",
      "====> Epoch: 171 Average recons loss: 5.6935, kld loss: 4.7035\n",
      "====> Test set recons loss: 39.1957, kld_loss: 7.6633\n",
      "Train Epoch: 172 [0/110 (0%)]\tRecons Loss: 158.966797; KLD Loss: 158.966797\n",
      "Train Epoch: 172 [64/110 (50%)]\tRecons Loss: 152.648315; KLD Loss: 152.648315\n",
      "====> Epoch: 172 Average recons loss: 5.4595, kld loss: 4.5730\n",
      "====> Test set recons loss: 42.6055, kld_loss: 6.8536\n",
      "Train Epoch: 173 [0/110 (0%)]\tRecons Loss: 147.487061; KLD Loss: 147.487061\n",
      "Train Epoch: 173 [64/110 (50%)]\tRecons Loss: 156.386475; KLD Loss: 156.386475\n",
      "====> Epoch: 173 Average recons loss: 5.5933, kld loss: 4.5113\n",
      "====> Test set recons loss: 40.0983, kld_loss: 6.5979\n",
      "Train Epoch: 174 [0/110 (0%)]\tRecons Loss: 155.955933; KLD Loss: 155.955933\n",
      "Train Epoch: 174 [64/110 (50%)]\tRecons Loss: 175.587082; KLD Loss: 175.587082\n",
      "====> Epoch: 174 Average recons loss: 5.8332, kld loss: 4.3515\n",
      "====> Epoch: 174 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 43.7591, kld_loss: 7.2978\n",
      "Train Epoch: 175 [0/110 (0%)]\tRecons Loss: 145.427200; KLD Loss: 145.427200\n",
      "Train Epoch: 175 [64/110 (50%)]\tRecons Loss: 161.818710; KLD Loss: 161.818710\n",
      "====> Epoch: 175 Average recons loss: 5.7319, kld loss: 4.6220\n",
      "====> Test set recons loss: 41.3998, kld_loss: 6.2664\n",
      "Train Epoch: 176 [0/110 (0%)]\tRecons Loss: 159.164337; KLD Loss: 159.164337\n",
      "Train Epoch: 176 [64/110 (50%)]\tRecons Loss: 137.913971; KLD Loss: 137.913971\n",
      "====> Epoch: 176 Average recons loss: 5.5377, kld loss: 4.4939\n",
      "====> Test set recons loss: 41.9167, kld_loss: 6.9680\n",
      "Train Epoch: 177 [0/110 (0%)]\tRecons Loss: 141.310089; KLD Loss: 141.310089\n",
      "Train Epoch: 177 [64/110 (50%)]\tRecons Loss: 141.352325; KLD Loss: 141.352325\n",
      "====> Epoch: 177 Average recons loss: 5.5239, kld loss: 4.5350\n",
      "====> Test set recons loss: 38.6908, kld_loss: 7.0171\n",
      "Train Epoch: 178 [0/110 (0%)]\tRecons Loss: 138.113831; KLD Loss: 138.113831\n",
      "Train Epoch: 178 [64/110 (50%)]\tRecons Loss: 165.547638; KLD Loss: 165.547638\n",
      "====> Epoch: 178 Average recons loss: 5.3607, kld loss: 4.4027\n",
      "====> Test set recons loss: 36.4001, kld_loss: 6.3158\n",
      "Train Epoch: 179 [0/110 (0%)]\tRecons Loss: 140.210815; KLD Loss: 140.210815\n",
      "Train Epoch: 179 [64/110 (50%)]\tRecons Loss: 151.051697; KLD Loss: 151.051697\n",
      "====> Epoch: 179 Average recons loss: 5.3874, kld loss: 4.2081\n",
      "====> Test set recons loss: 41.6798, kld_loss: 6.4282\n",
      "Train Epoch: 180 [0/110 (0%)]\tRecons Loss: 146.626038; KLD Loss: 146.626038\n",
      "Train Epoch: 180 [64/110 (50%)]\tRecons Loss: 151.353546; KLD Loss: 151.353546\n",
      "====> Epoch: 180 Average recons loss: 5.0948, kld loss: 4.4663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 42.7647, kld_loss: 5.4456\n",
      "Train Epoch: 181 [0/110 (0%)]\tRecons Loss: 150.136795; KLD Loss: 150.136795\n",
      "Train Epoch: 181 [64/110 (50%)]\tRecons Loss: 179.609802; KLD Loss: 179.609802\n",
      "====> Epoch: 181 Average recons loss: 5.3623, kld loss: 4.1080\n",
      "====> Test set recons loss: 38.8867, kld_loss: 7.1072\n",
      "Train Epoch: 182 [0/110 (0%)]\tRecons Loss: 142.811325; KLD Loss: 142.811325\n",
      "Train Epoch: 182 [64/110 (50%)]\tRecons Loss: 148.710724; KLD Loss: 148.710724\n",
      "====> Epoch: 182 Average recons loss: 5.2942, kld loss: 4.3428\n",
      "====> Test set recons loss: 42.9883, kld_loss: 5.8508\n",
      "Train Epoch: 183 [0/110 (0%)]\tRecons Loss: 142.293427; KLD Loss: 142.293427\n",
      "Train Epoch: 183 [64/110 (50%)]\tRecons Loss: 160.743134; KLD Loss: 160.743134\n",
      "====> Epoch: 183 Average recons loss: 5.5837, kld loss: 4.3349\n",
      "====> Test set recons loss: 38.5147, kld_loss: 7.1850\n",
      "Train Epoch: 184 [0/110 (0%)]\tRecons Loss: 129.118546; KLD Loss: 129.118546\n",
      "Train Epoch: 184 [64/110 (50%)]\tRecons Loss: 148.050568; KLD Loss: 148.050568\n",
      "====> Epoch: 184 Average recons loss: 4.9617, kld loss: 4.3319\n",
      "====> Epoch: 184 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.6972, kld_loss: 5.3662\n",
      "Train Epoch: 185 [0/110 (0%)]\tRecons Loss: 181.628540; KLD Loss: 181.628540\n",
      "Train Epoch: 185 [64/110 (50%)]\tRecons Loss: 141.264099; KLD Loss: 141.264099\n",
      "====> Epoch: 185 Average recons loss: 5.7891, kld loss: 4.2562\n",
      "====> Test set recons loss: 45.6179, kld_loss: 6.5473\n",
      "Train Epoch: 186 [0/110 (0%)]\tRecons Loss: 138.095444; KLD Loss: 138.095444\n",
      "Train Epoch: 186 [64/110 (50%)]\tRecons Loss: 157.522186; KLD Loss: 157.522186\n",
      "====> Epoch: 186 Average recons loss: 5.1095, kld loss: 4.3500\n",
      "====> Test set recons loss: 41.2171, kld_loss: 5.6124\n",
      "Train Epoch: 187 [0/110 (0%)]\tRecons Loss: 146.485870; KLD Loss: 146.485870\n",
      "Train Epoch: 187 [64/110 (50%)]\tRecons Loss: 148.706146; KLD Loss: 148.706146\n",
      "====> Epoch: 187 Average recons loss: 5.0265, kld loss: 4.2238\n",
      "====> Test set recons loss: 41.8695, kld_loss: 6.7813\n",
      "Train Epoch: 188 [0/110 (0%)]\tRecons Loss: 145.717346; KLD Loss: 145.717346\n",
      "Train Epoch: 188 [64/110 (50%)]\tRecons Loss: 149.964478; KLD Loss: 149.964478\n",
      "====> Epoch: 188 Average recons loss: 5.2059, kld loss: 4.2618\n",
      "====> Test set recons loss: 40.5222, kld_loss: 6.0046\n",
      "Train Epoch: 189 [0/110 (0%)]\tRecons Loss: 163.274460; KLD Loss: 163.274460\n",
      "Train Epoch: 189 [64/110 (50%)]\tRecons Loss: 121.689636; KLD Loss: 121.689636\n",
      "====> Epoch: 189 Average recons loss: 5.3454, kld loss: 4.2857\n",
      "====> Test set recons loss: 41.5135, kld_loss: 6.1124\n",
      "Train Epoch: 190 [0/110 (0%)]\tRecons Loss: 137.534927; KLD Loss: 137.534927\n",
      "Train Epoch: 190 [64/110 (50%)]\tRecons Loss: 137.628220; KLD Loss: 137.628220\n",
      "====> Epoch: 190 Average recons loss: 4.9365, kld loss: 4.1865\n",
      "====> Test set recons loss: 40.4766, kld_loss: 5.4910\n",
      "Train Epoch: 191 [0/110 (0%)]\tRecons Loss: 123.026062; KLD Loss: 123.026062\n",
      "Train Epoch: 191 [64/110 (50%)]\tRecons Loss: 149.772888; KLD Loss: 149.772888\n",
      "====> Epoch: 191 Average recons loss: 5.0638, kld loss: 3.9475\n",
      "====> Test set recons loss: 40.6695, kld_loss: 5.8129\n",
      "Train Epoch: 192 [0/110 (0%)]\tRecons Loss: 140.576813; KLD Loss: 140.576813\n",
      "Train Epoch: 192 [64/110 (50%)]\tRecons Loss: 138.138031; KLD Loss: 138.138031\n",
      "====> Epoch: 192 Average recons loss: 4.9682, kld loss: 4.1597\n",
      "====> Test set recons loss: 40.1097, kld_loss: 5.3301\n",
      "Train Epoch: 193 [0/110 (0%)]\tRecons Loss: 140.621735; KLD Loss: 140.621735\n",
      "Train Epoch: 193 [64/110 (50%)]\tRecons Loss: 145.971283; KLD Loss: 145.971283\n",
      "====> Epoch: 193 Average recons loss: 5.2285, kld loss: 4.0287\n",
      "====> Test set recons loss: 41.6183, kld_loss: 6.1457\n",
      "Train Epoch: 194 [0/110 (0%)]\tRecons Loss: 130.797424; KLD Loss: 130.797424\n",
      "Train Epoch: 194 [64/110 (50%)]\tRecons Loss: 136.007874; KLD Loss: 136.007874\n",
      "====> Epoch: 194 Average recons loss: 4.7104, kld loss: 4.0696\n",
      "====> Epoch: 194 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 40.4437, kld_loss: 5.2372\n",
      "Train Epoch: 195 [0/110 (0%)]\tRecons Loss: 135.449585; KLD Loss: 135.449585\n",
      "Train Epoch: 195 [64/110 (50%)]\tRecons Loss: 149.014832; KLD Loss: 149.014832\n",
      "====> Epoch: 195 Average recons loss: 5.1625, kld loss: 3.9856\n",
      "====> Test set recons loss: 40.0771, kld_loss: 5.2629\n",
      "Train Epoch: 196 [0/110 (0%)]\tRecons Loss: 144.262833; KLD Loss: 144.262833\n",
      "Train Epoch: 196 [64/110 (50%)]\tRecons Loss: 148.712921; KLD Loss: 148.712921\n",
      "====> Epoch: 196 Average recons loss: 5.4295, kld loss: 3.8596\n",
      "====> Test set recons loss: 38.9576, kld_loss: 5.8577\n",
      "Train Epoch: 197 [0/110 (0%)]\tRecons Loss: 135.143951; KLD Loss: 135.143951\n",
      "Train Epoch: 197 [64/110 (50%)]\tRecons Loss: 165.530441; KLD Loss: 165.530441\n",
      "====> Epoch: 197 Average recons loss: 5.2884, kld loss: 3.8957\n",
      "====> Test set recons loss: 40.8005, kld_loss: 5.1119\n",
      "Train Epoch: 198 [0/110 (0%)]\tRecons Loss: 138.732635; KLD Loss: 138.732635\n",
      "Train Epoch: 198 [64/110 (50%)]\tRecons Loss: 142.374664; KLD Loss: 142.374664\n",
      "====> Epoch: 198 Average recons loss: 5.4394, kld loss: 3.8902\n",
      "====> Test set recons loss: 38.1921, kld_loss: 6.1780\n",
      "Train Epoch: 199 [0/110 (0%)]\tRecons Loss: 130.392212; KLD Loss: 130.392212\n",
      "Train Epoch: 199 [64/110 (50%)]\tRecons Loss: 132.502731; KLD Loss: 132.502731\n",
      "====> Epoch: 199 Average recons loss: 4.6541, kld loss: 3.9866\n",
      "====> Test set recons loss: 41.5947, kld_loss: 4.0511\n",
      "Train Epoch: 200 [0/110 (0%)]\tRecons Loss: 161.063095; KLD Loss: 161.063095\n",
      "Train Epoch: 200 [64/110 (50%)]\tRecons Loss: 132.789703; KLD Loss: 132.789703\n",
      "====> Epoch: 200 Average recons loss: 4.9222, kld loss: 3.6701\n",
      "====> Test set recons loss: 41.5687, kld_loss: 5.3899\n",
      "Train Epoch: 201 [0/110 (0%)]\tRecons Loss: 141.879745; KLD Loss: 141.879745\n",
      "Train Epoch: 201 [64/110 (50%)]\tRecons Loss: 142.048920; KLD Loss: 142.048920\n",
      "====> Epoch: 201 Average recons loss: 5.1038, kld loss: 3.6327\n",
      "====> Test set recons loss: 44.2021, kld_loss: 4.8129\n",
      "Train Epoch: 202 [0/110 (0%)]\tRecons Loss: 124.425621; KLD Loss: 124.425621\n",
      "Train Epoch: 202 [64/110 (50%)]\tRecons Loss: 138.600220; KLD Loss: 138.600220\n",
      "====> Epoch: 202 Average recons loss: 4.7014, kld loss: 3.7778\n",
      "====> Test set recons loss: 43.2946, kld_loss: 5.3387\n",
      "Train Epoch: 203 [0/110 (0%)]\tRecons Loss: 126.533806; KLD Loss: 126.533806\n",
      "Train Epoch: 203 [64/110 (50%)]\tRecons Loss: 144.793167; KLD Loss: 144.793167\n",
      "====> Epoch: 203 Average recons loss: 4.8652, kld loss: 3.6748\n",
      "====> Test set recons loss: 39.9843, kld_loss: 5.6216\n",
      "Train Epoch: 204 [0/110 (0%)]\tRecons Loss: 119.950348; KLD Loss: 119.950348\n",
      "Train Epoch: 204 [64/110 (50%)]\tRecons Loss: 121.454826; KLD Loss: 121.454826\n",
      "====> Epoch: 204 Average recons loss: 4.5778, kld loss: 3.7447\n",
      "====> Epoch: 204 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.6454, kld_loss: 4.1637\n",
      "Train Epoch: 205 [0/110 (0%)]\tRecons Loss: 145.258743; KLD Loss: 145.258743\n",
      "Train Epoch: 205 [64/110 (50%)]\tRecons Loss: 140.398666; KLD Loss: 140.398666\n",
      "====> Epoch: 205 Average recons loss: 4.9807, kld loss: 3.6047\n",
      "====> Test set recons loss: 42.4626, kld_loss: 5.8695\n",
      "Train Epoch: 206 [0/110 (0%)]\tRecons Loss: 141.962143; KLD Loss: 141.962143\n",
      "Train Epoch: 206 [64/110 (50%)]\tRecons Loss: 132.710831; KLD Loss: 132.710831\n",
      "====> Epoch: 206 Average recons loss: 4.8526, kld loss: 3.7985\n",
      "====> Test set recons loss: 38.2261, kld_loss: 4.1228\n",
      "Train Epoch: 207 [0/110 (0%)]\tRecons Loss: 140.798737; KLD Loss: 140.798737\n",
      "Train Epoch: 207 [64/110 (50%)]\tRecons Loss: 117.781334; KLD Loss: 117.781334\n",
      "====> Epoch: 207 Average recons loss: 4.8927, kld loss: 3.5471\n",
      "====> Test set recons loss: 43.7353, kld_loss: 5.0888\n",
      "Train Epoch: 208 [0/110 (0%)]\tRecons Loss: 134.622589; KLD Loss: 134.622589\n",
      "Train Epoch: 208 [64/110 (50%)]\tRecons Loss: 140.321228; KLD Loss: 140.321228\n",
      "====> Epoch: 208 Average recons loss: 4.7488, kld loss: 3.6712\n",
      "====> Test set recons loss: 38.7290, kld_loss: 4.2029\n",
      "Train Epoch: 209 [0/110 (0%)]\tRecons Loss: 125.121185; KLD Loss: 125.121185\n",
      "Train Epoch: 209 [64/110 (50%)]\tRecons Loss: 137.407898; KLD Loss: 137.407898\n",
      "====> Epoch: 209 Average recons loss: 5.0001, kld loss: 3.4413\n",
      "====> Test set recons loss: 42.6322, kld_loss: 5.1784\n",
      "Train Epoch: 210 [0/110 (0%)]\tRecons Loss: 127.936478; KLD Loss: 127.936478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 210 [64/110 (50%)]\tRecons Loss: 131.298843; KLD Loss: 131.298843\n",
      "====> Epoch: 210 Average recons loss: 4.7185, kld loss: 3.9277\n",
      "====> Test set recons loss: 42.6193, kld_loss: 4.4340\n",
      "Train Epoch: 211 [0/110 (0%)]\tRecons Loss: 134.717880; KLD Loss: 134.717880\n",
      "Train Epoch: 211 [64/110 (50%)]\tRecons Loss: 141.399597; KLD Loss: 141.399597\n",
      "====> Epoch: 211 Average recons loss: 4.8607, kld loss: 3.5070\n",
      "====> Test set recons loss: 40.7829, kld_loss: 4.3839\n",
      "Train Epoch: 212 [0/110 (0%)]\tRecons Loss: 114.421471; KLD Loss: 114.421471\n",
      "Train Epoch: 212 [64/110 (50%)]\tRecons Loss: 134.886673; KLD Loss: 134.886673\n",
      "====> Epoch: 212 Average recons loss: 4.7402, kld loss: 3.6114\n",
      "====> Test set recons loss: 40.5084, kld_loss: 4.6149\n",
      "Train Epoch: 213 [0/110 (0%)]\tRecons Loss: 123.289818; KLD Loss: 123.289818\n",
      "Train Epoch: 213 [64/110 (50%)]\tRecons Loss: 135.145325; KLD Loss: 135.145325\n",
      "====> Epoch: 213 Average recons loss: 4.5253, kld loss: 3.4827\n",
      "====> Test set recons loss: 41.5631, kld_loss: 3.9146\n",
      "Train Epoch: 214 [0/110 (0%)]\tRecons Loss: 137.120361; KLD Loss: 137.120361\n",
      "Train Epoch: 214 [64/110 (50%)]\tRecons Loss: 131.063492; KLD Loss: 131.063492\n",
      "====> Epoch: 214 Average recons loss: 4.9262, kld loss: 3.5408\n",
      "====> Epoch: 214 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 42.9383, kld_loss: 4.8977\n",
      "Train Epoch: 215 [0/110 (0%)]\tRecons Loss: 114.443161; KLD Loss: 114.443161\n",
      "Train Epoch: 215 [64/110 (50%)]\tRecons Loss: 129.368195; KLD Loss: 129.368195\n",
      "====> Epoch: 215 Average recons loss: 4.4152, kld loss: 3.5452\n",
      "====> Test set recons loss: 41.9200, kld_loss: 3.7830\n",
      "Train Epoch: 216 [0/110 (0%)]\tRecons Loss: 134.427460; KLD Loss: 134.427460\n",
      "Train Epoch: 216 [64/110 (50%)]\tRecons Loss: 134.226837; KLD Loss: 134.226837\n",
      "====> Epoch: 216 Average recons loss: 4.6131, kld loss: 3.3911\n",
      "====> Test set recons loss: 43.8521, kld_loss: 3.8440\n",
      "Train Epoch: 217 [0/110 (0%)]\tRecons Loss: 128.487442; KLD Loss: 128.487442\n",
      "Train Epoch: 217 [64/110 (50%)]\tRecons Loss: 127.597572; KLD Loss: 127.597572\n",
      "====> Epoch: 217 Average recons loss: 4.5214, kld loss: 3.2457\n",
      "====> Test set recons loss: 40.9932, kld_loss: 3.7344\n",
      "Train Epoch: 218 [0/110 (0%)]\tRecons Loss: 137.496109; KLD Loss: 137.496109\n",
      "Train Epoch: 218 [64/110 (50%)]\tRecons Loss: 135.209488; KLD Loss: 135.209488\n",
      "====> Epoch: 218 Average recons loss: 4.7452, kld loss: 3.3350\n",
      "====> Test set recons loss: 41.3020, kld_loss: 4.0787\n",
      "Train Epoch: 219 [0/110 (0%)]\tRecons Loss: 117.076294; KLD Loss: 117.076294\n",
      "Train Epoch: 219 [64/110 (50%)]\tRecons Loss: 123.239822; KLD Loss: 123.239822\n",
      "====> Epoch: 219 Average recons loss: 4.3792, kld loss: 3.3146\n",
      "====> Test set recons loss: 37.9151, kld_loss: 3.7590\n",
      "Train Epoch: 220 [0/110 (0%)]\tRecons Loss: 117.040352; KLD Loss: 117.040352\n",
      "Train Epoch: 220 [64/110 (50%)]\tRecons Loss: 149.825745; KLD Loss: 149.825745\n",
      "====> Epoch: 220 Average recons loss: 4.6064, kld loss: 3.2568\n",
      "====> Test set recons loss: 39.6543, kld_loss: 4.1193\n",
      "Train Epoch: 221 [0/110 (0%)]\tRecons Loss: 120.140366; KLD Loss: 120.140366\n",
      "Train Epoch: 221 [64/110 (50%)]\tRecons Loss: 129.107574; KLD Loss: 129.107574\n",
      "====> Epoch: 221 Average recons loss: 4.4861, kld loss: 3.2798\n",
      "====> Test set recons loss: 38.2710, kld_loss: 3.8476\n",
      "Train Epoch: 222 [0/110 (0%)]\tRecons Loss: 135.909134; KLD Loss: 135.909134\n",
      "Train Epoch: 222 [64/110 (50%)]\tRecons Loss: 114.835266; KLD Loss: 114.835266\n",
      "====> Epoch: 222 Average recons loss: 4.5769, kld loss: 3.2528\n",
      "====> Test set recons loss: 41.0935, kld_loss: 3.9765\n",
      "Train Epoch: 223 [0/110 (0%)]\tRecons Loss: 108.461189; KLD Loss: 108.461189\n",
      "Train Epoch: 223 [64/110 (50%)]\tRecons Loss: 137.043762; KLD Loss: 137.043762\n",
      "====> Epoch: 223 Average recons loss: 4.3699, kld loss: 3.2305\n",
      "====> Test set recons loss: 41.4180, kld_loss: 3.7363\n",
      "Train Epoch: 224 [0/110 (0%)]\tRecons Loss: 138.376968; KLD Loss: 138.376968\n",
      "Train Epoch: 224 [64/110 (50%)]\tRecons Loss: 118.961075; KLD Loss: 118.961075\n",
      "====> Epoch: 224 Average recons loss: 4.5809, kld loss: 3.2724\n",
      "====> Epoch: 224 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 40.8113, kld_loss: 4.0541\n",
      "Train Epoch: 225 [0/110 (0%)]\tRecons Loss: 123.541382; KLD Loss: 123.541382\n",
      "Train Epoch: 225 [64/110 (50%)]\tRecons Loss: 128.525925; KLD Loss: 128.525925\n",
      "====> Epoch: 225 Average recons loss: 4.6798, kld loss: 3.1211\n",
      "====> Test set recons loss: 39.5001, kld_loss: 3.9321\n",
      "Train Epoch: 226 [0/110 (0%)]\tRecons Loss: 130.244812; KLD Loss: 130.244812\n",
      "Train Epoch: 226 [64/110 (50%)]\tRecons Loss: 112.504227; KLD Loss: 112.504227\n",
      "====> Epoch: 226 Average recons loss: 4.3940, kld loss: 3.3838\n",
      "====> Test set recons loss: 44.0116, kld_loss: 3.8818\n",
      "Train Epoch: 227 [0/110 (0%)]\tRecons Loss: 142.893539; KLD Loss: 142.893539\n",
      "Train Epoch: 227 [64/110 (50%)]\tRecons Loss: 117.110458; KLD Loss: 117.110458\n",
      "====> Epoch: 227 Average recons loss: 4.8559, kld loss: 3.2340\n",
      "====> Test set recons loss: 42.1555, kld_loss: 3.8591\n",
      "Train Epoch: 228 [0/110 (0%)]\tRecons Loss: 111.432602; KLD Loss: 111.432602\n",
      "Train Epoch: 228 [64/110 (50%)]\tRecons Loss: 126.885040; KLD Loss: 126.885040\n",
      "====> Epoch: 228 Average recons loss: 4.3347, kld loss: 3.2026\n",
      "====> Test set recons loss: 38.7289, kld_loss: 3.3333\n",
      "Train Epoch: 229 [0/110 (0%)]\tRecons Loss: 105.608307; KLD Loss: 105.608307\n",
      "Train Epoch: 229 [64/110 (50%)]\tRecons Loss: 127.067413; KLD Loss: 127.067413\n",
      "====> Epoch: 229 Average recons loss: 4.2452, kld loss: 3.1098\n",
      "====> Test set recons loss: 40.2535, kld_loss: 3.5887\n",
      "Train Epoch: 230 [0/110 (0%)]\tRecons Loss: 122.359741; KLD Loss: 122.359741\n",
      "Train Epoch: 230 [64/110 (50%)]\tRecons Loss: 134.041245; KLD Loss: 134.041245\n",
      "====> Epoch: 230 Average recons loss: 4.5753, kld loss: 3.2261\n",
      "====> Test set recons loss: 39.9529, kld_loss: 4.0962\n",
      "Train Epoch: 231 [0/110 (0%)]\tRecons Loss: 107.878754; KLD Loss: 107.878754\n",
      "Train Epoch: 231 [64/110 (50%)]\tRecons Loss: 130.582657; KLD Loss: 130.582657\n",
      "====> Epoch: 231 Average recons loss: 4.4729, kld loss: 3.2217\n",
      "====> Test set recons loss: 42.4582, kld_loss: 3.1045\n",
      "Train Epoch: 232 [0/110 (0%)]\tRecons Loss: 112.619225; KLD Loss: 112.619225\n",
      "Train Epoch: 232 [64/110 (50%)]\tRecons Loss: 112.591980; KLD Loss: 112.591980\n",
      "====> Epoch: 232 Average recons loss: 4.4582, kld loss: 3.0560\n",
      "====> Test set recons loss: 39.6904, kld_loss: 4.1366\n",
      "Train Epoch: 233 [0/110 (0%)]\tRecons Loss: 120.077927; KLD Loss: 120.077927\n",
      "Train Epoch: 233 [64/110 (50%)]\tRecons Loss: 122.674118; KLD Loss: 122.674118\n",
      "====> Epoch: 233 Average recons loss: 4.2364, kld loss: 3.2884\n",
      "====> Test set recons loss: 40.1265, kld_loss: 3.4710\n",
      "Train Epoch: 234 [0/110 (0%)]\tRecons Loss: 131.890305; KLD Loss: 131.890305\n",
      "Train Epoch: 234 [64/110 (50%)]\tRecons Loss: 127.603348; KLD Loss: 127.603348\n",
      "====> Epoch: 234 Average recons loss: 4.6343, kld loss: 2.9795\n",
      "====> Epoch: 234 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.2548, kld_loss: 3.5673\n",
      "Train Epoch: 235 [0/110 (0%)]\tRecons Loss: 110.765732; KLD Loss: 110.765732\n",
      "Train Epoch: 235 [64/110 (50%)]\tRecons Loss: 117.026016; KLD Loss: 117.026016\n",
      "====> Epoch: 235 Average recons loss: 4.2690, kld loss: 3.0993\n",
      "====> Test set recons loss: 42.7100, kld_loss: 3.2671\n",
      "Train Epoch: 236 [0/110 (0%)]\tRecons Loss: 115.660538; KLD Loss: 115.660538\n",
      "Train Epoch: 236 [64/110 (50%)]\tRecons Loss: 127.377762; KLD Loss: 127.377762\n",
      "====> Epoch: 236 Average recons loss: 4.4803, kld loss: 3.0228\n",
      "====> Test set recons loss: 42.7388, kld_loss: 4.0012\n",
      "Train Epoch: 237 [0/110 (0%)]\tRecons Loss: 119.555290; KLD Loss: 119.555290\n",
      "Train Epoch: 237 [64/110 (50%)]\tRecons Loss: 112.645370; KLD Loss: 112.645370\n",
      "====> Epoch: 237 Average recons loss: 4.0353, kld loss: 3.3281\n",
      "====> Test set recons loss: 40.8913, kld_loss: 3.0627\n",
      "Train Epoch: 238 [0/110 (0%)]\tRecons Loss: 139.506836; KLD Loss: 139.506836\n",
      "Train Epoch: 238 [64/110 (50%)]\tRecons Loss: 127.801994; KLD Loss: 127.801994\n",
      "====> Epoch: 238 Average recons loss: 4.5071, kld loss: 2.9523\n",
      "====> Test set recons loss: 42.1977, kld_loss: 3.8243\n",
      "Train Epoch: 239 [0/110 (0%)]\tRecons Loss: 102.981758; KLD Loss: 102.981758\n",
      "Train Epoch: 239 [64/110 (50%)]\tRecons Loss: 116.691818; KLD Loss: 116.691818\n",
      "====> Epoch: 239 Average recons loss: 4.0778, kld loss: 3.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 36.9243, kld_loss: 2.8301\n",
      "Train Epoch: 240 [0/110 (0%)]\tRecons Loss: 121.698059; KLD Loss: 121.698059\n",
      "Train Epoch: 240 [64/110 (50%)]\tRecons Loss: 125.527390; KLD Loss: 125.527390\n",
      "====> Epoch: 240 Average recons loss: 4.3437, kld loss: 3.0376\n",
      "====> Test set recons loss: 42.7992, kld_loss: 4.5228\n",
      "Train Epoch: 241 [0/110 (0%)]\tRecons Loss: 108.800911; KLD Loss: 108.800911\n",
      "Train Epoch: 241 [64/110 (50%)]\tRecons Loss: 111.397438; KLD Loss: 111.397438\n",
      "====> Epoch: 241 Average recons loss: 4.2123, kld loss: 3.1701\n",
      "====> Test set recons loss: 41.0085, kld_loss: 2.4592\n",
      "Train Epoch: 242 [0/110 (0%)]\tRecons Loss: 110.594070; KLD Loss: 110.594070\n",
      "Train Epoch: 242 [64/110 (50%)]\tRecons Loss: 119.060608; KLD Loss: 119.060608\n",
      "====> Epoch: 242 Average recons loss: 4.4299, kld loss: 3.0031\n",
      "====> Test set recons loss: 47.0250, kld_loss: 3.8903\n",
      "Train Epoch: 243 [0/110 (0%)]\tRecons Loss: 112.220825; KLD Loss: 112.220825\n",
      "Train Epoch: 243 [64/110 (50%)]\tRecons Loss: 105.345650; KLD Loss: 105.345650\n",
      "====> Epoch: 243 Average recons loss: 4.3716, kld loss: 3.1144\n",
      "====> Test set recons loss: 38.3472, kld_loss: 3.0580\n",
      "Train Epoch: 244 [0/110 (0%)]\tRecons Loss: 116.068344; KLD Loss: 116.068344\n",
      "Train Epoch: 244 [64/110 (50%)]\tRecons Loss: 107.753609; KLD Loss: 107.753609\n",
      "====> Epoch: 244 Average recons loss: 3.9408, kld loss: 3.0097\n",
      "====> Epoch: 244 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 40.0597, kld_loss: 3.4461\n",
      "Train Epoch: 245 [0/110 (0%)]\tRecons Loss: 130.768845; KLD Loss: 130.768845\n",
      "Train Epoch: 245 [64/110 (50%)]\tRecons Loss: 118.787537; KLD Loss: 118.787537\n",
      "====> Epoch: 245 Average recons loss: 4.1309, kld loss: 2.9837\n",
      "====> Test set recons loss: 41.8713, kld_loss: 2.8772\n",
      "Train Epoch: 246 [0/110 (0%)]\tRecons Loss: 117.230881; KLD Loss: 117.230881\n",
      "Train Epoch: 246 [64/110 (50%)]\tRecons Loss: 119.526146; KLD Loss: 119.526146\n",
      "====> Epoch: 246 Average recons loss: 4.3045, kld loss: 2.9151\n",
      "====> Test set recons loss: 40.9215, kld_loss: 3.7472\n",
      "Train Epoch: 247 [0/110 (0%)]\tRecons Loss: 111.106606; KLD Loss: 111.106606\n",
      "Train Epoch: 247 [64/110 (50%)]\tRecons Loss: 107.286255; KLD Loss: 107.286255\n",
      "====> Epoch: 247 Average recons loss: 3.9620, kld loss: 3.1150\n",
      "====> Test set recons loss: 42.1918, kld_loss: 3.2920\n",
      "Train Epoch: 248 [0/110 (0%)]\tRecons Loss: 109.651611; KLD Loss: 109.651611\n",
      "Train Epoch: 248 [64/110 (50%)]\tRecons Loss: 100.370674; KLD Loss: 100.370674\n",
      "====> Epoch: 248 Average recons loss: 4.0744, kld loss: 3.0041\n",
      "====> Test set recons loss: 44.0170, kld_loss: 3.4172\n",
      "Train Epoch: 249 [0/110 (0%)]\tRecons Loss: 97.509811; KLD Loss: 97.509811\n",
      "Train Epoch: 249 [64/110 (50%)]\tRecons Loss: 115.268005; KLD Loss: 115.268005\n",
      "====> Epoch: 249 Average recons loss: 4.0498, kld loss: 2.9442\n",
      "====> Test set recons loss: 41.1855, kld_loss: 3.3194\n",
      "Train Epoch: 250 [0/110 (0%)]\tRecons Loss: 100.857605; KLD Loss: 100.857605\n",
      "Train Epoch: 250 [64/110 (50%)]\tRecons Loss: 133.830063; KLD Loss: 133.830063\n",
      "====> Epoch: 250 Average recons loss: 4.2830, kld loss: 3.1265\n",
      "====> Test set recons loss: 35.7732, kld_loss: 3.6682\n",
      "Train Epoch: 251 [0/110 (0%)]\tRecons Loss: 104.830719; KLD Loss: 104.830719\n",
      "Train Epoch: 251 [64/110 (50%)]\tRecons Loss: 108.237488; KLD Loss: 108.237488\n",
      "====> Epoch: 251 Average recons loss: 3.9600, kld loss: 2.9919\n",
      "====> Test set recons loss: 41.6936, kld_loss: 2.7992\n",
      "Train Epoch: 252 [0/110 (0%)]\tRecons Loss: 106.456688; KLD Loss: 106.456688\n",
      "Train Epoch: 252 [64/110 (50%)]\tRecons Loss: 120.640106; KLD Loss: 120.640106\n",
      "====> Epoch: 252 Average recons loss: 4.1691, kld loss: 2.9592\n",
      "====> Test set recons loss: 39.5648, kld_loss: 3.8549\n",
      "Train Epoch: 253 [0/110 (0%)]\tRecons Loss: 94.553192; KLD Loss: 94.553192\n",
      "Train Epoch: 253 [64/110 (50%)]\tRecons Loss: 127.461403; KLD Loss: 127.461403\n",
      "====> Epoch: 253 Average recons loss: 4.1437, kld loss: 3.0071\n",
      "====> Test set recons loss: 42.5833, kld_loss: 3.1099\n",
      "Train Epoch: 254 [0/110 (0%)]\tRecons Loss: 116.749100; KLD Loss: 116.749100\n",
      "Train Epoch: 254 [64/110 (50%)]\tRecons Loss: 92.268929; KLD Loss: 92.268929\n",
      "====> Epoch: 254 Average recons loss: 4.0776, kld loss: 2.9986\n",
      "====> Epoch: 254 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 43.3843, kld_loss: 3.3103\n",
      "Train Epoch: 255 [0/110 (0%)]\tRecons Loss: 95.246017; KLD Loss: 95.246017\n",
      "Train Epoch: 255 [64/110 (50%)]\tRecons Loss: 101.862450; KLD Loss: 101.862450\n",
      "====> Epoch: 255 Average recons loss: 3.7060, kld loss: 2.9316\n",
      "====> Test set recons loss: 44.1765, kld_loss: 2.8369\n",
      "Train Epoch: 256 [0/110 (0%)]\tRecons Loss: 109.941650; KLD Loss: 109.941650\n",
      "Train Epoch: 256 [64/110 (50%)]\tRecons Loss: 112.802780; KLD Loss: 112.802780\n",
      "====> Epoch: 256 Average recons loss: 3.9081, kld loss: 2.8632\n",
      "====> Test set recons loss: 41.2249, kld_loss: 2.7775\n",
      "Train Epoch: 257 [0/110 (0%)]\tRecons Loss: 110.946884; KLD Loss: 110.946884\n",
      "Train Epoch: 257 [64/110 (50%)]\tRecons Loss: 112.812592; KLD Loss: 112.812592\n",
      "====> Epoch: 257 Average recons loss: 3.9668, kld loss: 2.7712\n",
      "====> Test set recons loss: 37.8563, kld_loss: 2.7350\n",
      "Train Epoch: 258 [0/110 (0%)]\tRecons Loss: 109.810654; KLD Loss: 109.810654\n",
      "Train Epoch: 258 [64/110 (50%)]\tRecons Loss: 104.006996; KLD Loss: 104.006996\n",
      "====> Epoch: 258 Average recons loss: 4.0236, kld loss: 2.8017\n",
      "====> Test set recons loss: 39.4088, kld_loss: 3.2679\n",
      "Train Epoch: 259 [0/110 (0%)]\tRecons Loss: 89.918251; KLD Loss: 89.918251\n",
      "Train Epoch: 259 [64/110 (50%)]\tRecons Loss: 127.590393; KLD Loss: 127.590393\n",
      "====> Epoch: 259 Average recons loss: 3.8944, kld loss: 2.7464\n",
      "====> Test set recons loss: 41.6035, kld_loss: 3.0251\n",
      "Train Epoch: 260 [0/110 (0%)]\tRecons Loss: 106.336624; KLD Loss: 106.336624\n",
      "Train Epoch: 260 [64/110 (50%)]\tRecons Loss: 102.418152; KLD Loss: 102.418152\n",
      "====> Epoch: 260 Average recons loss: 3.7438, kld loss: 2.8087\n",
      "====> Test set recons loss: 39.5676, kld_loss: 2.7049\n",
      "Train Epoch: 261 [0/110 (0%)]\tRecons Loss: 118.164345; KLD Loss: 118.164345\n",
      "Train Epoch: 261 [64/110 (50%)]\tRecons Loss: 99.866653; KLD Loss: 99.866653\n",
      "====> Epoch: 261 Average recons loss: 3.9501, kld loss: 2.6682\n",
      "====> Test set recons loss: 40.7835, kld_loss: 2.6298\n",
      "Train Epoch: 262 [0/110 (0%)]\tRecons Loss: 109.568100; KLD Loss: 109.568100\n",
      "Train Epoch: 262 [64/110 (50%)]\tRecons Loss: 115.912003; KLD Loss: 115.912003\n",
      "====> Epoch: 262 Average recons loss: 4.0699, kld loss: 2.7760\n",
      "====> Test set recons loss: 46.0410, kld_loss: 2.8496\n",
      "Train Epoch: 263 [0/110 (0%)]\tRecons Loss: 108.764229; KLD Loss: 108.764229\n",
      "Train Epoch: 263 [64/110 (50%)]\tRecons Loss: 108.513641; KLD Loss: 108.513641\n",
      "====> Epoch: 263 Average recons loss: 3.7664, kld loss: 2.6286\n",
      "====> Test set recons loss: 43.5841, kld_loss: 2.6689\n",
      "Train Epoch: 264 [0/110 (0%)]\tRecons Loss: 102.079742; KLD Loss: 102.079742\n",
      "Train Epoch: 264 [64/110 (50%)]\tRecons Loss: 120.819710; KLD Loss: 120.819710\n",
      "====> Epoch: 264 Average recons loss: 3.8349, kld loss: 2.6975\n",
      "====> Epoch: 264 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.5179, kld_loss: 2.7295\n",
      "Train Epoch: 265 [0/110 (0%)]\tRecons Loss: 102.252853; KLD Loss: 102.252853\n",
      "Train Epoch: 265 [64/110 (50%)]\tRecons Loss: 97.449860; KLD Loss: 97.449860\n",
      "====> Epoch: 265 Average recons loss: 3.6301, kld loss: 2.6545\n",
      "====> Test set recons loss: 43.0961, kld_loss: 2.5446\n",
      "Train Epoch: 266 [0/110 (0%)]\tRecons Loss: 92.930557; KLD Loss: 92.930557\n",
      "Train Epoch: 266 [64/110 (50%)]\tRecons Loss: 101.589989; KLD Loss: 101.589989\n",
      "====> Epoch: 266 Average recons loss: 3.7802, kld loss: 2.5641\n",
      "====> Test set recons loss: 39.4514, kld_loss: 2.7334\n",
      "Train Epoch: 267 [0/110 (0%)]\tRecons Loss: 104.645370; KLD Loss: 104.645370\n",
      "Train Epoch: 267 [64/110 (50%)]\tRecons Loss: 109.315628; KLD Loss: 109.315628\n",
      "====> Epoch: 267 Average recons loss: 4.0214, kld loss: 2.6233\n",
      "====> Test set recons loss: 45.7863, kld_loss: 2.6585\n",
      "Train Epoch: 268 [0/110 (0%)]\tRecons Loss: 92.901329; KLD Loss: 92.901329\n",
      "Train Epoch: 268 [64/110 (50%)]\tRecons Loss: 118.197350; KLD Loss: 118.197350\n",
      "====> Epoch: 268 Average recons loss: 3.8430, kld loss: 2.6550\n",
      "====> Test set recons loss: 38.7641, kld_loss: 2.4684\n",
      "Train Epoch: 269 [0/110 (0%)]\tRecons Loss: 109.503433; KLD Loss: 109.503433\n",
      "Train Epoch: 269 [64/110 (50%)]\tRecons Loss: 101.169579; KLD Loss: 101.169579\n",
      "====> Epoch: 269 Average recons loss: 3.7363, kld loss: 2.5868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 40.9883, kld_loss: 2.6161\n",
      "Train Epoch: 270 [0/110 (0%)]\tRecons Loss: 99.809891; KLD Loss: 99.809891\n",
      "Train Epoch: 270 [64/110 (50%)]\tRecons Loss: 106.311783; KLD Loss: 106.311783\n",
      "====> Epoch: 270 Average recons loss: 3.6538, kld loss: 2.5985\n",
      "====> Test set recons loss: 40.0450, kld_loss: 2.3527\n",
      "Train Epoch: 271 [0/110 (0%)]\tRecons Loss: 106.111748; KLD Loss: 106.111748\n",
      "Train Epoch: 271 [64/110 (50%)]\tRecons Loss: 92.427666; KLD Loss: 92.427666\n",
      "====> Epoch: 271 Average recons loss: 4.1263, kld loss: 2.5618\n",
      "====> Test set recons loss: 42.8865, kld_loss: 3.4498\n",
      "Train Epoch: 272 [0/110 (0%)]\tRecons Loss: 100.128784; KLD Loss: 100.128784\n",
      "Train Epoch: 272 [64/110 (50%)]\tRecons Loss: 104.980515; KLD Loss: 104.980515\n",
      "====> Epoch: 272 Average recons loss: 3.8781, kld loss: 2.8613\n",
      "====> Test set recons loss: 42.7443, kld_loss: 2.8796\n",
      "Train Epoch: 273 [0/110 (0%)]\tRecons Loss: 106.967224; KLD Loss: 106.967224\n",
      "Train Epoch: 273 [64/110 (50%)]\tRecons Loss: 107.973312; KLD Loss: 107.973312\n",
      "====> Epoch: 273 Average recons loss: 3.7021, kld loss: 2.7354\n",
      "====> Test set recons loss: 40.1151, kld_loss: 2.5022\n",
      "Train Epoch: 274 [0/110 (0%)]\tRecons Loss: 104.945877; KLD Loss: 104.945877\n",
      "Train Epoch: 274 [64/110 (50%)]\tRecons Loss: 103.639648; KLD Loss: 103.639648\n",
      "====> Epoch: 274 Average recons loss: 3.9769, kld loss: 2.6471\n",
      "====> Epoch: 274 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.2722, kld_loss: 2.8293\n",
      "Train Epoch: 275 [0/110 (0%)]\tRecons Loss: 99.021378; KLD Loss: 99.021378\n",
      "Train Epoch: 275 [64/110 (50%)]\tRecons Loss: 99.088921; KLD Loss: 99.088921\n",
      "====> Epoch: 275 Average recons loss: 3.5340, kld loss: 2.7206\n",
      "====> Test set recons loss: 38.8918, kld_loss: 2.4490\n",
      "Train Epoch: 276 [0/110 (0%)]\tRecons Loss: 92.145920; KLD Loss: 92.145920\n",
      "Train Epoch: 276 [64/110 (50%)]\tRecons Loss: 112.610260; KLD Loss: 112.610260\n",
      "====> Epoch: 276 Average recons loss: 3.7114, kld loss: 2.4859\n",
      "====> Test set recons loss: 40.4723, kld_loss: 2.5187\n",
      "Train Epoch: 277 [0/110 (0%)]\tRecons Loss: 102.746407; KLD Loss: 102.746407\n",
      "Train Epoch: 277 [64/110 (50%)]\tRecons Loss: 108.728340; KLD Loss: 108.728340\n",
      "====> Epoch: 277 Average recons loss: 3.8146, kld loss: 2.5973\n",
      "====> Test set recons loss: 41.8882, kld_loss: 2.6801\n",
      "Train Epoch: 278 [0/110 (0%)]\tRecons Loss: 97.782410; KLD Loss: 97.782410\n",
      "Train Epoch: 278 [64/110 (50%)]\tRecons Loss: 93.241730; KLD Loss: 93.241730\n",
      "====> Epoch: 278 Average recons loss: 3.6299, kld loss: 2.5826\n",
      "====> Test set recons loss: 42.2180, kld_loss: 2.3364\n",
      "Train Epoch: 279 [0/110 (0%)]\tRecons Loss: 95.453323; KLD Loss: 95.453323\n",
      "Train Epoch: 279 [64/110 (50%)]\tRecons Loss: 108.857414; KLD Loss: 108.857414\n",
      "====> Epoch: 279 Average recons loss: 3.6394, kld loss: 2.5556\n",
      "====> Test set recons loss: 43.5861, kld_loss: 2.5548\n",
      "Train Epoch: 280 [0/110 (0%)]\tRecons Loss: 101.744247; KLD Loss: 101.744247\n",
      "Train Epoch: 280 [64/110 (50%)]\tRecons Loss: 108.484421; KLD Loss: 108.484421\n",
      "====> Epoch: 280 Average recons loss: 3.5763, kld loss: 2.5206\n",
      "====> Test set recons loss: 40.8458, kld_loss: 2.3075\n",
      "Train Epoch: 281 [0/110 (0%)]\tRecons Loss: 94.965919; KLD Loss: 94.965919\n",
      "Train Epoch: 281 [64/110 (50%)]\tRecons Loss: 103.640076; KLD Loss: 103.640076\n",
      "====> Epoch: 281 Average recons loss: 3.5423, kld loss: 2.5237\n",
      "====> Test set recons loss: 41.4051, kld_loss: 2.5190\n",
      "Train Epoch: 282 [0/110 (0%)]\tRecons Loss: 110.950150; KLD Loss: 110.950150\n",
      "Train Epoch: 282 [64/110 (50%)]\tRecons Loss: 104.991150; KLD Loss: 104.991150\n",
      "====> Epoch: 282 Average recons loss: 3.7535, kld loss: 2.4530\n",
      "====> Test set recons loss: 41.5469, kld_loss: 2.2375\n",
      "Train Epoch: 283 [0/110 (0%)]\tRecons Loss: 93.831757; KLD Loss: 93.831757\n",
      "Train Epoch: 283 [64/110 (50%)]\tRecons Loss: 98.119064; KLD Loss: 98.119064\n",
      "====> Epoch: 283 Average recons loss: 3.5204, kld loss: 2.5221\n",
      "====> Test set recons loss: 39.8075, kld_loss: 2.4751\n",
      "Train Epoch: 284 [0/110 (0%)]\tRecons Loss: 95.290710; KLD Loss: 95.290710\n",
      "Train Epoch: 284 [64/110 (50%)]\tRecons Loss: 99.956284; KLD Loss: 99.956284\n",
      "====> Epoch: 284 Average recons loss: 3.8449, kld loss: 2.4399\n",
      "====> Epoch: 284 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.9038, kld_loss: 2.3822\n",
      "Train Epoch: 285 [0/110 (0%)]\tRecons Loss: 92.722046; KLD Loss: 92.722046\n",
      "Train Epoch: 285 [64/110 (50%)]\tRecons Loss: 81.661667; KLD Loss: 81.661667\n",
      "====> Epoch: 285 Average recons loss: 3.3739, kld loss: 2.5216\n",
      "====> Test set recons loss: 40.1890, kld_loss: 2.1619\n",
      "Train Epoch: 286 [0/110 (0%)]\tRecons Loss: 105.373085; KLD Loss: 105.373085\n",
      "Train Epoch: 286 [64/110 (50%)]\tRecons Loss: 104.363037; KLD Loss: 104.363037\n",
      "====> Epoch: 286 Average recons loss: 3.5881, kld loss: 2.3843\n",
      "====> Test set recons loss: 38.4357, kld_loss: 2.1830\n",
      "Train Epoch: 287 [0/110 (0%)]\tRecons Loss: 108.497932; KLD Loss: 108.497932\n",
      "Train Epoch: 287 [64/110 (50%)]\tRecons Loss: 103.268494; KLD Loss: 103.268494\n",
      "====> Epoch: 287 Average recons loss: 3.8341, kld loss: 2.3355\n",
      "====> Test set recons loss: 45.7336, kld_loss: 2.3967\n",
      "Train Epoch: 288 [0/110 (0%)]\tRecons Loss: 98.051239; KLD Loss: 98.051239\n",
      "Train Epoch: 288 [64/110 (50%)]\tRecons Loss: 90.108765; KLD Loss: 90.108765\n",
      "====> Epoch: 288 Average recons loss: 3.3776, kld loss: 2.5021\n",
      "====> Test set recons loss: 41.9169, kld_loss: 2.0973\n",
      "Train Epoch: 289 [0/110 (0%)]\tRecons Loss: 98.976166; KLD Loss: 98.976166\n",
      "Train Epoch: 289 [64/110 (50%)]\tRecons Loss: 137.833984; KLD Loss: 137.833984\n",
      "====> Epoch: 289 Average recons loss: 4.0066, kld loss: 2.3376\n",
      "====> Test set recons loss: 44.4454, kld_loss: 2.9270\n",
      "Train Epoch: 290 [0/110 (0%)]\tRecons Loss: 100.961609; KLD Loss: 100.961609\n",
      "Train Epoch: 290 [64/110 (50%)]\tRecons Loss: 85.190674; KLD Loss: 85.190674\n",
      "====> Epoch: 290 Average recons loss: 3.3833, kld loss: 2.6755\n",
      "====> Test set recons loss: 36.0444, kld_loss: 2.0156\n",
      "Train Epoch: 291 [0/110 (0%)]\tRecons Loss: 97.248444; KLD Loss: 97.248444\n",
      "Train Epoch: 291 [64/110 (50%)]\tRecons Loss: 111.279633; KLD Loss: 111.279633\n",
      "====> Epoch: 291 Average recons loss: 3.6987, kld loss: 2.2840\n",
      "====> Test set recons loss: 42.0263, kld_loss: 2.1564\n",
      "Train Epoch: 292 [0/110 (0%)]\tRecons Loss: 95.415062; KLD Loss: 95.415062\n",
      "Train Epoch: 292 [64/110 (50%)]\tRecons Loss: 106.050591; KLD Loss: 106.050591\n",
      "====> Epoch: 292 Average recons loss: 3.5342, kld loss: 2.4006\n",
      "====> Test set recons loss: 45.0363, kld_loss: 2.2378\n",
      "Train Epoch: 293 [0/110 (0%)]\tRecons Loss: 85.188644; KLD Loss: 85.188644\n",
      "Train Epoch: 293 [64/110 (50%)]\tRecons Loss: 101.857803; KLD Loss: 101.857803\n",
      "====> Epoch: 293 Average recons loss: 3.5877, kld loss: 2.4455\n",
      "====> Test set recons loss: 40.6308, kld_loss: 2.4327\n",
      "Train Epoch: 294 [0/110 (0%)]\tRecons Loss: 94.747414; KLD Loss: 94.747414\n",
      "Train Epoch: 294 [64/110 (50%)]\tRecons Loss: 91.229866; KLD Loss: 91.229866\n",
      "====> Epoch: 294 Average recons loss: 3.4690, kld loss: 2.4540\n",
      "====> Epoch: 294 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 43.2880, kld_loss: 2.1959\n",
      "Train Epoch: 295 [0/110 (0%)]\tRecons Loss: 96.768051; KLD Loss: 96.768051\n",
      "Train Epoch: 295 [64/110 (50%)]\tRecons Loss: 102.514900; KLD Loss: 102.514900\n",
      "====> Epoch: 295 Average recons loss: 3.7370, kld loss: 2.4474\n",
      "====> Test set recons loss: 42.9609, kld_loss: 2.6414\n",
      "Train Epoch: 296 [0/110 (0%)]\tRecons Loss: 85.826172; KLD Loss: 85.826172\n",
      "Train Epoch: 296 [64/110 (50%)]\tRecons Loss: 91.467529; KLD Loss: 91.467529\n",
      "====> Epoch: 296 Average recons loss: 3.2869, kld loss: 2.5802\n",
      "====> Test set recons loss: 41.7975, kld_loss: 2.2576\n",
      "Train Epoch: 297 [0/110 (0%)]\tRecons Loss: 89.277008; KLD Loss: 89.277008\n",
      "Train Epoch: 297 [64/110 (50%)]\tRecons Loss: 96.271088; KLD Loss: 96.271088\n",
      "====> Epoch: 297 Average recons loss: 3.3052, kld loss: 2.4835\n",
      "====> Test set recons loss: 41.9223, kld_loss: 2.4344\n",
      "Train Epoch: 298 [0/110 (0%)]\tRecons Loss: 106.110016; KLD Loss: 106.110016\n",
      "Train Epoch: 298 [64/110 (50%)]\tRecons Loss: 110.606079; KLD Loss: 110.606079\n",
      "====> Epoch: 298 Average recons loss: 3.9646, kld loss: 2.4468\n",
      "====> Test set recons loss: 41.2339, kld_loss: 2.4444\n",
      "Train Epoch: 299 [0/110 (0%)]\tRecons Loss: 79.331047; KLD Loss: 79.331047\n",
      "Train Epoch: 299 [64/110 (50%)]\tRecons Loss: 99.997375; KLD Loss: 99.997375\n",
      "====> Epoch: 299 Average recons loss: 3.3371, kld loss: 2.5045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 39.9497, kld_loss: 2.2162\n",
      "Train Epoch: 300 [0/110 (0%)]\tRecons Loss: 86.917145; KLD Loss: 86.917145\n",
      "Train Epoch: 300 [64/110 (50%)]\tRecons Loss: 92.552277; KLD Loss: 92.552277\n",
      "====> Epoch: 300 Average recons loss: 3.2375, kld loss: 2.4397\n",
      "====> Test set recons loss: 42.6685, kld_loss: 2.0365\n",
      "Train Epoch: 301 [0/110 (0%)]\tRecons Loss: 95.986275; KLD Loss: 95.986275\n",
      "Train Epoch: 301 [64/110 (50%)]\tRecons Loss: 88.035484; KLD Loss: 88.035484\n",
      "====> Epoch: 301 Average recons loss: 3.5863, kld loss: 2.2389\n",
      "====> Test set recons loss: 40.7913, kld_loss: 1.9934\n",
      "Train Epoch: 302 [0/110 (0%)]\tRecons Loss: 104.440399; KLD Loss: 104.440399\n",
      "Train Epoch: 302 [64/110 (50%)]\tRecons Loss: 88.013870; KLD Loss: 88.013870\n",
      "====> Epoch: 302 Average recons loss: 3.5060, kld loss: 2.3952\n",
      "====> Test set recons loss: 39.7630, kld_loss: 2.3102\n",
      "Train Epoch: 303 [0/110 (0%)]\tRecons Loss: 114.404968; KLD Loss: 114.404968\n",
      "Train Epoch: 303 [64/110 (50%)]\tRecons Loss: 92.103355; KLD Loss: 92.103355\n",
      "====> Epoch: 303 Average recons loss: 3.4801, kld loss: 2.5415\n",
      "====> Test set recons loss: 39.3526, kld_loss: 2.1026\n",
      "Train Epoch: 304 [0/110 (0%)]\tRecons Loss: 88.331627; KLD Loss: 88.331627\n",
      "Train Epoch: 304 [64/110 (50%)]\tRecons Loss: 93.394035; KLD Loss: 93.394035\n",
      "====> Epoch: 304 Average recons loss: 3.3122, kld loss: 2.2295\n",
      "====> Epoch: 304 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 42.2282, kld_loss: 1.8335\n",
      "Train Epoch: 305 [0/110 (0%)]\tRecons Loss: 92.756828; KLD Loss: 92.756828\n",
      "Train Epoch: 305 [64/110 (50%)]\tRecons Loss: 80.852570; KLD Loss: 80.852570\n",
      "====> Epoch: 305 Average recons loss: 3.4058, kld loss: 2.2725\n",
      "====> Test set recons loss: 44.8963, kld_loss: 2.1950\n",
      "Train Epoch: 306 [0/110 (0%)]\tRecons Loss: 100.918259; KLD Loss: 100.918259\n",
      "Train Epoch: 306 [64/110 (50%)]\tRecons Loss: 74.808273; KLD Loss: 74.808273\n",
      "====> Epoch: 306 Average recons loss: 3.2090, kld loss: 2.3507\n",
      "====> Test set recons loss: 41.2656, kld_loss: 1.7169\n",
      "Train Epoch: 307 [0/110 (0%)]\tRecons Loss: 111.329506; KLD Loss: 111.329506\n",
      "Train Epoch: 307 [64/110 (50%)]\tRecons Loss: 85.641022; KLD Loss: 85.641022\n",
      "====> Epoch: 307 Average recons loss: 3.4447, kld loss: 2.1877\n",
      "====> Test set recons loss: 38.7080, kld_loss: 2.0990\n",
      "Train Epoch: 308 [0/110 (0%)]\tRecons Loss: 93.918587; KLD Loss: 93.918587\n",
      "Train Epoch: 308 [64/110 (50%)]\tRecons Loss: 94.542427; KLD Loss: 94.542427\n",
      "====> Epoch: 308 Average recons loss: 3.5040, kld loss: 2.3074\n",
      "====> Test set recons loss: 39.3268, kld_loss: 1.9626\n",
      "Train Epoch: 309 [0/110 (0%)]\tRecons Loss: 105.403046; KLD Loss: 105.403046\n",
      "Train Epoch: 309 [64/110 (50%)]\tRecons Loss: 94.549278; KLD Loss: 94.549278\n",
      "====> Epoch: 309 Average recons loss: 3.2455, kld loss: 2.3089\n",
      "====> Test set recons loss: 40.3697, kld_loss: 1.9390\n",
      "Train Epoch: 310 [0/110 (0%)]\tRecons Loss: 104.590294; KLD Loss: 104.590294\n",
      "Train Epoch: 310 [64/110 (50%)]\tRecons Loss: 91.463425; KLD Loss: 91.463425\n",
      "====> Epoch: 310 Average recons loss: 3.6010, kld loss: 2.3314\n",
      "====> Test set recons loss: 41.1762, kld_loss: 2.3572\n",
      "Train Epoch: 311 [0/110 (0%)]\tRecons Loss: 80.507370; KLD Loss: 80.507370\n",
      "Train Epoch: 311 [64/110 (50%)]\tRecons Loss: 96.811531; KLD Loss: 96.811531\n",
      "====> Epoch: 311 Average recons loss: 3.5545, kld loss: 2.3919\n",
      "====> Test set recons loss: 43.2454, kld_loss: 2.0486\n",
      "Train Epoch: 312 [0/110 (0%)]\tRecons Loss: 83.948967; KLD Loss: 83.948967\n",
      "Train Epoch: 312 [64/110 (50%)]\tRecons Loss: 105.550705; KLD Loss: 105.550705\n",
      "====> Epoch: 312 Average recons loss: 3.4253, kld loss: 2.3867\n",
      "====> Test set recons loss: 41.5723, kld_loss: 2.0866\n",
      "Train Epoch: 313 [0/110 (0%)]\tRecons Loss: 86.909698; KLD Loss: 86.909698\n",
      "Train Epoch: 313 [64/110 (50%)]\tRecons Loss: 95.686981; KLD Loss: 95.686981\n",
      "====> Epoch: 313 Average recons loss: 3.3556, kld loss: 2.3292\n",
      "====> Test set recons loss: 41.9322, kld_loss: 2.2156\n",
      "Train Epoch: 314 [0/110 (0%)]\tRecons Loss: 88.864227; KLD Loss: 88.864227\n",
      "Train Epoch: 314 [64/110 (50%)]\tRecons Loss: 93.039352; KLD Loss: 93.039352\n",
      "====> Epoch: 314 Average recons loss: 3.3859, kld loss: 2.3443\n",
      "====> Epoch: 314 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 40.0642, kld_loss: 1.9988\n",
      "Train Epoch: 315 [0/110 (0%)]\tRecons Loss: 95.761749; KLD Loss: 95.761749\n",
      "Train Epoch: 315 [64/110 (50%)]\tRecons Loss: 85.393791; KLD Loss: 85.393791\n",
      "====> Epoch: 315 Average recons loss: 3.2277, kld loss: 2.3115\n",
      "====> Test set recons loss: 39.8696, kld_loss: 2.0319\n",
      "Train Epoch: 316 [0/110 (0%)]\tRecons Loss: 85.692368; KLD Loss: 85.692368\n",
      "Train Epoch: 316 [64/110 (50%)]\tRecons Loss: 96.732857; KLD Loss: 96.732857\n",
      "====> Epoch: 316 Average recons loss: 3.4105, kld loss: 2.2023\n",
      "====> Test set recons loss: 41.2073, kld_loss: 1.8560\n",
      "Train Epoch: 317 [0/110 (0%)]\tRecons Loss: 88.900421; KLD Loss: 88.900421\n",
      "Train Epoch: 317 [64/110 (50%)]\tRecons Loss: 86.543457; KLD Loss: 86.543457\n",
      "====> Epoch: 317 Average recons loss: 3.3898, kld loss: 2.2939\n",
      "====> Test set recons loss: 44.7285, kld_loss: 2.1899\n",
      "Train Epoch: 318 [0/110 (0%)]\tRecons Loss: 95.397461; KLD Loss: 95.397461\n",
      "Train Epoch: 318 [64/110 (50%)]\tRecons Loss: 95.552933; KLD Loss: 95.552933\n",
      "====> Epoch: 318 Average recons loss: 3.2720, kld loss: 2.3554\n",
      "====> Test set recons loss: 42.5168, kld_loss: 2.1340\n",
      "Train Epoch: 319 [0/110 (0%)]\tRecons Loss: 81.195595; KLD Loss: 81.195595\n",
      "Train Epoch: 319 [64/110 (50%)]\tRecons Loss: 100.412483; KLD Loss: 100.412483\n",
      "====> Epoch: 319 Average recons loss: 3.3543, kld loss: 2.2496\n",
      "====> Test set recons loss: 40.0285, kld_loss: 2.1147\n",
      "Train Epoch: 320 [0/110 (0%)]\tRecons Loss: 101.256042; KLD Loss: 101.256042\n",
      "Train Epoch: 320 [64/110 (50%)]\tRecons Loss: 96.620308; KLD Loss: 96.620308\n",
      "====> Epoch: 320 Average recons loss: 3.2456, kld loss: 2.4567\n",
      "====> Test set recons loss: 38.8518, kld_loss: 2.4357\n",
      "Train Epoch: 321 [0/110 (0%)]\tRecons Loss: 84.334862; KLD Loss: 84.334862\n",
      "Train Epoch: 321 [64/110 (50%)]\tRecons Loss: 105.870544; KLD Loss: 105.870544\n",
      "====> Epoch: 321 Average recons loss: 3.2650, kld loss: 2.4570\n",
      "====> Test set recons loss: 41.3014, kld_loss: 1.9806\n",
      "Train Epoch: 322 [0/110 (0%)]\tRecons Loss: 89.572327; KLD Loss: 89.572327\n",
      "Train Epoch: 322 [64/110 (50%)]\tRecons Loss: 96.895882; KLD Loss: 96.895882\n",
      "====> Epoch: 322 Average recons loss: 3.3829, kld loss: 2.2816\n",
      "====> Test set recons loss: 44.0311, kld_loss: 2.1134\n",
      "Train Epoch: 323 [0/110 (0%)]\tRecons Loss: 74.865120; KLD Loss: 74.865120\n",
      "Train Epoch: 323 [64/110 (50%)]\tRecons Loss: 95.456940; KLD Loss: 95.456940\n",
      "====> Epoch: 323 Average recons loss: 3.0328, kld loss: 2.3641\n",
      "====> Test set recons loss: 43.8793, kld_loss: 1.9126\n",
      "Train Epoch: 324 [0/110 (0%)]\tRecons Loss: 96.669235; KLD Loss: 96.669235\n",
      "Train Epoch: 324 [64/110 (50%)]\tRecons Loss: 88.217781; KLD Loss: 88.217781\n",
      "====> Epoch: 324 Average recons loss: 3.3833, kld loss: 2.1599\n",
      "====> Epoch: 324 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 43.7053, kld_loss: 1.9483\n",
      "Train Epoch: 325 [0/110 (0%)]\tRecons Loss: 85.196350; KLD Loss: 85.196350\n",
      "Train Epoch: 325 [64/110 (50%)]\tRecons Loss: 86.756363; KLD Loss: 86.756363\n",
      "====> Epoch: 325 Average recons loss: 3.1445, kld loss: 2.3505\n",
      "====> Test set recons loss: 42.0944, kld_loss: 2.4721\n",
      "Train Epoch: 326 [0/110 (0%)]\tRecons Loss: 80.885880; KLD Loss: 80.885880\n",
      "Train Epoch: 326 [64/110 (50%)]\tRecons Loss: 96.700211; KLD Loss: 96.700211\n",
      "====> Epoch: 326 Average recons loss: 3.2122, kld loss: 2.4098\n",
      "====> Test set recons loss: 43.6951, kld_loss: 1.9435\n",
      "Train Epoch: 327 [0/110 (0%)]\tRecons Loss: 92.959747; KLD Loss: 92.959747\n",
      "Train Epoch: 327 [64/110 (50%)]\tRecons Loss: 85.891907; KLD Loss: 85.891907\n",
      "====> Epoch: 327 Average recons loss: 3.3240, kld loss: 2.2775\n",
      "====> Test set recons loss: 45.1086, kld_loss: 1.9941\n",
      "Train Epoch: 328 [0/110 (0%)]\tRecons Loss: 86.644043; KLD Loss: 86.644043\n",
      "Train Epoch: 328 [64/110 (50%)]\tRecons Loss: 98.451477; KLD Loss: 98.451477\n",
      "====> Epoch: 328 Average recons loss: 3.5022, kld loss: 2.2390\n",
      "====> Test set recons loss: 38.7868, kld_loss: 2.2374\n",
      "Train Epoch: 329 [0/110 (0%)]\tRecons Loss: 75.606689; KLD Loss: 75.606689\n",
      "Train Epoch: 329 [64/110 (50%)]\tRecons Loss: 98.785294; KLD Loss: 98.785294\n",
      "====> Epoch: 329 Average recons loss: 3.2892, kld loss: 2.4176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 43.5685, kld_loss: 2.1139\n",
      "Train Epoch: 330 [0/110 (0%)]\tRecons Loss: 78.723633; KLD Loss: 78.723633\n",
      "Train Epoch: 330 [64/110 (50%)]\tRecons Loss: 99.679199; KLD Loss: 99.679199\n",
      "====> Epoch: 330 Average recons loss: 3.3726, kld loss: 2.2815\n",
      "====> Test set recons loss: 38.3927, kld_loss: 1.8513\n",
      "Train Epoch: 331 [0/110 (0%)]\tRecons Loss: 99.407539; KLD Loss: 99.407539\n",
      "Train Epoch: 331 [64/110 (50%)]\tRecons Loss: 75.551285; KLD Loss: 75.551285\n",
      "====> Epoch: 331 Average recons loss: 3.6200, kld loss: 2.4124\n",
      "====> Test set recons loss: 42.1340, kld_loss: 2.4600\n",
      "Train Epoch: 332 [0/110 (0%)]\tRecons Loss: 96.133774; KLD Loss: 96.133774\n",
      "Train Epoch: 332 [64/110 (50%)]\tRecons Loss: 98.855148; KLD Loss: 98.855148\n",
      "====> Epoch: 332 Average recons loss: 3.3074, kld loss: 2.6311\n",
      "====> Test set recons loss: 37.9223, kld_loss: 2.3361\n",
      "Train Epoch: 333 [0/110 (0%)]\tRecons Loss: 90.139587; KLD Loss: 90.139587\n",
      "Train Epoch: 333 [64/110 (50%)]\tRecons Loss: 91.717484; KLD Loss: 91.717484\n",
      "====> Epoch: 333 Average recons loss: 3.3011, kld loss: 2.4177\n",
      "====> Test set recons loss: 42.8225, kld_loss: 2.0843\n",
      "Train Epoch: 334 [0/110 (0%)]\tRecons Loss: 89.734566; KLD Loss: 89.734566\n",
      "Train Epoch: 334 [64/110 (50%)]\tRecons Loss: 105.184845; KLD Loss: 105.184845\n",
      "====> Epoch: 334 Average recons loss: 3.0806, kld loss: 2.4179\n",
      "====> Epoch: 334 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 44.2123, kld_loss: 2.1889\n",
      "Train Epoch: 335 [0/110 (0%)]\tRecons Loss: 102.810501; KLD Loss: 102.810501\n",
      "Train Epoch: 335 [64/110 (50%)]\tRecons Loss: 99.104813; KLD Loss: 99.104813\n",
      "====> Epoch: 335 Average recons loss: 3.7782, kld loss: 2.3726\n",
      "====> Test set recons loss: 40.8942, kld_loss: 2.4226\n",
      "Train Epoch: 336 [0/110 (0%)]\tRecons Loss: 73.606339; KLD Loss: 73.606339\n",
      "Train Epoch: 336 [64/110 (50%)]\tRecons Loss: 83.854683; KLD Loss: 83.854683\n",
      "====> Epoch: 336 Average recons loss: 2.9744, kld loss: 2.5747\n",
      "====> Test set recons loss: 40.7271, kld_loss: 2.1201\n",
      "Train Epoch: 337 [0/110 (0%)]\tRecons Loss: 92.674637; KLD Loss: 92.674637\n",
      "Train Epoch: 337 [64/110 (50%)]\tRecons Loss: 116.887238; KLD Loss: 116.887238\n",
      "====> Epoch: 337 Average recons loss: 3.6297, kld loss: 2.3224\n",
      "====> Test set recons loss: 42.3649, kld_loss: 2.4568\n",
      "Train Epoch: 338 [0/110 (0%)]\tRecons Loss: 75.305573; KLD Loss: 75.305573\n",
      "Train Epoch: 338 [64/110 (50%)]\tRecons Loss: 77.176674; KLD Loss: 77.176674\n",
      "====> Epoch: 338 Average recons loss: 3.0976, kld loss: 2.5498\n",
      "====> Test set recons loss: 41.0615, kld_loss: 2.4687\n",
      "Train Epoch: 339 [0/110 (0%)]\tRecons Loss: 79.164658; KLD Loss: 79.164658\n",
      "Train Epoch: 339 [64/110 (50%)]\tRecons Loss: 88.306343; KLD Loss: 88.306343\n",
      "====> Epoch: 339 Average recons loss: 3.0775, kld loss: 2.6066\n",
      "====> Test set recons loss: 41.2917, kld_loss: 2.3889\n",
      "Train Epoch: 340 [0/110 (0%)]\tRecons Loss: 85.685822; KLD Loss: 85.685822\n",
      "Train Epoch: 340 [64/110 (50%)]\tRecons Loss: 92.776886; KLD Loss: 92.776886\n",
      "====> Epoch: 340 Average recons loss: 3.3478, kld loss: 2.2495\n",
      "====> Test set recons loss: 42.7118, kld_loss: 2.2563\n",
      "Train Epoch: 341 [0/110 (0%)]\tRecons Loss: 85.756729; KLD Loss: 85.756729\n",
      "Train Epoch: 341 [64/110 (50%)]\tRecons Loss: 82.887924; KLD Loss: 82.887924\n",
      "====> Epoch: 341 Average recons loss: 3.1086, kld loss: 2.4939\n",
      "====> Test set recons loss: 39.3738, kld_loss: 2.3122\n",
      "Train Epoch: 342 [0/110 (0%)]\tRecons Loss: 81.573669; KLD Loss: 81.573669\n",
      "Train Epoch: 342 [64/110 (50%)]\tRecons Loss: 99.474983; KLD Loss: 99.474983\n",
      "====> Epoch: 342 Average recons loss: 3.3911, kld loss: 2.3431\n",
      "====> Test set recons loss: 44.0958, kld_loss: 2.2125\n",
      "Train Epoch: 343 [0/110 (0%)]\tRecons Loss: 80.407539; KLD Loss: 80.407539\n",
      "Train Epoch: 343 [64/110 (50%)]\tRecons Loss: 85.163696; KLD Loss: 85.163696\n",
      "====> Epoch: 343 Average recons loss: 3.0029, kld loss: 2.4948\n",
      "====> Test set recons loss: 41.1727, kld_loss: 2.0235\n",
      "Train Epoch: 344 [0/110 (0%)]\tRecons Loss: 94.578308; KLD Loss: 94.578308\n",
      "Train Epoch: 344 [64/110 (50%)]\tRecons Loss: 100.146759; KLD Loss: 100.146759\n",
      "====> Epoch: 344 Average recons loss: 3.4734, kld loss: 2.2713\n",
      "====> Epoch: 344 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.3674, kld_loss: 2.0886\n",
      "Train Epoch: 345 [0/110 (0%)]\tRecons Loss: 83.371132; KLD Loss: 83.371132\n",
      "Train Epoch: 345 [64/110 (50%)]\tRecons Loss: 89.078445; KLD Loss: 89.078445\n",
      "====> Epoch: 345 Average recons loss: 3.2397, kld loss: 2.5102\n",
      "====> Test set recons loss: 43.1109, kld_loss: 2.3684\n",
      "Train Epoch: 346 [0/110 (0%)]\tRecons Loss: 75.418411; KLD Loss: 75.418411\n",
      "Train Epoch: 346 [64/110 (50%)]\tRecons Loss: 84.452919; KLD Loss: 84.452919\n",
      "====> Epoch: 346 Average recons loss: 3.1355, kld loss: 2.4302\n",
      "====> Test set recons loss: 42.8882, kld_loss: 2.0043\n",
      "Train Epoch: 347 [0/110 (0%)]\tRecons Loss: 89.382950; KLD Loss: 89.382950\n",
      "Train Epoch: 347 [64/110 (50%)]\tRecons Loss: 85.087769; KLD Loss: 85.087769\n",
      "====> Epoch: 347 Average recons loss: 3.4107, kld loss: 2.3427\n",
      "====> Test set recons loss: 42.7096, kld_loss: 2.5683\n",
      "Train Epoch: 348 [0/110 (0%)]\tRecons Loss: 87.540779; KLD Loss: 87.540779\n",
      "Train Epoch: 348 [64/110 (50%)]\tRecons Loss: 84.577667; KLD Loss: 84.577667\n",
      "====> Epoch: 348 Average recons loss: 2.9945, kld loss: 2.6290\n",
      "====> Test set recons loss: 41.7767, kld_loss: 2.1242\n",
      "Train Epoch: 349 [0/110 (0%)]\tRecons Loss: 85.009308; KLD Loss: 85.009308\n",
      "Train Epoch: 349 [64/110 (50%)]\tRecons Loss: 93.907608; KLD Loss: 93.907608\n",
      "====> Epoch: 349 Average recons loss: 3.0133, kld loss: 2.3153\n",
      "====> Test set recons loss: 48.8636, kld_loss: 1.9590\n",
      "Train Epoch: 350 [0/110 (0%)]\tRecons Loss: 90.012764; KLD Loss: 90.012764\n",
      "Train Epoch: 350 [64/110 (50%)]\tRecons Loss: 80.775452; KLD Loss: 80.775452\n",
      "====> Epoch: 350 Average recons loss: 3.1158, kld loss: 2.2142\n",
      "====> Test set recons loss: 42.7889, kld_loss: 2.1038\n",
      "Train Epoch: 351 [0/110 (0%)]\tRecons Loss: 90.804565; KLD Loss: 90.804565\n",
      "Train Epoch: 351 [64/110 (50%)]\tRecons Loss: 75.833786; KLD Loss: 75.833786\n",
      "====> Epoch: 351 Average recons loss: 3.0938, kld loss: 2.3665\n",
      "====> Test set recons loss: 41.6173, kld_loss: 1.9942\n",
      "Train Epoch: 352 [0/110 (0%)]\tRecons Loss: 89.045860; KLD Loss: 89.045860\n",
      "Train Epoch: 352 [64/110 (50%)]\tRecons Loss: 87.461609; KLD Loss: 87.461609\n",
      "====> Epoch: 352 Average recons loss: 2.9590, kld loss: 2.2585\n",
      "====> Test set recons loss: 42.2414, kld_loss: 1.9966\n",
      "Train Epoch: 353 [0/110 (0%)]\tRecons Loss: 78.980080; KLD Loss: 78.980080\n",
      "Train Epoch: 353 [64/110 (50%)]\tRecons Loss: 78.859833; KLD Loss: 78.859833\n",
      "====> Epoch: 353 Average recons loss: 3.0739, kld loss: 2.1590\n",
      "====> Test set recons loss: 42.5902, kld_loss: 1.9937\n",
      "Train Epoch: 354 [0/110 (0%)]\tRecons Loss: 85.115891; KLD Loss: 85.115891\n",
      "Train Epoch: 354 [64/110 (50%)]\tRecons Loss: 82.311844; KLD Loss: 82.311844\n",
      "====> Epoch: 354 Average recons loss: 3.1156, kld loss: 2.1840\n",
      "====> Epoch: 354 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.0075, kld_loss: 1.9638\n",
      "Train Epoch: 355 [0/110 (0%)]\tRecons Loss: 87.350899; KLD Loss: 87.350899\n",
      "Train Epoch: 355 [64/110 (50%)]\tRecons Loss: 68.156601; KLD Loss: 68.156601\n",
      "====> Epoch: 355 Average recons loss: 2.9942, kld loss: 2.2523\n",
      "====> Test set recons loss: 46.9367, kld_loss: 1.8053\n",
      "Train Epoch: 356 [0/110 (0%)]\tRecons Loss: 93.363533; KLD Loss: 93.363533\n",
      "Train Epoch: 356 [64/110 (50%)]\tRecons Loss: 70.558571; KLD Loss: 70.558571\n",
      "====> Epoch: 356 Average recons loss: 3.1733, kld loss: 2.1997\n",
      "====> Test set recons loss: 42.0666, kld_loss: 1.8371\n",
      "Train Epoch: 357 [0/110 (0%)]\tRecons Loss: 87.954193; KLD Loss: 87.954193\n",
      "Train Epoch: 357 [64/110 (50%)]\tRecons Loss: 76.206192; KLD Loss: 76.206192\n",
      "====> Epoch: 357 Average recons loss: 3.1748, kld loss: 2.1847\n",
      "====> Test set recons loss: 40.8399, kld_loss: 1.8631\n",
      "Train Epoch: 358 [0/110 (0%)]\tRecons Loss: 78.231201; KLD Loss: 78.231201\n",
      "Train Epoch: 358 [64/110 (50%)]\tRecons Loss: 85.661469; KLD Loss: 85.661469\n",
      "====> Epoch: 358 Average recons loss: 3.1067, kld loss: 2.0891\n",
      "====> Test set recons loss: 45.5749, kld_loss: 1.8545\n",
      "Train Epoch: 359 [0/110 (0%)]\tRecons Loss: 83.601555; KLD Loss: 83.601555\n",
      "Train Epoch: 359 [64/110 (50%)]\tRecons Loss: 88.781197; KLD Loss: 88.781197\n",
      "====> Epoch: 359 Average recons loss: 2.9711, kld loss: 2.2351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 42.9228, kld_loss: 2.0567\n",
      "Train Epoch: 360 [0/110 (0%)]\tRecons Loss: 80.402153; KLD Loss: 80.402153\n",
      "Train Epoch: 360 [64/110 (50%)]\tRecons Loss: 82.237778; KLD Loss: 82.237778\n",
      "====> Epoch: 360 Average recons loss: 2.9516, kld loss: 2.1803\n",
      "====> Test set recons loss: 39.8106, kld_loss: 1.9193\n",
      "Train Epoch: 361 [0/110 (0%)]\tRecons Loss: 96.985558; KLD Loss: 96.985558\n",
      "Train Epoch: 361 [64/110 (50%)]\tRecons Loss: 86.462753; KLD Loss: 86.462753\n",
      "====> Epoch: 361 Average recons loss: 3.1496, kld loss: 2.1056\n",
      "====> Test set recons loss: 42.3718, kld_loss: 2.0640\n",
      "Train Epoch: 362 [0/110 (0%)]\tRecons Loss: 69.735687; KLD Loss: 69.735687\n",
      "Train Epoch: 362 [64/110 (50%)]\tRecons Loss: 91.054138; KLD Loss: 91.054138\n",
      "====> Epoch: 362 Average recons loss: 3.1231, kld loss: 2.1668\n",
      "====> Test set recons loss: 37.6985, kld_loss: 2.0276\n",
      "Train Epoch: 363 [0/110 (0%)]\tRecons Loss: 87.003876; KLD Loss: 87.003876\n",
      "Train Epoch: 363 [64/110 (50%)]\tRecons Loss: 71.187584; KLD Loss: 71.187584\n",
      "====> Epoch: 363 Average recons loss: 2.7176, kld loss: 2.2884\n",
      "====> Test set recons loss: 40.0338, kld_loss: 1.8702\n",
      "Train Epoch: 364 [0/110 (0%)]\tRecons Loss: 95.837021; KLD Loss: 95.837021\n",
      "Train Epoch: 364 [64/110 (50%)]\tRecons Loss: 90.140427; KLD Loss: 90.140427\n",
      "====> Epoch: 364 Average recons loss: 3.2007, kld loss: 2.0115\n",
      "====> Epoch: 364 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 40.9200, kld_loss: 1.9532\n",
      "Train Epoch: 365 [0/110 (0%)]\tRecons Loss: 81.115143; KLD Loss: 81.115143\n",
      "Train Epoch: 365 [64/110 (50%)]\tRecons Loss: 78.499512; KLD Loss: 78.499512\n",
      "====> Epoch: 365 Average recons loss: 2.8671, kld loss: 2.2950\n",
      "====> Test set recons loss: 40.9928, kld_loss: 2.0640\n",
      "Train Epoch: 366 [0/110 (0%)]\tRecons Loss: 77.153320; KLD Loss: 77.153320\n",
      "Train Epoch: 366 [64/110 (50%)]\tRecons Loss: 85.486755; KLD Loss: 85.486755\n",
      "====> Epoch: 366 Average recons loss: 3.0112, kld loss: 2.0811\n",
      "====> Test set recons loss: 45.7503, kld_loss: 1.8475\n",
      "Train Epoch: 367 [0/110 (0%)]\tRecons Loss: 82.167946; KLD Loss: 82.167946\n",
      "Train Epoch: 367 [64/110 (50%)]\tRecons Loss: 77.526344; KLD Loss: 77.526344\n",
      "====> Epoch: 367 Average recons loss: 2.8772, kld loss: 2.0413\n",
      "====> Test set recons loss: 39.9223, kld_loss: 1.8685\n",
      "Train Epoch: 368 [0/110 (0%)]\tRecons Loss: 82.339951; KLD Loss: 82.339951\n",
      "Train Epoch: 368 [64/110 (50%)]\tRecons Loss: 74.823959; KLD Loss: 74.823959\n",
      "====> Epoch: 368 Average recons loss: 3.2261, kld loss: 2.1891\n",
      "====> Test set recons loss: 41.8202, kld_loss: 2.0893\n",
      "Train Epoch: 369 [0/110 (0%)]\tRecons Loss: 79.854607; KLD Loss: 79.854607\n",
      "Train Epoch: 369 [64/110 (50%)]\tRecons Loss: 74.846184; KLD Loss: 74.846184\n",
      "====> Epoch: 369 Average recons loss: 2.8283, kld loss: 2.2709\n",
      "====> Test set recons loss: 39.6635, kld_loss: 1.9527\n",
      "Train Epoch: 370 [0/110 (0%)]\tRecons Loss: 70.239288; KLD Loss: 70.239288\n",
      "Train Epoch: 370 [64/110 (50%)]\tRecons Loss: 82.584015; KLD Loss: 82.584015\n",
      "====> Epoch: 370 Average recons loss: 2.7627, kld loss: 2.0912\n",
      "====> Test set recons loss: 42.8656, kld_loss: 1.9023\n",
      "Train Epoch: 371 [0/110 (0%)]\tRecons Loss: 69.479645; KLD Loss: 69.479645\n",
      "Train Epoch: 371 [64/110 (50%)]\tRecons Loss: 74.688469; KLD Loss: 74.688469\n",
      "====> Epoch: 371 Average recons loss: 2.8602, kld loss: 2.0962\n",
      "====> Test set recons loss: 41.4668, kld_loss: 1.9549\n",
      "Train Epoch: 372 [0/110 (0%)]\tRecons Loss: 77.999817; KLD Loss: 77.999817\n",
      "Train Epoch: 372 [64/110 (50%)]\tRecons Loss: 74.947853; KLD Loss: 74.947853\n",
      "====> Epoch: 372 Average recons loss: 2.5865, kld loss: 2.2224\n",
      "====> Test set recons loss: 38.1285, kld_loss: 1.8126\n",
      "Train Epoch: 373 [0/110 (0%)]\tRecons Loss: 78.974319; KLD Loss: 78.974319\n",
      "Train Epoch: 373 [64/110 (50%)]\tRecons Loss: 78.618149; KLD Loss: 78.618149\n",
      "====> Epoch: 373 Average recons loss: 2.7635, kld loss: 1.9602\n",
      "====> Test set recons loss: 45.0805, kld_loss: 1.7914\n",
      "Train Epoch: 374 [0/110 (0%)]\tRecons Loss: 76.093925; KLD Loss: 76.093925\n",
      "Train Epoch: 374 [64/110 (50%)]\tRecons Loss: 78.427261; KLD Loss: 78.427261\n",
      "====> Epoch: 374 Average recons loss: 2.9058, kld loss: 1.9808\n",
      "====> Epoch: 374 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.9638, kld_loss: 1.7826\n",
      "Train Epoch: 375 [0/110 (0%)]\tRecons Loss: 76.795059; KLD Loss: 76.795059\n",
      "Train Epoch: 375 [64/110 (50%)]\tRecons Loss: 71.092987; KLD Loss: 71.092987\n",
      "====> Epoch: 375 Average recons loss: 2.5822, kld loss: 2.1014\n",
      "====> Test set recons loss: 43.7011, kld_loss: 1.7411\n",
      "Train Epoch: 376 [0/110 (0%)]\tRecons Loss: 81.342361; KLD Loss: 81.342361\n",
      "Train Epoch: 376 [64/110 (50%)]\tRecons Loss: 81.237968; KLD Loss: 81.237968\n",
      "====> Epoch: 376 Average recons loss: 2.8447, kld loss: 1.9037\n",
      "====> Test set recons loss: 41.7179, kld_loss: 1.7729\n",
      "Train Epoch: 377 [0/110 (0%)]\tRecons Loss: 85.325356; KLD Loss: 85.325356\n",
      "Train Epoch: 377 [64/110 (50%)]\tRecons Loss: 83.545654; KLD Loss: 83.545654\n",
      "====> Epoch: 377 Average recons loss: 3.1470, kld loss: 1.9607\n",
      "====> Test set recons loss: 40.3053, kld_loss: 1.9401\n",
      "Train Epoch: 378 [0/110 (0%)]\tRecons Loss: 73.437645; KLD Loss: 73.437645\n",
      "Train Epoch: 378 [64/110 (50%)]\tRecons Loss: 84.723480; KLD Loss: 84.723480\n",
      "====> Epoch: 378 Average recons loss: 2.8444, kld loss: 2.1908\n",
      "====> Test set recons loss: 40.2275, kld_loss: 1.8120\n",
      "Train Epoch: 379 [0/110 (0%)]\tRecons Loss: 82.725151; KLD Loss: 82.725151\n",
      "Train Epoch: 379 [64/110 (50%)]\tRecons Loss: 80.552795; KLD Loss: 80.552795\n",
      "====> Epoch: 379 Average recons loss: 2.7744, kld loss: 2.0422\n",
      "====> Test set recons loss: 44.9678, kld_loss: 1.7881\n",
      "Train Epoch: 380 [0/110 (0%)]\tRecons Loss: 91.716690; KLD Loss: 91.716690\n",
      "Train Epoch: 380 [64/110 (50%)]\tRecons Loss: 78.918251; KLD Loss: 78.918251\n",
      "====> Epoch: 380 Average recons loss: 2.8855, kld loss: 1.9893\n",
      "====> Test set recons loss: 39.6760, kld_loss: 1.8578\n",
      "Train Epoch: 381 [0/110 (0%)]\tRecons Loss: 86.747078; KLD Loss: 86.747078\n",
      "Train Epoch: 381 [64/110 (50%)]\tRecons Loss: 62.432743; KLD Loss: 62.432743\n",
      "====> Epoch: 381 Average recons loss: 2.7140, kld loss: 2.1075\n",
      "====> Test set recons loss: 43.1727, kld_loss: 1.8498\n",
      "Train Epoch: 382 [0/110 (0%)]\tRecons Loss: 85.032730; KLD Loss: 85.032730\n",
      "Train Epoch: 382 [64/110 (50%)]\tRecons Loss: 76.092041; KLD Loss: 76.092041\n",
      "====> Epoch: 382 Average recons loss: 2.9007, kld loss: 1.9313\n",
      "====> Test set recons loss: 45.3265, kld_loss: 1.9218\n",
      "Train Epoch: 383 [0/110 (0%)]\tRecons Loss: 80.302116; KLD Loss: 80.302116\n",
      "Train Epoch: 383 [64/110 (50%)]\tRecons Loss: 80.164719; KLD Loss: 80.164719\n",
      "====> Epoch: 383 Average recons loss: 3.0668, kld loss: 1.9380\n",
      "====> Test set recons loss: 43.1432, kld_loss: 1.9617\n",
      "Train Epoch: 384 [0/110 (0%)]\tRecons Loss: 77.337601; KLD Loss: 77.337601\n",
      "Train Epoch: 384 [64/110 (50%)]\tRecons Loss: 65.494217; KLD Loss: 65.494217\n",
      "====> Epoch: 384 Average recons loss: 2.6208, kld loss: 2.3209\n",
      "====> Epoch: 384 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 39.6915, kld_loss: 1.9088\n",
      "Train Epoch: 385 [0/110 (0%)]\tRecons Loss: 70.964394; KLD Loss: 70.964394\n",
      "Train Epoch: 385 [64/110 (50%)]\tRecons Loss: 85.966904; KLD Loss: 85.966904\n",
      "====> Epoch: 385 Average recons loss: 2.8660, kld loss: 2.0067\n",
      "====> Test set recons loss: 42.4754, kld_loss: 1.9633\n",
      "Train Epoch: 386 [0/110 (0%)]\tRecons Loss: 63.638420; KLD Loss: 63.638420\n",
      "Train Epoch: 386 [64/110 (50%)]\tRecons Loss: 75.581367; KLD Loss: 75.581367\n",
      "====> Epoch: 386 Average recons loss: 2.5386, kld loss: 2.0373\n",
      "====> Test set recons loss: 40.0339, kld_loss: 1.9995\n",
      "Train Epoch: 387 [0/110 (0%)]\tRecons Loss: 72.725800; KLD Loss: 72.725800\n",
      "Train Epoch: 387 [64/110 (50%)]\tRecons Loss: 72.302200; KLD Loss: 72.302200\n",
      "====> Epoch: 387 Average recons loss: 2.6948, kld loss: 1.9605\n",
      "====> Test set recons loss: 43.9671, kld_loss: 1.8800\n",
      "Train Epoch: 388 [0/110 (0%)]\tRecons Loss: 88.244247; KLD Loss: 88.244247\n",
      "Train Epoch: 388 [64/110 (50%)]\tRecons Loss: 64.318253; KLD Loss: 64.318253\n",
      "====> Epoch: 388 Average recons loss: 2.7415, kld loss: 1.9197\n",
      "====> Test set recons loss: 41.5415, kld_loss: 1.7905\n",
      "Train Epoch: 389 [0/110 (0%)]\tRecons Loss: 83.271515; KLD Loss: 83.271515\n",
      "Train Epoch: 389 [64/110 (50%)]\tRecons Loss: 69.089233; KLD Loss: 69.089233\n",
      "====> Epoch: 389 Average recons loss: 2.8293, kld loss: 1.9047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 38.8850, kld_loss: 1.7529\n",
      "Train Epoch: 390 [0/110 (0%)]\tRecons Loss: 75.418762; KLD Loss: 75.418762\n",
      "Train Epoch: 390 [64/110 (50%)]\tRecons Loss: 67.706970; KLD Loss: 67.706970\n",
      "====> Epoch: 390 Average recons loss: 2.6803, kld loss: 1.9559\n",
      "====> Test set recons loss: 42.7978, kld_loss: 1.7432\n",
      "Train Epoch: 391 [0/110 (0%)]\tRecons Loss: 69.329361; KLD Loss: 69.329361\n",
      "Train Epoch: 391 [64/110 (50%)]\tRecons Loss: 67.679123; KLD Loss: 67.679123\n",
      "====> Epoch: 391 Average recons loss: 2.7603, kld loss: 2.0615\n",
      "====> Test set recons loss: 44.3822, kld_loss: 1.8872\n",
      "Train Epoch: 392 [0/110 (0%)]\tRecons Loss: 74.416000; KLD Loss: 74.416000\n",
      "Train Epoch: 392 [64/110 (50%)]\tRecons Loss: 77.244934; KLD Loss: 77.244934\n",
      "====> Epoch: 392 Average recons loss: 2.7954, kld loss: 2.0197\n",
      "====> Test set recons loss: 44.9315, kld_loss: 1.8992\n",
      "Train Epoch: 393 [0/110 (0%)]\tRecons Loss: 64.325241; KLD Loss: 64.325241\n",
      "Train Epoch: 393 [64/110 (50%)]\tRecons Loss: 81.166748; KLD Loss: 81.166748\n",
      "====> Epoch: 393 Average recons loss: 2.7870, kld loss: 1.8745\n",
      "====> Test set recons loss: 39.7616, kld_loss: 1.8728\n",
      "Train Epoch: 394 [0/110 (0%)]\tRecons Loss: 77.837906; KLD Loss: 77.837906\n",
      "Train Epoch: 394 [64/110 (50%)]\tRecons Loss: 71.615875; KLD Loss: 71.615875\n",
      "====> Epoch: 394 Average recons loss: 2.7040, kld loss: 2.0796\n",
      "====> Epoch: 394 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 45.1947, kld_loss: 1.7803\n",
      "Train Epoch: 395 [0/110 (0%)]\tRecons Loss: 66.275284; KLD Loss: 66.275284\n",
      "Train Epoch: 395 [64/110 (50%)]\tRecons Loss: 80.115639; KLD Loss: 80.115639\n",
      "====> Epoch: 395 Average recons loss: 2.9464, kld loss: 1.9354\n",
      "====> Test set recons loss: 38.7356, kld_loss: 1.8228\n",
      "Train Epoch: 396 [0/110 (0%)]\tRecons Loss: 80.706253; KLD Loss: 80.706253\n",
      "Train Epoch: 396 [64/110 (50%)]\tRecons Loss: 78.844849; KLD Loss: 78.844849\n",
      "====> Epoch: 396 Average recons loss: 2.8256, kld loss: 2.0028\n",
      "====> Test set recons loss: 41.1368, kld_loss: 1.8498\n",
      "Train Epoch: 397 [0/110 (0%)]\tRecons Loss: 77.210526; KLD Loss: 77.210526\n",
      "Train Epoch: 397 [64/110 (50%)]\tRecons Loss: 68.158173; KLD Loss: 68.158173\n",
      "====> Epoch: 397 Average recons loss: 2.7446, kld loss: 2.0791\n",
      "====> Test set recons loss: 45.8932, kld_loss: 1.8138\n",
      "Train Epoch: 398 [0/110 (0%)]\tRecons Loss: 68.992798; KLD Loss: 68.992798\n",
      "Train Epoch: 398 [64/110 (50%)]\tRecons Loss: 68.217987; KLD Loss: 68.217987\n",
      "====> Epoch: 398 Average recons loss: 2.6792, kld loss: 1.9249\n",
      "====> Test set recons loss: 43.0774, kld_loss: 1.8099\n",
      "Train Epoch: 399 [0/110 (0%)]\tRecons Loss: 74.095657; KLD Loss: 74.095657\n",
      "Train Epoch: 399 [64/110 (50%)]\tRecons Loss: 70.359177; KLD Loss: 70.359177\n",
      "====> Epoch: 399 Average recons loss: 2.5421, kld loss: 2.0244\n",
      "====> Test set recons loss: 41.8472, kld_loss: 1.8516\n",
      "Train Epoch: 400 [0/110 (0%)]\tRecons Loss: 82.116913; KLD Loss: 82.116913\n",
      "Train Epoch: 400 [64/110 (50%)]\tRecons Loss: 72.341515; KLD Loss: 72.341515\n",
      "====> Epoch: 400 Average recons loss: 2.6076, kld loss: 1.9610\n",
      "====> Test set recons loss: 44.8698, kld_loss: 1.9197\n",
      "Train Epoch: 401 [0/110 (0%)]\tRecons Loss: 69.752274; KLD Loss: 69.752274\n",
      "Train Epoch: 401 [64/110 (50%)]\tRecons Loss: 84.335701; KLD Loss: 84.335701\n",
      "====> Epoch: 401 Average recons loss: 2.9711, kld loss: 1.8767\n",
      "====> Test set recons loss: 40.4709, kld_loss: 1.8907\n",
      "Train Epoch: 402 [0/110 (0%)]\tRecons Loss: 72.667458; KLD Loss: 72.667458\n",
      "Train Epoch: 402 [64/110 (50%)]\tRecons Loss: 76.674843; KLD Loss: 76.674843\n",
      "====> Epoch: 402 Average recons loss: 2.8668, kld loss: 2.1125\n",
      "====> Test set recons loss: 43.1582, kld_loss: 1.8648\n",
      "Train Epoch: 403 [0/110 (0%)]\tRecons Loss: 65.129265; KLD Loss: 65.129265\n",
      "Train Epoch: 403 [64/110 (50%)]\tRecons Loss: 78.726013; KLD Loss: 78.726013\n",
      "====> Epoch: 403 Average recons loss: 2.6464, kld loss: 2.0993\n",
      "====> Test set recons loss: 39.5107, kld_loss: 1.8257\n",
      "Train Epoch: 404 [0/110 (0%)]\tRecons Loss: 71.262413; KLD Loss: 71.262413\n",
      "Train Epoch: 404 [64/110 (50%)]\tRecons Loss: 99.704170; KLD Loss: 99.704170\n",
      "====> Epoch: 404 Average recons loss: 3.0744, kld loss: 1.9913\n",
      "====> Epoch: 404 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.2556, kld_loss: 1.8454\n",
      "Train Epoch: 405 [0/110 (0%)]\tRecons Loss: 77.655060; KLD Loss: 77.655060\n",
      "Train Epoch: 405 [64/110 (50%)]\tRecons Loss: 63.615189; KLD Loss: 63.615189\n",
      "====> Epoch: 405 Average recons loss: 2.5219, kld loss: 2.1486\n",
      "====> Test set recons loss: 43.7414, kld_loss: 1.8337\n",
      "Train Epoch: 406 [0/110 (0%)]\tRecons Loss: 64.766296; KLD Loss: 64.766296\n",
      "Train Epoch: 406 [64/110 (50%)]\tRecons Loss: 74.118378; KLD Loss: 74.118378\n",
      "====> Epoch: 406 Average recons loss: 2.6553, kld loss: 1.9524\n",
      "====> Test set recons loss: 46.3042, kld_loss: 1.9072\n",
      "Train Epoch: 407 [0/110 (0%)]\tRecons Loss: 82.810791; KLD Loss: 82.810791\n",
      "Train Epoch: 407 [64/110 (50%)]\tRecons Loss: 65.693710; KLD Loss: 65.693710\n",
      "====> Epoch: 407 Average recons loss: 2.6773, kld loss: 1.8828\n",
      "====> Test set recons loss: 41.6072, kld_loss: 1.9119\n",
      "Train Epoch: 408 [0/110 (0%)]\tRecons Loss: 73.977539; KLD Loss: 73.977539\n",
      "Train Epoch: 408 [64/110 (50%)]\tRecons Loss: 67.335709; KLD Loss: 67.335709\n",
      "====> Epoch: 408 Average recons loss: 2.5582, kld loss: 1.9976\n",
      "====> Test set recons loss: 42.9905, kld_loss: 1.9061\n",
      "Train Epoch: 409 [0/110 (0%)]\tRecons Loss: 76.201088; KLD Loss: 76.201088\n",
      "Train Epoch: 409 [64/110 (50%)]\tRecons Loss: 66.911392; KLD Loss: 66.911392\n",
      "====> Epoch: 409 Average recons loss: 2.7514, kld loss: 1.9600\n",
      "====> Test set recons loss: 43.0867, kld_loss: 1.8620\n",
      "Train Epoch: 410 [0/110 (0%)]\tRecons Loss: 67.783669; KLD Loss: 67.783669\n",
      "Train Epoch: 410 [64/110 (50%)]\tRecons Loss: 71.845604; KLD Loss: 71.845604\n",
      "====> Epoch: 410 Average recons loss: 2.7416, kld loss: 1.9392\n",
      "====> Test set recons loss: 41.9280, kld_loss: 1.7805\n",
      "Train Epoch: 411 [0/110 (0%)]\tRecons Loss: 60.930126; KLD Loss: 60.930126\n",
      "Train Epoch: 411 [64/110 (50%)]\tRecons Loss: 68.228394; KLD Loss: 68.228394\n",
      "====> Epoch: 411 Average recons loss: 2.5252, kld loss: 1.9860\n",
      "====> Test set recons loss: 40.0624, kld_loss: 1.8142\n",
      "Train Epoch: 412 [0/110 (0%)]\tRecons Loss: 70.276344; KLD Loss: 70.276344\n",
      "Train Epoch: 412 [64/110 (50%)]\tRecons Loss: 80.544548; KLD Loss: 80.544548\n",
      "====> Epoch: 412 Average recons loss: 2.5993, kld loss: 1.9438\n",
      "====> Test set recons loss: 41.3849, kld_loss: 1.8689\n",
      "Train Epoch: 413 [0/110 (0%)]\tRecons Loss: 64.182243; KLD Loss: 64.182243\n",
      "Train Epoch: 413 [64/110 (50%)]\tRecons Loss: 72.197342; KLD Loss: 72.197342\n",
      "====> Epoch: 413 Average recons loss: 2.5103, kld loss: 1.9162\n",
      "====> Test set recons loss: 41.8569, kld_loss: 1.9315\n",
      "Train Epoch: 414 [0/110 (0%)]\tRecons Loss: 76.342255; KLD Loss: 76.342255\n",
      "Train Epoch: 414 [64/110 (50%)]\tRecons Loss: 74.523865; KLD Loss: 74.523865\n",
      "====> Epoch: 414 Average recons loss: 2.6603, kld loss: 1.8458\n",
      "====> Epoch: 414 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 43.6894, kld_loss: 1.8182\n",
      "Train Epoch: 415 [0/110 (0%)]\tRecons Loss: 75.816605; KLD Loss: 75.816605\n",
      "Train Epoch: 415 [64/110 (50%)]\tRecons Loss: 69.302917; KLD Loss: 69.302917\n",
      "====> Epoch: 415 Average recons loss: 2.5048, kld loss: 1.9444\n",
      "====> Test set recons loss: 40.2022, kld_loss: 1.8033\n",
      "Train Epoch: 416 [0/110 (0%)]\tRecons Loss: 77.963493; KLD Loss: 77.963493\n",
      "Train Epoch: 416 [64/110 (50%)]\tRecons Loss: 75.352806; KLD Loss: 75.352806\n",
      "====> Epoch: 416 Average recons loss: 3.0432, kld loss: 1.8020\n",
      "====> Test set recons loss: 44.6914, kld_loss: 1.7760\n",
      "Train Epoch: 417 [0/110 (0%)]\tRecons Loss: 72.734802; KLD Loss: 72.734802\n",
      "Train Epoch: 417 [64/110 (50%)]\tRecons Loss: 70.025101; KLD Loss: 70.025101\n",
      "====> Epoch: 417 Average recons loss: 2.5036, kld loss: 1.9489\n",
      "====> Test set recons loss: 45.0355, kld_loss: 1.7670\n",
      "Train Epoch: 418 [0/110 (0%)]\tRecons Loss: 75.115303; KLD Loss: 75.115303\n",
      "Train Epoch: 418 [64/110 (50%)]\tRecons Loss: 76.704117; KLD Loss: 76.704117\n",
      "====> Epoch: 418 Average recons loss: 2.7836, kld loss: 1.9238\n",
      "====> Test set recons loss: 46.8786, kld_loss: 1.8423\n",
      "Train Epoch: 419 [0/110 (0%)]\tRecons Loss: 75.863586; KLD Loss: 75.863586\n",
      "Train Epoch: 419 [64/110 (50%)]\tRecons Loss: 71.075043; KLD Loss: 71.075043\n",
      "====> Epoch: 419 Average recons loss: 2.6545, kld loss: 1.9939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 41.7056, kld_loss: 1.8632\n",
      "Train Epoch: 420 [0/110 (0%)]\tRecons Loss: 72.104492; KLD Loss: 72.104492\n",
      "Train Epoch: 420 [64/110 (50%)]\tRecons Loss: 69.301941; KLD Loss: 69.301941\n",
      "====> Epoch: 420 Average recons loss: 2.6680, kld loss: 1.9688\n",
      "====> Test set recons loss: 41.8257, kld_loss: 1.9242\n",
      "Train Epoch: 421 [0/110 (0%)]\tRecons Loss: 75.328613; KLD Loss: 75.328613\n",
      "Train Epoch: 421 [64/110 (50%)]\tRecons Loss: 77.982094; KLD Loss: 77.982094\n",
      "====> Epoch: 421 Average recons loss: 2.7547, kld loss: 1.9821\n",
      "====> Test set recons loss: 40.8772, kld_loss: 1.9060\n",
      "Train Epoch: 422 [0/110 (0%)]\tRecons Loss: 79.190292; KLD Loss: 79.190292\n",
      "Train Epoch: 422 [64/110 (50%)]\tRecons Loss: 68.261559; KLD Loss: 68.261559\n",
      "====> Epoch: 422 Average recons loss: 2.5304, kld loss: 2.1186\n",
      "====> Test set recons loss: 44.7621, kld_loss: 1.9294\n",
      "Train Epoch: 423 [0/110 (0%)]\tRecons Loss: 60.396660; KLD Loss: 60.396660\n",
      "Train Epoch: 423 [64/110 (50%)]\tRecons Loss: 66.558617; KLD Loss: 66.558617\n",
      "====> Epoch: 423 Average recons loss: 2.5751, kld loss: 1.9816\n",
      "====> Test set recons loss: 38.4895, kld_loss: 1.9634\n",
      "Train Epoch: 424 [0/110 (0%)]\tRecons Loss: 72.306458; KLD Loss: 72.306458\n",
      "Train Epoch: 424 [64/110 (50%)]\tRecons Loss: 74.507370; KLD Loss: 74.507370\n",
      "====> Epoch: 424 Average recons loss: 2.5626, kld loss: 1.9365\n",
      "====> Epoch: 424 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.8424, kld_loss: 1.8442\n",
      "Train Epoch: 425 [0/110 (0%)]\tRecons Loss: 73.462067; KLD Loss: 73.462067\n",
      "Train Epoch: 425 [64/110 (50%)]\tRecons Loss: 77.596497; KLD Loss: 77.596497\n",
      "====> Epoch: 425 Average recons loss: 2.6138, kld loss: 1.9833\n",
      "====> Test set recons loss: 38.9163, kld_loss: 1.8799\n",
      "Train Epoch: 426 [0/110 (0%)]\tRecons Loss: 70.331192; KLD Loss: 70.331192\n",
      "Train Epoch: 426 [64/110 (50%)]\tRecons Loss: 69.780197; KLD Loss: 69.780197\n",
      "====> Epoch: 426 Average recons loss: 2.5803, kld loss: 1.9155\n",
      "====> Test set recons loss: 45.1500, kld_loss: 1.9077\n",
      "Train Epoch: 427 [0/110 (0%)]\tRecons Loss: 62.954121; KLD Loss: 62.954121\n",
      "Train Epoch: 427 [64/110 (50%)]\tRecons Loss: 76.734009; KLD Loss: 76.734009\n",
      "====> Epoch: 427 Average recons loss: 2.5253, kld loss: 1.9126\n",
      "====> Test set recons loss: 38.2551, kld_loss: 1.8584\n",
      "Train Epoch: 428 [0/110 (0%)]\tRecons Loss: 78.504639; KLD Loss: 78.504639\n",
      "Train Epoch: 428 [64/110 (50%)]\tRecons Loss: 67.794991; KLD Loss: 67.794991\n",
      "====> Epoch: 428 Average recons loss: 2.4235, kld loss: 2.0385\n",
      "====> Test set recons loss: 37.7247, kld_loss: 1.8413\n",
      "Train Epoch: 429 [0/110 (0%)]\tRecons Loss: 63.547600; KLD Loss: 63.547600\n",
      "Train Epoch: 429 [64/110 (50%)]\tRecons Loss: 91.334717; KLD Loss: 91.334717\n",
      "====> Epoch: 429 Average recons loss: 2.8091, kld loss: 1.7707\n",
      "====> Test set recons loss: 43.3746, kld_loss: 1.8319\n",
      "Train Epoch: 430 [0/110 (0%)]\tRecons Loss: 66.614830; KLD Loss: 66.614830\n",
      "Train Epoch: 430 [64/110 (50%)]\tRecons Loss: 59.109383; KLD Loss: 59.109383\n",
      "====> Epoch: 430 Average recons loss: 2.4623, kld loss: 1.9202\n",
      "====> Test set recons loss: 43.7069, kld_loss: 1.8024\n",
      "Train Epoch: 431 [0/110 (0%)]\tRecons Loss: 62.706516; KLD Loss: 62.706516\n",
      "Train Epoch: 431 [64/110 (50%)]\tRecons Loss: 78.096191; KLD Loss: 78.096191\n",
      "====> Epoch: 431 Average recons loss: 2.5418, kld loss: 1.9296\n",
      "====> Test set recons loss: 42.7157, kld_loss: 1.8482\n",
      "Train Epoch: 432 [0/110 (0%)]\tRecons Loss: 62.890282; KLD Loss: 62.890282\n",
      "Train Epoch: 432 [64/110 (50%)]\tRecons Loss: 74.221245; KLD Loss: 74.221245\n",
      "====> Epoch: 432 Average recons loss: 2.3503, kld loss: 1.8377\n",
      "====> Test set recons loss: 44.4710, kld_loss: 1.9728\n",
      "Train Epoch: 433 [0/110 (0%)]\tRecons Loss: 72.740761; KLD Loss: 72.740761\n",
      "Train Epoch: 433 [64/110 (50%)]\tRecons Loss: 67.901062; KLD Loss: 67.901062\n",
      "====> Epoch: 433 Average recons loss: 2.4658, kld loss: 1.8891\n",
      "====> Test set recons loss: 43.3033, kld_loss: 1.8051\n",
      "Train Epoch: 434 [0/110 (0%)]\tRecons Loss: 62.765877; KLD Loss: 62.765877\n",
      "Train Epoch: 434 [64/110 (50%)]\tRecons Loss: 58.694702; KLD Loss: 58.694702\n",
      "====> Epoch: 434 Average recons loss: 2.2634, kld loss: 1.8182\n",
      "====> Epoch: 434 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 38.7146, kld_loss: 2.1107\n",
      "Train Epoch: 435 [0/110 (0%)]\tRecons Loss: 66.561554; KLD Loss: 66.561554\n",
      "Train Epoch: 435 [64/110 (50%)]\tRecons Loss: 73.576752; KLD Loss: 73.576752\n",
      "====> Epoch: 435 Average recons loss: 2.7129, kld loss: 1.7069\n",
      "====> Test set recons loss: 43.0521, kld_loss: 1.7369\n",
      "Train Epoch: 436 [0/110 (0%)]\tRecons Loss: 66.996780; KLD Loss: 66.996780\n",
      "Train Epoch: 436 [64/110 (50%)]\tRecons Loss: 65.518730; KLD Loss: 65.518730\n",
      "====> Epoch: 436 Average recons loss: 2.3309, kld loss: 2.0187\n",
      "====> Test set recons loss: 45.3150, kld_loss: 1.7086\n",
      "Train Epoch: 437 [0/110 (0%)]\tRecons Loss: 70.125908; KLD Loss: 70.125908\n",
      "Train Epoch: 437 [64/110 (50%)]\tRecons Loss: 71.074585; KLD Loss: 71.074585\n",
      "====> Epoch: 437 Average recons loss: 2.7419, kld loss: 1.7872\n",
      "====> Test set recons loss: 40.1120, kld_loss: 1.8032\n",
      "Train Epoch: 438 [0/110 (0%)]\tRecons Loss: 73.354271; KLD Loss: 73.354271\n",
      "Train Epoch: 438 [64/110 (50%)]\tRecons Loss: 63.363529; KLD Loss: 63.363529\n",
      "====> Epoch: 438 Average recons loss: 2.4916, kld loss: 1.9471\n",
      "====> Test set recons loss: 40.0029, kld_loss: 1.7956\n",
      "Train Epoch: 439 [0/110 (0%)]\tRecons Loss: 67.398331; KLD Loss: 67.398331\n",
      "Train Epoch: 439 [64/110 (50%)]\tRecons Loss: 75.451294; KLD Loss: 75.451294\n",
      "====> Epoch: 439 Average recons loss: 2.5681, kld loss: 2.0736\n",
      "====> Test set recons loss: 41.6053, kld_loss: 1.8471\n",
      "Train Epoch: 440 [0/110 (0%)]\tRecons Loss: 78.981644; KLD Loss: 78.981644\n",
      "Train Epoch: 440 [64/110 (50%)]\tRecons Loss: 87.971474; KLD Loss: 87.971474\n",
      "====> Epoch: 440 Average recons loss: 2.7477, kld loss: 1.9792\n",
      "====> Test set recons loss: 38.8421, kld_loss: 1.9223\n",
      "Train Epoch: 441 [0/110 (0%)]\tRecons Loss: 82.708046; KLD Loss: 82.708046\n",
      "Train Epoch: 441 [64/110 (50%)]\tRecons Loss: 66.158447; KLD Loss: 66.158447\n",
      "====> Epoch: 441 Average recons loss: 2.6842, kld loss: 1.9429\n",
      "====> Test set recons loss: 39.1672, kld_loss: 1.8986\n",
      "Train Epoch: 442 [0/110 (0%)]\tRecons Loss: 70.524933; KLD Loss: 70.524933\n",
      "Train Epoch: 442 [64/110 (50%)]\tRecons Loss: 71.551834; KLD Loss: 71.551834\n",
      "====> Epoch: 442 Average recons loss: 2.5347, kld loss: 1.9352\n",
      "====> Test set recons loss: 43.1676, kld_loss: 1.9720\n",
      "Train Epoch: 443 [0/110 (0%)]\tRecons Loss: 61.754562; KLD Loss: 61.754562\n",
      "Train Epoch: 443 [64/110 (50%)]\tRecons Loss: 74.719620; KLD Loss: 74.719620\n",
      "====> Epoch: 443 Average recons loss: 2.5641, kld loss: 1.8969\n",
      "====> Test set recons loss: 39.4435, kld_loss: 1.9799\n",
      "Train Epoch: 444 [0/110 (0%)]\tRecons Loss: 77.561409; KLD Loss: 77.561409\n",
      "Train Epoch: 444 [64/110 (50%)]\tRecons Loss: 68.153748; KLD Loss: 68.153748\n",
      "====> Epoch: 444 Average recons loss: 2.5155, kld loss: 1.9932\n",
      "====> Epoch: 444 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.9682, kld_loss: 1.9911\n",
      "Train Epoch: 445 [0/110 (0%)]\tRecons Loss: 59.260323; KLD Loss: 59.260323\n",
      "Train Epoch: 445 [64/110 (50%)]\tRecons Loss: 75.021858; KLD Loss: 75.021858\n",
      "====> Epoch: 445 Average recons loss: 2.3564, kld loss: 1.8778\n",
      "====> Test set recons loss: 40.8161, kld_loss: 2.2945\n",
      "Train Epoch: 446 [0/110 (0%)]\tRecons Loss: 68.711174; KLD Loss: 68.711174\n",
      "Train Epoch: 446 [64/110 (50%)]\tRecons Loss: 63.319527; KLD Loss: 63.319527\n",
      "====> Epoch: 446 Average recons loss: 2.3491, kld loss: 1.7681\n",
      "====> Test set recons loss: 40.6362, kld_loss: 2.0662\n",
      "Train Epoch: 447 [0/110 (0%)]\tRecons Loss: 65.548790; KLD Loss: 65.548790\n",
      "Train Epoch: 447 [64/110 (50%)]\tRecons Loss: 70.045349; KLD Loss: 70.045349\n",
      "====> Epoch: 447 Average recons loss: 2.5021, kld loss: 1.7512\n",
      "====> Test set recons loss: 43.7225, kld_loss: 1.8660\n",
      "Train Epoch: 448 [0/110 (0%)]\tRecons Loss: 59.564381; KLD Loss: 59.564381\n",
      "Train Epoch: 448 [64/110 (50%)]\tRecons Loss: 73.332108; KLD Loss: 73.332108\n",
      "====> Epoch: 448 Average recons loss: 2.4841, kld loss: 1.8265\n",
      "====> Test set recons loss: 43.9384, kld_loss: 1.7644\n",
      "Train Epoch: 449 [0/110 (0%)]\tRecons Loss: 53.177189; KLD Loss: 53.177189\n",
      "Train Epoch: 449 [64/110 (50%)]\tRecons Loss: 70.187134; KLD Loss: 70.187134\n",
      "====> Epoch: 449 Average recons loss: 2.3723, kld loss: 1.9059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 44.8156, kld_loss: 1.8758\n",
      "Train Epoch: 450 [0/110 (0%)]\tRecons Loss: 72.776825; KLD Loss: 72.776825\n",
      "Train Epoch: 450 [64/110 (50%)]\tRecons Loss: 61.695877; KLD Loss: 61.695877\n",
      "====> Epoch: 450 Average recons loss: 2.4770, kld loss: 1.9676\n",
      "====> Test set recons loss: 43.2628, kld_loss: 1.8581\n",
      "Train Epoch: 451 [0/110 (0%)]\tRecons Loss: 59.484104; KLD Loss: 59.484104\n",
      "Train Epoch: 451 [64/110 (50%)]\tRecons Loss: 71.890900; KLD Loss: 71.890900\n",
      "====> Epoch: 451 Average recons loss: 2.2834, kld loss: 1.8800\n",
      "====> Test set recons loss: 41.7503, kld_loss: 2.1835\n",
      "Train Epoch: 452 [0/110 (0%)]\tRecons Loss: 73.065155; KLD Loss: 73.065155\n",
      "Train Epoch: 452 [64/110 (50%)]\tRecons Loss: 78.325302; KLD Loss: 78.325302\n",
      "====> Epoch: 452 Average recons loss: 2.4885, kld loss: 1.8016\n",
      "====> Test set recons loss: 49.4168, kld_loss: 1.8215\n",
      "Train Epoch: 453 [0/110 (0%)]\tRecons Loss: 71.850754; KLD Loss: 71.850754\n",
      "Train Epoch: 453 [64/110 (50%)]\tRecons Loss: 65.716415; KLD Loss: 65.716415\n",
      "====> Epoch: 453 Average recons loss: 2.4229, kld loss: 1.8841\n",
      "====> Test set recons loss: 46.0556, kld_loss: 1.8201\n",
      "Train Epoch: 454 [0/110 (0%)]\tRecons Loss: 63.357555; KLD Loss: 63.357555\n",
      "Train Epoch: 454 [64/110 (50%)]\tRecons Loss: 64.159988; KLD Loss: 64.159988\n",
      "====> Epoch: 454 Average recons loss: 2.3422, kld loss: 1.8123\n",
      "====> Epoch: 454 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 42.9138, kld_loss: 1.8889\n",
      "Train Epoch: 455 [0/110 (0%)]\tRecons Loss: 63.960560; KLD Loss: 63.960560\n",
      "Train Epoch: 455 [64/110 (50%)]\tRecons Loss: 72.177368; KLD Loss: 72.177368\n",
      "====> Epoch: 455 Average recons loss: 2.4225, kld loss: 1.8135\n",
      "====> Test set recons loss: 43.4689, kld_loss: 1.8190\n",
      "Train Epoch: 456 [0/110 (0%)]\tRecons Loss: 66.002396; KLD Loss: 66.002396\n",
      "Train Epoch: 456 [64/110 (50%)]\tRecons Loss: 61.233620; KLD Loss: 61.233620\n",
      "====> Epoch: 456 Average recons loss: 2.2734, kld loss: 1.9183\n",
      "====> Test set recons loss: 41.2559, kld_loss: 1.8343\n",
      "Train Epoch: 457 [0/110 (0%)]\tRecons Loss: 58.309097; KLD Loss: 58.309097\n",
      "Train Epoch: 457 [64/110 (50%)]\tRecons Loss: 62.552895; KLD Loss: 62.552895\n",
      "====> Epoch: 457 Average recons loss: 2.3903, kld loss: 1.8082\n",
      "====> Test set recons loss: 45.0924, kld_loss: 2.0186\n",
      "Train Epoch: 458 [0/110 (0%)]\tRecons Loss: 78.091049; KLD Loss: 78.091049\n",
      "Train Epoch: 458 [64/110 (50%)]\tRecons Loss: 64.373901; KLD Loss: 64.373901\n",
      "====> Epoch: 458 Average recons loss: 2.5202, kld loss: 1.7781\n",
      "====> Test set recons loss: 41.9308, kld_loss: 1.8358\n",
      "Train Epoch: 459 [0/110 (0%)]\tRecons Loss: 62.402641; KLD Loss: 62.402641\n",
      "Train Epoch: 459 [64/110 (50%)]\tRecons Loss: 61.650154; KLD Loss: 61.650154\n",
      "====> Epoch: 459 Average recons loss: 2.4066, kld loss: 1.9050\n",
      "====> Test set recons loss: 43.1227, kld_loss: 1.8327\n",
      "Train Epoch: 460 [0/110 (0%)]\tRecons Loss: 67.210144; KLD Loss: 67.210144\n",
      "Train Epoch: 460 [64/110 (50%)]\tRecons Loss: 61.675064; KLD Loss: 61.675064\n",
      "====> Epoch: 460 Average recons loss: 2.4317, kld loss: 1.8465\n",
      "====> Test set recons loss: 44.1021, kld_loss: 1.9364\n",
      "Train Epoch: 461 [0/110 (0%)]\tRecons Loss: 68.909622; KLD Loss: 68.909622\n",
      "Train Epoch: 461 [64/110 (50%)]\tRecons Loss: 68.206619; KLD Loss: 68.206619\n",
      "====> Epoch: 461 Average recons loss: 2.3078, kld loss: 1.8653\n",
      "====> Test set recons loss: 40.7527, kld_loss: 1.8260\n",
      "Train Epoch: 462 [0/110 (0%)]\tRecons Loss: 61.489918; KLD Loss: 61.489918\n",
      "Train Epoch: 462 [64/110 (50%)]\tRecons Loss: 71.185181; KLD Loss: 71.185181\n",
      "====> Epoch: 462 Average recons loss: 2.2919, kld loss: 1.7804\n",
      "====> Test set recons loss: 46.8522, kld_loss: 1.9382\n",
      "Train Epoch: 463 [0/110 (0%)]\tRecons Loss: 66.657440; KLD Loss: 66.657440\n",
      "Train Epoch: 463 [64/110 (50%)]\tRecons Loss: 72.945801; KLD Loss: 72.945801\n",
      "====> Epoch: 463 Average recons loss: 2.4551, kld loss: 1.7651\n",
      "====> Test set recons loss: 42.5569, kld_loss: 1.8150\n",
      "Train Epoch: 464 [0/110 (0%)]\tRecons Loss: 67.649963; KLD Loss: 67.649963\n",
      "Train Epoch: 464 [64/110 (50%)]\tRecons Loss: 61.352722; KLD Loss: 61.352722\n",
      "====> Epoch: 464 Average recons loss: 2.3304, kld loss: 1.9538\n",
      "====> Epoch: 464 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.9994, kld_loss: 1.9606\n",
      "Train Epoch: 465 [0/110 (0%)]\tRecons Loss: 62.957291; KLD Loss: 62.957291\n",
      "Train Epoch: 465 [64/110 (50%)]\tRecons Loss: 62.679897; KLD Loss: 62.679897\n",
      "====> Epoch: 465 Average recons loss: 2.2979, kld loss: 1.8740\n",
      "====> Test set recons loss: 44.4877, kld_loss: 2.3308\n",
      "Train Epoch: 466 [0/110 (0%)]\tRecons Loss: 71.817825; KLD Loss: 71.817825\n",
      "Train Epoch: 466 [64/110 (50%)]\tRecons Loss: 64.154694; KLD Loss: 64.154694\n",
      "====> Epoch: 466 Average recons loss: 2.3880, kld loss: 1.6822\n",
      "====> Test set recons loss: 45.4715, kld_loss: 1.9717\n",
      "Train Epoch: 467 [0/110 (0%)]\tRecons Loss: 67.752975; KLD Loss: 67.752975\n",
      "Train Epoch: 467 [64/110 (50%)]\tRecons Loss: 61.210846; KLD Loss: 61.210846\n",
      "====> Epoch: 467 Average recons loss: 2.2256, kld loss: 1.8459\n",
      "====> Test set recons loss: 42.1203, kld_loss: 1.8827\n",
      "Train Epoch: 468 [0/110 (0%)]\tRecons Loss: 63.665016; KLD Loss: 63.665016\n",
      "Train Epoch: 468 [64/110 (50%)]\tRecons Loss: 55.706699; KLD Loss: 55.706699\n",
      "====> Epoch: 468 Average recons loss: 2.4440, kld loss: 1.7541\n",
      "====> Test set recons loss: 42.7099, kld_loss: 1.9538\n",
      "Train Epoch: 469 [0/110 (0%)]\tRecons Loss: 62.840763; KLD Loss: 62.840763\n",
      "Train Epoch: 469 [64/110 (50%)]\tRecons Loss: 64.302078; KLD Loss: 64.302078\n",
      "====> Epoch: 469 Average recons loss: 2.3408, kld loss: 1.8478\n",
      "====> Test set recons loss: 44.1544, kld_loss: 1.7766\n",
      "Train Epoch: 470 [0/110 (0%)]\tRecons Loss: 57.520737; KLD Loss: 57.520737\n",
      "Train Epoch: 470 [64/110 (50%)]\tRecons Loss: 59.348846; KLD Loss: 59.348846\n",
      "====> Epoch: 470 Average recons loss: 2.3632, kld loss: 1.8868\n",
      "====> Test set recons loss: 43.1898, kld_loss: 1.8589\n",
      "Train Epoch: 471 [0/110 (0%)]\tRecons Loss: 60.237732; KLD Loss: 60.237732\n",
      "Train Epoch: 471 [64/110 (50%)]\tRecons Loss: 64.319733; KLD Loss: 64.319733\n",
      "====> Epoch: 471 Average recons loss: 2.1857, kld loss: 1.8560\n",
      "====> Test set recons loss: 38.5999, kld_loss: 1.9615\n",
      "Train Epoch: 472 [0/110 (0%)]\tRecons Loss: 65.515076; KLD Loss: 65.515076\n",
      "Train Epoch: 472 [64/110 (50%)]\tRecons Loss: 75.640167; KLD Loss: 75.640167\n",
      "====> Epoch: 472 Average recons loss: 2.5586, kld loss: 1.7428\n",
      "====> Test set recons loss: 39.8374, kld_loss: 1.7755\n",
      "Train Epoch: 473 [0/110 (0%)]\tRecons Loss: 76.559601; KLD Loss: 76.559601\n",
      "Train Epoch: 473 [64/110 (50%)]\tRecons Loss: 57.489059; KLD Loss: 57.489059\n",
      "====> Epoch: 473 Average recons loss: 2.4379, kld loss: 2.0023\n",
      "====> Test set recons loss: 44.9113, kld_loss: 1.7463\n",
      "Train Epoch: 474 [0/110 (0%)]\tRecons Loss: 64.347900; KLD Loss: 64.347900\n",
      "Train Epoch: 474 [64/110 (50%)]\tRecons Loss: 58.291664; KLD Loss: 58.291664\n",
      "====> Epoch: 474 Average recons loss: 2.3245, kld loss: 1.9000\n",
      "====> Epoch: 474 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 41.0587, kld_loss: 1.9739\n",
      "Train Epoch: 475 [0/110 (0%)]\tRecons Loss: 60.120136; KLD Loss: 60.120136\n",
      "Train Epoch: 475 [64/110 (50%)]\tRecons Loss: 55.917953; KLD Loss: 55.917953\n",
      "====> Epoch: 475 Average recons loss: 2.2539, kld loss: 1.7451\n",
      "====> Test set recons loss: 43.0859, kld_loss: 2.2333\n",
      "Train Epoch: 476 [0/110 (0%)]\tRecons Loss: 79.917747; KLD Loss: 79.917747\n",
      "Train Epoch: 476 [64/110 (50%)]\tRecons Loss: 75.492172; KLD Loss: 75.492172\n",
      "====> Epoch: 476 Average recons loss: 2.4985, kld loss: 1.7909\n",
      "====> Test set recons loss: 42.5092, kld_loss: 1.8253\n",
      "Train Epoch: 477 [0/110 (0%)]\tRecons Loss: 62.275635; KLD Loss: 62.275635\n",
      "Train Epoch: 477 [64/110 (50%)]\tRecons Loss: 59.489243; KLD Loss: 59.489243\n",
      "====> Epoch: 477 Average recons loss: 2.4202, kld loss: 2.0070\n",
      "====> Test set recons loss: 37.7136, kld_loss: 1.8367\n",
      "Train Epoch: 478 [0/110 (0%)]\tRecons Loss: 61.782406; KLD Loss: 61.782406\n",
      "Train Epoch: 478 [64/110 (50%)]\tRecons Loss: 62.754974; KLD Loss: 62.754974\n",
      "====> Epoch: 478 Average recons loss: 2.4253, kld loss: 1.8568\n",
      "====> Test set recons loss: 45.3757, kld_loss: 2.1338\n",
      "Train Epoch: 479 [0/110 (0%)]\tRecons Loss: 77.812531; KLD Loss: 77.812531\n",
      "Train Epoch: 479 [64/110 (50%)]\tRecons Loss: 58.206383; KLD Loss: 58.206383\n",
      "====> Epoch: 479 Average recons loss: 2.5913, kld loss: 1.9080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set recons loss: 42.9613, kld_loss: 1.9616\n",
      "Train Epoch: 480 [0/110 (0%)]\tRecons Loss: 63.312038; KLD Loss: 63.312038\n",
      "Train Epoch: 480 [64/110 (50%)]\tRecons Loss: 83.755463; KLD Loss: 83.755463\n",
      "====> Epoch: 480 Average recons loss: 2.5285, kld loss: 1.9981\n",
      "====> Test set recons loss: 43.5024, kld_loss: 1.9444\n",
      "Train Epoch: 481 [0/110 (0%)]\tRecons Loss: 54.575157; KLD Loss: 54.575157\n",
      "Train Epoch: 481 [64/110 (50%)]\tRecons Loss: 71.923286; KLD Loss: 71.923286\n",
      "====> Epoch: 481 Average recons loss: 2.2253, kld loss: 1.9091\n",
      "====> Test set recons loss: 43.4638, kld_loss: 2.2716\n",
      "Train Epoch: 482 [0/110 (0%)]\tRecons Loss: 72.037292; KLD Loss: 72.037292\n",
      "Train Epoch: 482 [64/110 (50%)]\tRecons Loss: 72.236664; KLD Loss: 72.236664\n",
      "====> Epoch: 482 Average recons loss: 2.5876, kld loss: 1.7849\n",
      "====> Test set recons loss: 45.5515, kld_loss: 1.8997\n",
      "Train Epoch: 483 [0/110 (0%)]\tRecons Loss: 63.090298; KLD Loss: 63.090298\n",
      "Train Epoch: 483 [64/110 (50%)]\tRecons Loss: 61.066380; KLD Loss: 61.066380\n",
      "====> Epoch: 483 Average recons loss: 2.1398, kld loss: 2.0073\n",
      "====> Test set recons loss: 40.8408, kld_loss: 1.9142\n",
      "Train Epoch: 484 [0/110 (0%)]\tRecons Loss: 67.539764; KLD Loss: 67.539764\n",
      "Train Epoch: 484 [64/110 (50%)]\tRecons Loss: 67.838463; KLD Loss: 67.838463\n",
      "====> Epoch: 484 Average recons loss: 2.3300, kld loss: 1.7807\n",
      "====> Epoch: 484 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 42.0266, kld_loss: 2.5592\n",
      "Train Epoch: 485 [0/110 (0%)]\tRecons Loss: 81.270096; KLD Loss: 81.270096\n",
      "Train Epoch: 485 [64/110 (50%)]\tRecons Loss: 66.228088; KLD Loss: 66.228088\n",
      "====> Epoch: 485 Average recons loss: 2.5547, kld loss: 1.7398\n",
      "====> Test set recons loss: 43.6195, kld_loss: 1.8056\n",
      "Train Epoch: 486 [0/110 (0%)]\tRecons Loss: 79.618332; KLD Loss: 79.618332\n",
      "Train Epoch: 486 [64/110 (50%)]\tRecons Loss: 53.781342; KLD Loss: 53.781342\n",
      "====> Epoch: 486 Average recons loss: 2.2008, kld loss: 2.0736\n",
      "====> Test set recons loss: 43.5839, kld_loss: 1.8793\n",
      "Train Epoch: 487 [0/110 (0%)]\tRecons Loss: 59.002113; KLD Loss: 59.002113\n",
      "Train Epoch: 487 [64/110 (50%)]\tRecons Loss: 59.908821; KLD Loss: 59.908821\n",
      "====> Epoch: 487 Average recons loss: 2.2793, kld loss: 1.8885\n",
      "====> Test set recons loss: 42.2301, kld_loss: 2.3054\n",
      "Train Epoch: 488 [0/110 (0%)]\tRecons Loss: 61.336609; KLD Loss: 61.336609\n",
      "Train Epoch: 488 [64/110 (50%)]\tRecons Loss: 53.942722; KLD Loss: 53.942722\n",
      "====> Epoch: 488 Average recons loss: 2.5532, kld loss: 1.6579\n",
      "====> Test set recons loss: 41.0209, kld_loss: 2.0037\n",
      "Train Epoch: 489 [0/110 (0%)]\tRecons Loss: 54.093864; KLD Loss: 54.093864\n",
      "Train Epoch: 489 [64/110 (50%)]\tRecons Loss: 64.629303; KLD Loss: 64.629303\n",
      "====> Epoch: 489 Average recons loss: 2.2022, kld loss: 1.8840\n",
      "====> Test set recons loss: 39.0839, kld_loss: 1.7764\n",
      "Train Epoch: 490 [0/110 (0%)]\tRecons Loss: 51.436577; KLD Loss: 51.436577\n",
      "Train Epoch: 490 [64/110 (50%)]\tRecons Loss: 72.438324; KLD Loss: 72.438324\n",
      "====> Epoch: 490 Average recons loss: 2.1757, kld loss: 1.8404\n",
      "====> Test set recons loss: 42.9692, kld_loss: 2.2235\n",
      "Train Epoch: 491 [0/110 (0%)]\tRecons Loss: 72.677170; KLD Loss: 72.677170\n",
      "Train Epoch: 491 [64/110 (50%)]\tRecons Loss: 74.842842; KLD Loss: 74.842842\n",
      "====> Epoch: 491 Average recons loss: 2.7219, kld loss: 1.7371\n",
      "====> Test set recons loss: 41.2541, kld_loss: 1.8070\n",
      "Train Epoch: 492 [0/110 (0%)]\tRecons Loss: 54.724258; KLD Loss: 54.724258\n",
      "Train Epoch: 492 [64/110 (50%)]\tRecons Loss: 70.988258; KLD Loss: 70.988258\n",
      "====> Epoch: 492 Average recons loss: 2.2061, kld loss: 1.9446\n",
      "====> Test set recons loss: 48.5616, kld_loss: 1.8820\n",
      "Train Epoch: 493 [0/110 (0%)]\tRecons Loss: 75.567200; KLD Loss: 75.567200\n",
      "Train Epoch: 493 [64/110 (50%)]\tRecons Loss: 64.899117; KLD Loss: 64.899117\n",
      "====> Epoch: 493 Average recons loss: 2.5139, kld loss: 1.8788\n",
      "====> Test set recons loss: 42.6578, kld_loss: 1.9692\n",
      "Train Epoch: 494 [0/110 (0%)]\tRecons Loss: 68.473740; KLD Loss: 68.473740\n",
      "Train Epoch: 494 [64/110 (50%)]\tRecons Loss: 57.831139; KLD Loss: 57.831139\n",
      "====> Epoch: 494 Average recons loss: 2.4103, kld loss: 1.8659\n",
      "====> Epoch: 494 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 42.3689, kld_loss: 1.9497\n",
      "Train Epoch: 495 [0/110 (0%)]\tRecons Loss: 61.634293; KLD Loss: 61.634293\n",
      "Train Epoch: 495 [64/110 (50%)]\tRecons Loss: 62.800720; KLD Loss: 62.800720\n",
      "====> Epoch: 495 Average recons loss: 2.1259, kld loss: 1.8858\n",
      "====> Test set recons loss: 45.6659, kld_loss: 2.0699\n",
      "Train Epoch: 496 [0/110 (0%)]\tRecons Loss: 62.818920; KLD Loss: 62.818920\n",
      "Train Epoch: 496 [64/110 (50%)]\tRecons Loss: 68.006088; KLD Loss: 68.006088\n",
      "====> Epoch: 496 Average recons loss: 2.3001, kld loss: 1.8078\n",
      "====> Test set recons loss: 41.7792, kld_loss: 2.1228\n",
      "Train Epoch: 497 [0/110 (0%)]\tRecons Loss: 49.842606; KLD Loss: 49.842606\n",
      "Train Epoch: 497 [64/110 (50%)]\tRecons Loss: 58.998512; KLD Loss: 58.998512\n",
      "====> Epoch: 497 Average recons loss: 2.0881, kld loss: 1.7937\n",
      "====> Test set recons loss: 47.0307, kld_loss: 2.3235\n",
      "Train Epoch: 498 [0/110 (0%)]\tRecons Loss: 70.522842; KLD Loss: 70.522842\n",
      "Train Epoch: 498 [64/110 (50%)]\tRecons Loss: 57.730320; KLD Loss: 57.730320\n",
      "====> Epoch: 498 Average recons loss: 2.1960, kld loss: 1.7857\n",
      "====> Test set recons loss: 40.5434, kld_loss: 2.0468\n",
      "Train Epoch: 499 [0/110 (0%)]\tRecons Loss: 56.130211; KLD Loss: 56.130211\n",
      "Train Epoch: 499 [64/110 (50%)]\tRecons Loss: 70.960052; KLD Loss: 70.960052\n",
      "====> Epoch: 499 Average recons loss: 2.2593, kld loss: 1.7849\n",
      "====> Test set recons loss: 42.2653, kld_loss: 2.0203\n",
      "Train Epoch: 500 [0/110 (0%)]\tRecons Loss: 58.709629; KLD Loss: 58.709629\n",
      "Train Epoch: 500 [64/110 (50%)]\tRecons Loss: 61.967258; KLD Loss: 61.967258\n",
      "====> Epoch: 500 Average recons loss: 2.2081, kld loss: 1.7658\n",
      "====> Test set recons loss: 41.8095, kld_loss: 2.0293\n",
      "Train Epoch: 501 [0/110 (0%)]\tRecons Loss: 56.829063; KLD Loss: 56.829063\n",
      "Train Epoch: 501 [64/110 (50%)]\tRecons Loss: 63.370289; KLD Loss: 63.370289\n",
      "====> Epoch: 501 Average recons loss: 2.2371, kld loss: 1.7883\n",
      "====> Test set recons loss: 42.5309, kld_loss: 1.9490\n",
      "Train Epoch: 502 [0/110 (0%)]\tRecons Loss: 68.097626; KLD Loss: 68.097626\n",
      "Train Epoch: 502 [64/110 (50%)]\tRecons Loss: 51.366535; KLD Loss: 51.366535\n",
      "====> Epoch: 502 Average recons loss: 2.2456, kld loss: 1.7943\n",
      "====> Test set recons loss: 45.7173, kld_loss: 2.2407\n",
      "Train Epoch: 503 [0/110 (0%)]\tRecons Loss: 60.670074; KLD Loss: 60.670074\n",
      "Train Epoch: 503 [64/110 (50%)]\tRecons Loss: 62.217068; KLD Loss: 62.217068\n",
      "====> Epoch: 503 Average recons loss: 2.0724, kld loss: 1.7132\n",
      "====> Test set recons loss: 45.4550, kld_loss: 2.4965\n",
      "Train Epoch: 504 [0/110 (0%)]\tRecons Loss: 58.999699; KLD Loss: 58.999699\n",
      "Train Epoch: 504 [64/110 (50%)]\tRecons Loss: 64.815613; KLD Loss: 64.815613\n",
      "====> Epoch: 504 Average recons loss: 2.3133, kld loss: 1.6453\n",
      "====> Epoch: 504 Eta is now 1.0000 due to sufficient recons results\n",
      "====> Test set recons loss: 44.7853, kld_loss: 1.9001\n",
      "Train Epoch: 505 [0/110 (0%)]\tRecons Loss: 72.666504; KLD Loss: 72.666504\n",
      "Train Epoch: 505 [64/110 (50%)]\tRecons Loss: 56.195366; KLD Loss: 56.195366\n",
      "====> Epoch: 505 Average recons loss: 2.2084, kld loss: 1.9570\n",
      "====> Test set recons loss: 39.9683, kld_loss: 1.7884\n",
      "Train Epoch: 506 [0/110 (0%)]\tRecons Loss: 57.715683; KLD Loss: 57.715683\n",
      "Train Epoch: 506 [64/110 (50%)]\tRecons Loss: 69.645714; KLD Loss: 69.645714\n",
      "====> Epoch: 506 Average recons loss: 2.1197, kld loss: 1.7847\n",
      "====> Test set recons loss: 44.5133, kld_loss: 2.4675\n",
      "Train Epoch: 507 [0/110 (0%)]\tRecons Loss: 75.993690; KLD Loss: 75.993690\n",
      "Train Epoch: 507 [64/110 (50%)]\tRecons Loss: 75.295883; KLD Loss: 75.295883\n",
      "====> Epoch: 507 Average recons loss: 2.5180, kld loss: 1.6037\n",
      "====> Test set recons loss: 42.5744, kld_loss: 1.9525\n",
      "Train Epoch: 508 [0/110 (0%)]\tRecons Loss: 62.039417; KLD Loss: 62.039417\n",
      "Train Epoch: 508 [64/110 (50%)]\tRecons Loss: 54.922626; KLD Loss: 54.922626\n",
      "====> Epoch: 508 Average recons loss: 1.9986, kld loss: 1.8311\n",
      "====> Test set recons loss: 42.4381, kld_loss: 1.8783\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-459b77bd74c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlog_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlog_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_eta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fc417121ca39>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, log_eta)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mrecons_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrecons_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_eta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkld_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_recons_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrecons_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtrain_kld_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mkld_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_eta = (-3., 0)\n",
    "for epoch in range(1, 1000 + 1):\n",
    "    log_eta = train(epoch, log_eta)\n",
    "    if epoch % 20 == 0:\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1048576/128/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0\n",
    "for batch_idx,( data, _) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    data = data.reshape([-1,3, 128*128])\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    loss = loss_function(recon_batch, data, mu, logvar)\n",
    "    loss.backward()\n",
    "    train_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
