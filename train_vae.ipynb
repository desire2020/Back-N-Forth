{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths = list(paths.list_images('data/Caltech101/001'))\n",
    "# image_paths = list(paths.list_images('data/cars_side-view'))\n",
    "image_paths = list(paths.list_images('data/Caltech101/016'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:00<00:00, 2736.52it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for img_path in tqdm(image_paths):\n",
    "    label = img_path.split(os.path.sep)[-2]\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    data.append(img)\n",
    "    labels.append(label)\n",
    "    if len(labels) > 5000:\n",
    "        break\n",
    "    \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Classes: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lb = LabelEncoder()\n",
    "labels = lb.fit_transform(labels)\n",
    "print(f\"Total Number of Classes: {len(lb.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 123})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train examples: (110, 197, 300, 3)\n",
      "x_test examples: (13, 197, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# divide the data into train and test set\n",
    "(x_train, x_test, y_train, y_test) = train_test_split(data, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "print(f\"x_train examples: {x_train.shape}\\nx_test examples: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {'size': 64, 'channels': 3, 'classes': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((dataset_config['size'], dataset_config['size'])),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((dataset_config['size'],dataset_config['size'])),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean = [0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "BS = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=BS, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=BS, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "# custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels= None, transforms = None):\n",
    "        self.labels = labels\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.images[index][:]\n",
    "        \n",
    "        if self.transforms:\n",
    "            data = self.transforms(data)\n",
    "            \n",
    "        \n",
    "        return (data, self.labels[index])\n",
    "        \n",
    "train_data = CustomDataset(x_train, y_train, train_transforms)\n",
    "test_data = CustomDataset(x_test, y_test, val_transform)       \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BS, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=BS, shuffle=True, num_workers=4, drop_last=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Main --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def gradients(y, x):\n",
    "    return autograd.grad(\n",
    "                outputs=y, inputs=x, retain_graph=True,\n",
    "                create_graph=True, grad_outputs=torch.ones_like(y), only_inputs=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HWReduction(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x -> [B, C, H, W]\n",
    "        return x.mean(dim=(-1, -2))\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, shape: list):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.reshape([batch_size] + self.shape)\n",
    "\n",
    "class VAE_Cifar10(nn.Module):\n",
    "    def __init__(self, label = 'cifar10', image_size = dataset_config['size'],\n",
    "                 channel_num = dataset_config['channels'],\n",
    "                 z_size=128):\n",
    "        # configurations\n",
    "        super().__init__()\n",
    "        self.label = label\n",
    "        self.image_size = image_size\n",
    "        self.channel_num = channel_num\n",
    "        self.z_size = z_size\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.capacity_conv(channel_num, 16), # 16 x 64 x 64\n",
    "            nn.InstanceNorm2d(16),\n",
    "            self.downsampling_conv(16, 32), # 32 x 32 x 32\n",
    "            self.capacity_conv(32, 64), # 64 x 32 x 32\n",
    "            self.downsampling_conv(64, 128), # 128 x 16 x 16\n",
    "            self.capacity_conv(128, 256), # 256 x 16 x 16\n",
    "            self.downsampling_conv(256, 512), # 512 x 8 x 8\n",
    "            HWReduction(),\n",
    "        )\n",
    "\n",
    "        # H, W will be reduced\n",
    "\n",
    "\n",
    "        # q\n",
    "        self.q_mean = self._linear(512, z_size, relu=False)\n",
    "        self.q_logvar = self._linear(512, z_size, relu=False)\n",
    "\n",
    "        # projection\n",
    "        self.project = nn.Sequential(\n",
    "            self._linear(z_size, 1024),\n",
    "            self._linear(1024, 8 * 8 * 128),\n",
    "            Reshape([128, 8, 8])\n",
    "        )\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            self.upsampling_conv(128, 64), # 32 x 16 x 16\n",
    "            self.capacity_conv(64, 64), # 64 x 16 x 16\n",
    "            self.upsampling_conv(64, 32), # 32 x 32 x 32\n",
    "            self.capacity_conv(32, 32), # 32 x 32 x 32\n",
    "            self.upsampling_conv(32, 16), # 16 x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                16, channel_num,\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # sample latent code z from q given x.\n",
    "        mean, logvar = self.q_mean(encoded), self.q_logvar(encoded)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        z_projected = self.project(mean)\n",
    "\n",
    "        # reconstruct x from z\n",
    "        x_reconstructed = self.decoder(z_projected)\n",
    "        return x_reconstructed, mean, logvar\n",
    "    \n",
    "    # ======\n",
    "    # Layers\n",
    "    # ======\n",
    "\n",
    "    def downsampling_conv(self, channel_size, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channel_size, kernel_num,\n",
    "                kernel_size=4, stride=2, padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(kernel_num),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "    def capacity_conv(self, channel_num, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channel_num, kernel_num,\n",
    "                kernel_size=3, stride=1, padding=1,\n",
    "            ),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "\n",
    "    def upsampling_conv(self, channel_num, kernel_num):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                channel_num, kernel_num,\n",
    "                kernel_size=4, stride=2, padding=1,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(kernel_num),\n",
    "            nn.LeakyReLU(negative_slope=0.1),\n",
    "        )\n",
    "\n",
    "    def _linear(self, in_size, out_size, relu=True):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.ReLU(),\n",
    "        ) if relu else nn.Linear(in_size, out_size)\n",
    "\n",
    "model = VAE_Cifar10().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = (recon_x - x.view(-1,  dataset_config['channels'],dataset_config['size'],dataset_config['size'])) ** 2\n",
    "    MSE = MSE.sum(dim=(-1,-2,-3))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=-1)\n",
    "\n",
    "    return MSE + 0.1 * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar).mean(dim=0)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).mean(dim=0).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch[:n].view(n,  dataset_config['channels'], dataset_config['size'], dataset_config['size'])[:n]])\n",
    "                if not os.path.exists(\"results/\"):\n",
    "                    os.mkdir(\"results\")\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "        for i, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).mean(dim=0).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch[:n].view(n,  dataset_config['channels'], dataset_config['size'], dataset_config['size'])[:n]])\n",
    "                if not os.path.exists(\"overfit_results/\"):\n",
    "                    os.mkdir(\"overfit_results\")\n",
    "                save_image(comparison.cpu(),\n",
    "                         'overfit_results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/110 (0%)]\tLoss: 28.170774\n",
      "Train Epoch: 1 [64/110 (50%)]\tLoss: 23.823635\n",
      "====> Epoch: 1 Average loss: 29.9374\n",
      "====> Test set loss: 274.3618\n",
      "Train Epoch: 2 [0/110 (0%)]\tLoss: 22.298748\n",
      "Train Epoch: 2 [64/110 (50%)]\tLoss: 20.933138\n",
      "====> Epoch: 2 Average loss: 24.9253\n",
      "====> Test set loss: 240.4846\n",
      "Train Epoch: 3 [0/110 (0%)]\tLoss: 21.730040\n",
      "Train Epoch: 3 [64/110 (50%)]\tLoss: 18.935472\n",
      "====> Epoch: 3 Average loss: 21.9280\n",
      "====> Test set loss: 228.0100\n",
      "Train Epoch: 4 [0/110 (0%)]\tLoss: 19.427828\n",
      "Train Epoch: 4 [64/110 (50%)]\tLoss: 17.748674\n",
      "====> Epoch: 4 Average loss: 20.7099\n",
      "====> Test set loss: 213.4932\n",
      "Train Epoch: 5 [0/110 (0%)]\tLoss: 16.576982\n",
      "Train Epoch: 5 [64/110 (50%)]\tLoss: 18.681341\n",
      "====> Epoch: 5 Average loss: 20.2377\n",
      "====> Test set loss: 207.8688\n",
      "Train Epoch: 6 [0/110 (0%)]\tLoss: 18.966953\n",
      "Train Epoch: 6 [64/110 (50%)]\tLoss: 16.130627\n",
      "====> Epoch: 6 Average loss: 19.4825\n",
      "====> Test set loss: 201.8396\n",
      "Train Epoch: 7 [0/110 (0%)]\tLoss: 16.367809\n",
      "Train Epoch: 7 [64/110 (50%)]\tLoss: 17.890377\n",
      "====> Epoch: 7 Average loss: 19.2276\n",
      "====> Test set loss: 205.5978\n",
      "Train Epoch: 8 [0/110 (0%)]\tLoss: 15.549746\n",
      "Train Epoch: 8 [64/110 (50%)]\tLoss: 16.453634\n",
      "====> Epoch: 8 Average loss: 19.7443\n",
      "====> Test set loss: 200.3232\n",
      "Train Epoch: 9 [0/110 (0%)]\tLoss: 18.516369\n",
      "Train Epoch: 9 [64/110 (50%)]\tLoss: 15.251620\n",
      "====> Epoch: 9 Average loss: 18.8182\n",
      "====> Test set loss: 201.6387\n",
      "Train Epoch: 10 [0/110 (0%)]\tLoss: 16.575459\n",
      "Train Epoch: 10 [64/110 (50%)]\tLoss: 16.580372\n",
      "====> Epoch: 10 Average loss: 19.0804\n",
      "====> Test set loss: 197.6224\n",
      "Train Epoch: 11 [0/110 (0%)]\tLoss: 15.912621\n",
      "Train Epoch: 11 [64/110 (50%)]\tLoss: 15.801091\n",
      "====> Epoch: 11 Average loss: 18.5795\n",
      "====> Test set loss: 196.4426\n",
      "Train Epoch: 12 [0/110 (0%)]\tLoss: 16.477884\n",
      "Train Epoch: 12 [64/110 (50%)]\tLoss: 16.591080\n",
      "====> Epoch: 12 Average loss: 18.4429\n",
      "====> Test set loss: 193.9426\n",
      "Train Epoch: 13 [0/110 (0%)]\tLoss: 16.935444\n",
      "Train Epoch: 13 [64/110 (50%)]\tLoss: 13.387197\n",
      "====> Epoch: 13 Average loss: 18.8231\n",
      "====> Test set loss: 189.4636\n",
      "Train Epoch: 14 [0/110 (0%)]\tLoss: 15.868517\n",
      "Train Epoch: 14 [64/110 (50%)]\tLoss: 14.991870\n",
      "====> Epoch: 14 Average loss: 18.3424\n",
      "====> Test set loss: 184.4448\n",
      "Train Epoch: 15 [0/110 (0%)]\tLoss: 14.890503\n",
      "Train Epoch: 15 [64/110 (50%)]\tLoss: 14.066494\n",
      "====> Epoch: 15 Average loss: 17.4732\n",
      "====> Test set loss: 179.8025\n",
      "Train Epoch: 16 [0/110 (0%)]\tLoss: 16.064156\n",
      "Train Epoch: 16 [64/110 (50%)]\tLoss: 15.252284\n",
      "====> Epoch: 16 Average loss: 16.7900\n",
      "====> Test set loss: 172.5494\n",
      "Train Epoch: 17 [0/110 (0%)]\tLoss: 14.551931\n",
      "Train Epoch: 17 [64/110 (50%)]\tLoss: 13.702019\n",
      "====> Epoch: 17 Average loss: 16.1604\n",
      "====> Test set loss: 170.6797\n",
      "Train Epoch: 18 [0/110 (0%)]\tLoss: 13.586824\n",
      "Train Epoch: 18 [64/110 (50%)]\tLoss: 13.658916\n",
      "====> Epoch: 18 Average loss: 15.7643\n",
      "====> Test set loss: 172.3643\n",
      "Train Epoch: 19 [0/110 (0%)]\tLoss: 13.364682\n",
      "Train Epoch: 19 [64/110 (50%)]\tLoss: 14.145823\n",
      "====> Epoch: 19 Average loss: 15.9474\n",
      "====> Test set loss: 163.1202\n",
      "Train Epoch: 20 [0/110 (0%)]\tLoss: 11.713876\n",
      "Train Epoch: 20 [64/110 (50%)]\tLoss: 13.668393\n",
      "====> Epoch: 20 Average loss: 15.3840\n",
      "====> Test set loss: 159.1876\n",
      "Train Epoch: 21 [0/110 (0%)]\tLoss: 12.180700\n",
      "Train Epoch: 21 [64/110 (50%)]\tLoss: 12.290710\n",
      "====> Epoch: 21 Average loss: 15.0004\n",
      "====> Test set loss: 152.7892\n",
      "Train Epoch: 22 [0/110 (0%)]\tLoss: 13.729633\n",
      "Train Epoch: 22 [64/110 (50%)]\tLoss: 12.704612\n",
      "====> Epoch: 22 Average loss: 14.4552\n",
      "====> Test set loss: 151.5002\n",
      "Train Epoch: 23 [0/110 (0%)]\tLoss: 12.692479\n",
      "Train Epoch: 23 [64/110 (50%)]\tLoss: 12.355905\n",
      "====> Epoch: 23 Average loss: 13.8417\n",
      "====> Test set loss: 147.0322\n",
      "Train Epoch: 24 [0/110 (0%)]\tLoss: 11.905449\n",
      "Train Epoch: 24 [64/110 (50%)]\tLoss: 13.023685\n",
      "====> Epoch: 24 Average loss: 13.8062\n",
      "====> Test set loss: 142.1009\n",
      "Train Epoch: 25 [0/110 (0%)]\tLoss: 12.203160\n",
      "Train Epoch: 25 [64/110 (50%)]\tLoss: 10.648956\n",
      "====> Epoch: 25 Average loss: 13.4990\n",
      "====> Test set loss: 141.7629\n",
      "Train Epoch: 26 [0/110 (0%)]\tLoss: 10.038969\n",
      "Train Epoch: 26 [64/110 (50%)]\tLoss: 11.880447\n",
      "====> Epoch: 26 Average loss: 13.3731\n",
      "====> Test set loss: 143.9899\n",
      "Train Epoch: 27 [0/110 (0%)]\tLoss: 12.130045\n",
      "Train Epoch: 27 [64/110 (50%)]\tLoss: 11.803588\n",
      "====> Epoch: 27 Average loss: 12.8556\n",
      "====> Test set loss: 136.8505\n",
      "Train Epoch: 28 [0/110 (0%)]\tLoss: 10.672211\n",
      "Train Epoch: 28 [64/110 (50%)]\tLoss: 11.481870\n",
      "====> Epoch: 28 Average loss: 12.9795\n",
      "====> Test set loss: 133.6060\n",
      "Train Epoch: 29 [0/110 (0%)]\tLoss: 9.787447\n",
      "Train Epoch: 29 [64/110 (50%)]\tLoss: 11.588299\n",
      "====> Epoch: 29 Average loss: 12.3415\n",
      "====> Test set loss: 128.8265\n",
      "Train Epoch: 30 [0/110 (0%)]\tLoss: 10.033543\n",
      "Train Epoch: 30 [64/110 (50%)]\tLoss: 10.310647\n",
      "====> Epoch: 30 Average loss: 11.8942\n",
      "====> Test set loss: 125.8123\n",
      "Train Epoch: 31 [0/110 (0%)]\tLoss: 9.081827\n",
      "Train Epoch: 31 [64/110 (50%)]\tLoss: 10.459425\n",
      "====> Epoch: 31 Average loss: 11.6322\n",
      "====> Test set loss: 127.4621\n",
      "Train Epoch: 32 [0/110 (0%)]\tLoss: 9.787104\n",
      "Train Epoch: 32 [64/110 (50%)]\tLoss: 9.510065\n",
      "====> Epoch: 32 Average loss: 11.4948\n",
      "====> Test set loss: 129.2159\n",
      "Train Epoch: 33 [0/110 (0%)]\tLoss: 10.396053\n",
      "Train Epoch: 33 [64/110 (50%)]\tLoss: 9.215920\n",
      "====> Epoch: 33 Average loss: 11.5405\n",
      "====> Test set loss: 118.2008\n",
      "Train Epoch: 34 [0/110 (0%)]\tLoss: 9.448515\n",
      "Train Epoch: 34 [64/110 (50%)]\tLoss: 10.401983\n",
      "====> Epoch: 34 Average loss: 10.8568\n",
      "====> Test set loss: 121.4529\n",
      "Train Epoch: 35 [0/110 (0%)]\tLoss: 9.024735\n",
      "Train Epoch: 35 [64/110 (50%)]\tLoss: 9.791908\n",
      "====> Epoch: 35 Average loss: 10.8557\n",
      "====> Test set loss: 118.6935\n",
      "Train Epoch: 36 [0/110 (0%)]\tLoss: 8.681849\n",
      "Train Epoch: 36 [64/110 (50%)]\tLoss: 9.144371\n",
      "====> Epoch: 36 Average loss: 10.4436\n",
      "====> Test set loss: 113.4073\n",
      "Train Epoch: 37 [0/110 (0%)]\tLoss: 9.193912\n",
      "Train Epoch: 37 [64/110 (50%)]\tLoss: 8.061401\n",
      "====> Epoch: 37 Average loss: 9.8110\n",
      "====> Test set loss: 113.9701\n",
      "Train Epoch: 38 [0/110 (0%)]\tLoss: 7.903269\n",
      "Train Epoch: 38 [64/110 (50%)]\tLoss: 7.967200\n",
      "====> Epoch: 38 Average loss: 9.7755\n",
      "====> Test set loss: 105.1631\n",
      "Train Epoch: 39 [0/110 (0%)]\tLoss: 7.759096\n",
      "Train Epoch: 39 [64/110 (50%)]\tLoss: 8.052636\n",
      "====> Epoch: 39 Average loss: 9.3440\n",
      "====> Test set loss: 106.6944\n",
      "Train Epoch: 40 [0/110 (0%)]\tLoss: 8.409512\n",
      "Train Epoch: 40 [64/110 (50%)]\tLoss: 7.118746\n",
      "====> Epoch: 40 Average loss: 9.1157\n",
      "====> Test set loss: 101.4450\n",
      "Train Epoch: 41 [0/110 (0%)]\tLoss: 7.978909\n",
      "Train Epoch: 41 [64/110 (50%)]\tLoss: 7.244000\n",
      "====> Epoch: 41 Average loss: 8.8940\n",
      "====> Test set loss: 100.8050\n",
      "Train Epoch: 42 [0/110 (0%)]\tLoss: 7.343012\n",
      "Train Epoch: 42 [64/110 (50%)]\tLoss: 7.205025\n",
      "====> Epoch: 42 Average loss: 8.4752\n",
      "====> Test set loss: 99.5088\n",
      "Train Epoch: 43 [0/110 (0%)]\tLoss: 7.637764\n",
      "Train Epoch: 43 [64/110 (50%)]\tLoss: 6.148766\n",
      "====> Epoch: 43 Average loss: 8.4219\n",
      "====> Test set loss: 94.0761\n",
      "Train Epoch: 44 [0/110 (0%)]\tLoss: 6.465680\n",
      "Train Epoch: 44 [64/110 (50%)]\tLoss: 7.411092\n",
      "====> Epoch: 44 Average loss: 8.1250\n",
      "====> Test set loss: 93.2691\n",
      "Train Epoch: 45 [0/110 (0%)]\tLoss: 5.915061\n",
      "Train Epoch: 45 [64/110 (50%)]\tLoss: 6.683605\n",
      "====> Epoch: 45 Average loss: 7.8930\n",
      "====> Test set loss: 93.4433\n",
      "Train Epoch: 46 [0/110 (0%)]\tLoss: 6.125780\n",
      "Train Epoch: 46 [64/110 (50%)]\tLoss: 6.768450\n",
      "====> Epoch: 46 Average loss: 7.3496\n",
      "====> Test set loss: 90.4485\n",
      "Train Epoch: 47 [0/110 (0%)]\tLoss: 5.600224\n",
      "Train Epoch: 47 [64/110 (50%)]\tLoss: 6.217116\n",
      "====> Epoch: 47 Average loss: 7.2012\n",
      "====> Test set loss: 88.8207\n",
      "Train Epoch: 48 [0/110 (0%)]\tLoss: 6.141818\n",
      "Train Epoch: 48 [64/110 (50%)]\tLoss: 6.297890\n",
      "====> Epoch: 48 Average loss: 6.8577\n",
      "====> Test set loss: 87.0082\n",
      "Train Epoch: 49 [0/110 (0%)]\tLoss: 5.483132\n",
      "Train Epoch: 49 [64/110 (50%)]\tLoss: 5.922506\n",
      "====> Epoch: 49 Average loss: 6.8702\n",
      "====> Test set loss: 85.0092\n",
      "Train Epoch: 50 [0/110 (0%)]\tLoss: 5.187329\n",
      "Train Epoch: 50 [64/110 (50%)]\tLoss: 6.051426\n",
      "====> Epoch: 50 Average loss: 6.6355\n",
      "====> Test set loss: 81.4127\n",
      "Train Epoch: 51 [0/110 (0%)]\tLoss: 5.004968\n",
      "Train Epoch: 51 [64/110 (50%)]\tLoss: 5.662358\n",
      "====> Epoch: 51 Average loss: 6.3023\n",
      "====> Test set loss: 81.2694\n",
      "Train Epoch: 52 [0/110 (0%)]\tLoss: 5.669345\n",
      "Train Epoch: 52 [64/110 (50%)]\tLoss: 5.274411\n",
      "====> Epoch: 52 Average loss: 5.9951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 81.8814\n",
      "Train Epoch: 53 [0/110 (0%)]\tLoss: 4.840095\n",
      "Train Epoch: 53 [64/110 (50%)]\tLoss: 4.944529\n",
      "====> Epoch: 53 Average loss: 5.8887\n",
      "====> Test set loss: 79.5196\n",
      "Train Epoch: 54 [0/110 (0%)]\tLoss: 5.168081\n",
      "Train Epoch: 54 [64/110 (50%)]\tLoss: 4.833917\n",
      "====> Epoch: 54 Average loss: 5.7154\n",
      "====> Test set loss: 76.2224\n",
      "Train Epoch: 55 [0/110 (0%)]\tLoss: 4.639230\n",
      "Train Epoch: 55 [64/110 (50%)]\tLoss: 4.780058\n",
      "====> Epoch: 55 Average loss: 5.4137\n",
      "====> Test set loss: 77.3674\n",
      "Train Epoch: 56 [0/110 (0%)]\tLoss: 4.545313\n",
      "Train Epoch: 56 [64/110 (50%)]\tLoss: 4.076295\n",
      "====> Epoch: 56 Average loss: 5.3630\n",
      "====> Test set loss: 73.6676\n",
      "Train Epoch: 57 [0/110 (0%)]\tLoss: 4.569382\n",
      "Train Epoch: 57 [64/110 (50%)]\tLoss: 4.130052\n",
      "====> Epoch: 57 Average loss: 5.0294\n",
      "====> Test set loss: 71.8292\n",
      "Train Epoch: 58 [0/110 (0%)]\tLoss: 4.028140\n",
      "Train Epoch: 58 [64/110 (50%)]\tLoss: 3.971303\n",
      "====> Epoch: 58 Average loss: 4.8439\n",
      "====> Test set loss: 70.2018\n",
      "Train Epoch: 59 [0/110 (0%)]\tLoss: 3.685906\n",
      "Train Epoch: 59 [64/110 (50%)]\tLoss: 4.474376\n",
      "====> Epoch: 59 Average loss: 4.6174\n",
      "====> Test set loss: 69.0239\n",
      "Train Epoch: 60 [0/110 (0%)]\tLoss: 4.024330\n",
      "Train Epoch: 60 [64/110 (50%)]\tLoss: 3.528771\n",
      "====> Epoch: 60 Average loss: 4.5751\n",
      "====> Test set loss: 70.5519\n",
      "Train Epoch: 61 [0/110 (0%)]\tLoss: 3.878919\n",
      "Train Epoch: 61 [64/110 (50%)]\tLoss: 3.679379\n",
      "====> Epoch: 61 Average loss: 4.2451\n",
      "====> Test set loss: 67.4040\n",
      "Train Epoch: 62 [0/110 (0%)]\tLoss: 3.904415\n",
      "Train Epoch: 62 [64/110 (50%)]\tLoss: 3.697577\n",
      "====> Epoch: 62 Average loss: 4.1680\n",
      "====> Test set loss: 64.7567\n",
      "Train Epoch: 63 [0/110 (0%)]\tLoss: 3.619541\n",
      "Train Epoch: 63 [64/110 (50%)]\tLoss: 3.375611\n",
      "====> Epoch: 63 Average loss: 4.0435\n",
      "====> Test set loss: 64.1390\n",
      "Train Epoch: 64 [0/110 (0%)]\tLoss: 3.733209\n",
      "Train Epoch: 64 [64/110 (50%)]\tLoss: 3.342438\n",
      "====> Epoch: 64 Average loss: 3.9865\n",
      "====> Test set loss: 64.1597\n",
      "Train Epoch: 65 [0/110 (0%)]\tLoss: 3.217010\n",
      "Train Epoch: 65 [64/110 (50%)]\tLoss: 3.276700\n",
      "====> Epoch: 65 Average loss: 3.9010\n",
      "====> Test set loss: 66.4395\n",
      "Train Epoch: 66 [0/110 (0%)]\tLoss: 3.390336\n",
      "Train Epoch: 66 [64/110 (50%)]\tLoss: 3.177205\n",
      "====> Epoch: 66 Average loss: 3.8795\n",
      "====> Test set loss: 64.6958\n",
      "Train Epoch: 67 [0/110 (0%)]\tLoss: 3.041121\n",
      "Train Epoch: 67 [64/110 (50%)]\tLoss: 3.197154\n",
      "====> Epoch: 67 Average loss: 3.8214\n",
      "====> Test set loss: 61.7056\n",
      "Train Epoch: 68 [0/110 (0%)]\tLoss: 3.220868\n",
      "Train Epoch: 68 [64/110 (50%)]\tLoss: 3.004855\n",
      "====> Epoch: 68 Average loss: 3.5273\n",
      "====> Test set loss: 65.4751\n",
      "Train Epoch: 69 [0/110 (0%)]\tLoss: 2.857154\n",
      "Train Epoch: 69 [64/110 (50%)]\tLoss: 3.311368\n",
      "====> Epoch: 69 Average loss: 3.6003\n",
      "====> Test set loss: 60.8695\n",
      "Train Epoch: 70 [0/110 (0%)]\tLoss: 2.745756\n",
      "Train Epoch: 70 [64/110 (50%)]\tLoss: 3.056365\n",
      "====> Epoch: 70 Average loss: 3.4147\n",
      "====> Test set loss: 59.0553\n",
      "Train Epoch: 71 [0/110 (0%)]\tLoss: 2.879372\n",
      "Train Epoch: 71 [64/110 (50%)]\tLoss: 2.651122\n",
      "====> Epoch: 71 Average loss: 3.4170\n",
      "====> Test set loss: 59.0355\n",
      "Train Epoch: 72 [0/110 (0%)]\tLoss: 2.917862\n",
      "Train Epoch: 72 [64/110 (50%)]\tLoss: 2.804945\n",
      "====> Epoch: 72 Average loss: 3.2270\n",
      "====> Test set loss: 58.3569\n",
      "Train Epoch: 73 [0/110 (0%)]\tLoss: 2.574998\n",
      "Train Epoch: 73 [64/110 (50%)]\tLoss: 2.620425\n",
      "====> Epoch: 73 Average loss: 3.1067\n",
      "====> Test set loss: 57.6394\n",
      "Train Epoch: 74 [0/110 (0%)]\tLoss: 2.844068\n",
      "Train Epoch: 74 [64/110 (50%)]\tLoss: 2.473476\n",
      "====> Epoch: 74 Average loss: 3.0497\n",
      "====> Test set loss: 56.5804\n",
      "Train Epoch: 75 [0/110 (0%)]\tLoss: 2.623827\n",
      "Train Epoch: 75 [64/110 (50%)]\tLoss: 2.480153\n",
      "====> Epoch: 75 Average loss: 2.8949\n",
      "====> Test set loss: 55.7007\n",
      "Train Epoch: 76 [0/110 (0%)]\tLoss: 2.575836\n",
      "Train Epoch: 76 [64/110 (50%)]\tLoss: 2.601246\n",
      "====> Epoch: 76 Average loss: 2.7336\n",
      "====> Test set loss: 54.8811\n",
      "Train Epoch: 77 [0/110 (0%)]\tLoss: 2.405637\n",
      "Train Epoch: 77 [64/110 (50%)]\tLoss: 2.272207\n",
      "====> Epoch: 77 Average loss: 2.6988\n",
      "====> Test set loss: 53.6199\n",
      "Train Epoch: 78 [0/110 (0%)]\tLoss: 2.091209\n",
      "Train Epoch: 78 [64/110 (50%)]\tLoss: 2.474151\n",
      "====> Epoch: 78 Average loss: 2.6220\n",
      "====> Test set loss: 53.1661\n",
      "Train Epoch: 79 [0/110 (0%)]\tLoss: 2.028752\n",
      "Train Epoch: 79 [64/110 (50%)]\tLoss: 2.176764\n",
      "====> Epoch: 79 Average loss: 2.5546\n",
      "====> Test set loss: 53.2194\n",
      "Train Epoch: 80 [0/110 (0%)]\tLoss: 2.005455\n",
      "Train Epoch: 80 [64/110 (50%)]\tLoss: 2.193604\n",
      "====> Epoch: 80 Average loss: 2.4357\n",
      "====> Test set loss: 52.6944\n",
      "Train Epoch: 81 [0/110 (0%)]\tLoss: 2.115444\n",
      "Train Epoch: 81 [64/110 (50%)]\tLoss: 1.939230\n",
      "====> Epoch: 81 Average loss: 2.4572\n",
      "====> Test set loss: 51.0890\n",
      "Train Epoch: 82 [0/110 (0%)]\tLoss: 2.248536\n",
      "Train Epoch: 82 [64/110 (50%)]\tLoss: 1.902504\n",
      "====> Epoch: 82 Average loss: 2.3495\n",
      "====> Test set loss: 50.5748\n",
      "Train Epoch: 83 [0/110 (0%)]\tLoss: 1.892279\n",
      "Train Epoch: 83 [64/110 (50%)]\tLoss: 1.898277\n",
      "====> Epoch: 83 Average loss: 2.2756\n",
      "====> Test set loss: 52.0552\n",
      "Train Epoch: 84 [0/110 (0%)]\tLoss: 1.800397\n",
      "Train Epoch: 84 [64/110 (50%)]\tLoss: 1.837871\n",
      "====> Epoch: 84 Average loss: 2.2671\n",
      "====> Test set loss: 50.9964\n",
      "Train Epoch: 85 [0/110 (0%)]\tLoss: 1.947836\n",
      "Train Epoch: 85 [64/110 (50%)]\tLoss: 1.983469\n",
      "====> Epoch: 85 Average loss: 2.2079\n",
      "====> Test set loss: 50.8288\n",
      "Train Epoch: 86 [0/110 (0%)]\tLoss: 1.733851\n",
      "Train Epoch: 86 [64/110 (50%)]\tLoss: 2.097947\n",
      "====> Epoch: 86 Average loss: 2.2619\n",
      "====> Test set loss: 50.4407\n",
      "Train Epoch: 87 [0/110 (0%)]\tLoss: 1.943773\n",
      "Train Epoch: 87 [64/110 (50%)]\tLoss: 1.892084\n",
      "====> Epoch: 87 Average loss: 2.1706\n",
      "====> Test set loss: 49.8241\n",
      "Train Epoch: 88 [0/110 (0%)]\tLoss: 1.720354\n",
      "Train Epoch: 88 [64/110 (50%)]\tLoss: 1.773557\n",
      "====> Epoch: 88 Average loss: 2.1265\n",
      "====> Test set loss: 49.8722\n",
      "Train Epoch: 89 [0/110 (0%)]\tLoss: 1.840760\n",
      "Train Epoch: 89 [64/110 (50%)]\tLoss: 1.788596\n",
      "====> Epoch: 89 Average loss: 2.0516\n",
      "====> Test set loss: 48.6531\n",
      "Train Epoch: 90 [0/110 (0%)]\tLoss: 1.705915\n",
      "Train Epoch: 90 [64/110 (50%)]\tLoss: 1.797746\n",
      "====> Epoch: 90 Average loss: 1.9974\n",
      "====> Test set loss: 49.0686\n",
      "Train Epoch: 91 [0/110 (0%)]\tLoss: 1.702792\n",
      "Train Epoch: 91 [64/110 (50%)]\tLoss: 1.720740\n",
      "====> Epoch: 91 Average loss: 1.9779\n",
      "====> Test set loss: 48.7383\n",
      "Train Epoch: 92 [0/110 (0%)]\tLoss: 1.528925\n",
      "Train Epoch: 92 [64/110 (50%)]\tLoss: 1.584211\n",
      "====> Epoch: 92 Average loss: 1.9036\n",
      "====> Test set loss: 48.7608\n",
      "Train Epoch: 93 [0/110 (0%)]\tLoss: 1.592054\n",
      "Train Epoch: 93 [64/110 (50%)]\tLoss: 1.746113\n",
      "====> Epoch: 93 Average loss: 1.8667\n",
      "====> Test set loss: 47.9738\n",
      "Train Epoch: 94 [0/110 (0%)]\tLoss: 1.683146\n",
      "Train Epoch: 94 [64/110 (50%)]\tLoss: 1.494201\n",
      "====> Epoch: 94 Average loss: 1.8427\n",
      "====> Test set loss: 47.8007\n",
      "Train Epoch: 95 [0/110 (0%)]\tLoss: 1.675150\n",
      "Train Epoch: 95 [64/110 (50%)]\tLoss: 1.557961\n",
      "====> Epoch: 95 Average loss: 1.8094\n",
      "====> Test set loss: 47.3352\n",
      "Train Epoch: 96 [0/110 (0%)]\tLoss: 1.525572\n",
      "Train Epoch: 96 [64/110 (50%)]\tLoss: 1.411476\n",
      "====> Epoch: 96 Average loss: 1.8052\n",
      "====> Test set loss: 47.2172\n",
      "Train Epoch: 97 [0/110 (0%)]\tLoss: 1.594017\n",
      "Train Epoch: 97 [64/110 (50%)]\tLoss: 1.544323\n",
      "====> Epoch: 97 Average loss: 1.7012\n",
      "====> Test set loss: 47.4533\n",
      "Train Epoch: 98 [0/110 (0%)]\tLoss: 1.498594\n",
      "Train Epoch: 98 [64/110 (50%)]\tLoss: 1.494172\n",
      "====> Epoch: 98 Average loss: 1.7426\n",
      "====> Test set loss: 46.8198\n",
      "Train Epoch: 99 [0/110 (0%)]\tLoss: 1.400527\n",
      "Train Epoch: 99 [64/110 (50%)]\tLoss: 1.507476\n",
      "====> Epoch: 99 Average loss: 1.7131\n",
      "====> Test set loss: 46.8583\n",
      "Train Epoch: 100 [0/110 (0%)]\tLoss: 1.423842\n",
      "Train Epoch: 100 [64/110 (50%)]\tLoss: 1.382963\n",
      "====> Epoch: 100 Average loss: 1.6778\n",
      "====> Test set loss: 47.1999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 100 + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1048576/128/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 8, 3, 16384] to have 3 channels, but got 8 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-97b917284364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-778eff95ac67>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# sample latent code z from q given x.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 3, 3, 3], expected input[1, 8, 3, 16384] to have 3 channels, but got 8 channels instead"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_loss = 0\n",
    "for batch_idx,( data, _) in enumerate(train_loader):\n",
    "    data = data.to(device)\n",
    "    data = data.reshape([-1,3, 128*128])\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    loss = loss_function(recon_batch, data, mu, logvar)\n",
    "    loss.backward()\n",
    "    train_loss += loss.item()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader),\n",
    "            loss.item() / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
